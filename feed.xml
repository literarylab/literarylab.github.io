<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Stanford Literary Lab</title>
  <subtitle>A research collective that applies computational criticism, in all its forms, to the study of literature.</subtitle>
<link href="https://litlab.stanford.edu/" rel="self"/>
  <link href="https://litlab.stanford.edu/"/>
  <updated>2023-09-03T00:00:00Z</updated>
  <id>https://litlab.stanford.edu/</id>
  <author>
    <name>Stanford Literary Lab</name>
  </author>
  
  <entry>
    <title>Virtual Readers</title>
    <link href="https://litlab.stanford.edu/virtual-readers/"/>
    <updated>2016-08-25T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/virtual-readers/</id>
    <content type="html">&lt;p&gt;Often, the most exciting moment of a Lab project occurs when our research takes an unexpected direction: we thought we were doing &#39;a&#39;, but it turns out that all along we&#39;ve been doing &#39;b&#39; (or, more often, &lt;em&gt;should have&lt;/em&gt; been doing &#39;b&#39;). The realization that we&#39;ve discovered something unexpected, the ability to be guided by the research and its results: these are what differentiates a Lab project from the traditional pursuits of the humanities. And nowhere is this turn more obvious, more frustrating and yet rewarding, than when it occurs at the level of method: when a project whose underlying technology, such as principle component analysis, or sequence matching, proves frustratingly elusive to the data only to suddenly fall into place when one of the project participants points out that our data looks more like something that would respond to, for example, information theory or topic modeling. In such cases, more often than not, we realize that we can now think about the research entirely differently and that what we had expected and hoped to find with our original method is much less interesting than what we now can discover, seemingly by accident.&lt;/p&gt;
&lt;p&gt;The Lab&#39;s project on suspense literature underwent one such reversal midway through our research. Our initial goal in the project was to look for formal features in texts that were associated with the experience of suspense on the part of readers (whether or not they were actually causal), and our initial methods were largely exploratory and heavily informed by our assumptions as to what we might find. In our first attempts, we created features sets (effectively semantic fields) and traced their frequency through the narratives of novels from our hand-assembled suspenseful and unsuspenseful corpora. Despite finding a few promising moments of correlation between various topics and some potentially suspenseful moments in certain narratives (as identified and tagged by readers), we were left stymied by the sheer variety of features and types of suspense. It wasn&#39;t until one member of the group suggested (in jest) that it would be much easier if we could use our features to create a suspense detector that the project became clear. We realized that we had been thinking about our data in the wrong way. Rather than create graphs of semantic fields across narratives and try to interpret them ourselves, what if we had a computer create a model based on the patterns that were too subtle for our readerly comprehension? Then we could investigate the choices that it made to identify the patterns that seemed to be indicative of suspense.&lt;/p&gt;
&lt;p&gt;This move, from an exploratory, cluster based approach to one predicated on a classification model fundamentally shifted how we saw the project. In the former, the heuristic model is much more familiar to traditional humanities research: the computer transforms the data and then we use our own critical eye to recognize patterns in the data derived from the texts.&lt;a href=&quot;https://litlab.stanford.edu/virtual-readers/#ftn1&quot;&gt;[1]&lt;/a&gt; In the latter, we surrender the reading process to the computer and, by watching how the algorithm makes its decisions as to what a suspenseful passage is and what features indicate its presence, we identify the groups of features, and patterns that we had sought from the beginning. We can then judge these computer derived patterns against our critical knowledge on suspense literature. As an added benefit, the success or failure of the model would also indicate how strongly our selected formal features were indicative of the readerly perception of suspense.&lt;a href=&quot;https://litlab.stanford.edu/virtual-readers/#ftn2&quot;&gt;[2]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;But now that we had decided to make the move from clustering to classification, we were left with a new decision: what classification model would work best, given our somewhat unorthodox data? For our features, we had assembled 87 significantly distributed topics (out of a 150 Gibbs topic model of our suspense and unsuspense corpora), a list of words distinctive of our suspense corpus and a related list of words distinctive of our unsuspense corpus. To this we also added the average age of acquisition (AoA) scores of the texts, and distinctive words of suspense and unsuspense in a smaller short story corpus. We measured each of these fields in over 800 passages that had been hand rated for suspense by a group of readers (our passages were 2% slices of the text in a moving window advanced by 1%).&lt;a href=&quot;https://litlab.stanford.edu/virtual-readers/#ftn3&quot;&gt;[3]&lt;/a&gt; Initially, many more passages had been rated on a scale of 1 to 10 by members of the group, but for this analysis, we only kept those scored below a 3 or above a 7, to create a binary response variable of either &#39;suspense&#39; or &#39;unsuspense.&#39; Given our new goal, what we needed was a model capable of combining these features sets, which varied from an average of 23 words per topic field and between 3000 and 4000 for our distinctive word fields, not to mention age of acquisition scores, into a coherent model.&lt;/p&gt;
&lt;p&gt;Our first attempts, using the familiar classificatory models of logistic regression, discriminant function analysis (DFA) and support vector machines (SVM), failed. None classified with an accuracy of more than 65% (using a training sample of 75% of our data, cross-validated with a test set of the remaining 25%). In fact, our most successful model was the SVM, which simply classified all of our passages as suspenseful: due to an imbalance in our corpus between the suspenseful and unsuspenseful passages, this resulted in our 65% success rate (or, exactly equal to chance). Clearly, either our initial premise was wrong -- perhaps suspense was entirely located in reader&#39;s affect and experience -- or there was some complexity to our variables that resisted these classificatory models. We began to suspect that our features, especially given their diverse origins and unequal sizes, may not combine linearly with our response variables (e.g. a+b+c+d+e...) as the models assume, but may be non-linear (exponential, logarithmic, decaying or chaotic).&lt;/p&gt;
&lt;p&gt;It was here that we hit upon the Artificial Neural Network (or ANN). Neural networks have become much more popular in recent large-scale complex modeling (such as Google&#39;s deepnet) or in  word vector analysis (such as Word2Vec, where the relationships between the hidden nodes reveal relationships between words). Although our goals were much more straightforward, and our variables were much less complicated, the ANN offered a number of benefits over other classificatory models. Computationally, the advantage of the neural network lies in its ability to handle non-linear models As one of our eventual goals was to increase our feature set with even more diverse variables (measures of volatility, type token ratios, part of speech tags, narrative position, etc), we need our model to account, not just for our present set of variables, but for any that we chose to add in the future, whether continuous, categorical or something else. From a critical standpoint, too, the psudeo-cognative structure of the neural network suggested an intriguing resonance with the reader-focused aspect of the project: in it, we could combine all of the experiences of our group of readers as we trained a new, comprehensive virtual reader, whose actions we could observe, record and analyze.&lt;/p&gt;
&lt;p&gt;There are, however, a few detriments to using an ANN in this way. First, it comes with a much greater computational overhead than the other classificatory algorithms that I listed above. This overhead makes sense when working with the extremely large data sets that deep learning neural networks have become famous for (millions of images, or billions of words). Any additional overhead in these cases is utterly dwarfed by the size of the data (when you are working with petabytes of data, a few extra megabytes in the model is meaningless). Also, neural networks are the ultimate black boxes: peering beneath the hood reveals the weighting system among the hidden layers of nodes, but, unlike logistic regression, for example, it does not reveal a one-to-one correspondence between a variable and its importance to the classification. Nevertheless, even when taking these into consideration, the advantages of the ANN outweighed the detriments and we set out to train a neural network to identify suspense.&lt;/p&gt;
&lt;p&gt;For our initial attempt, I created a set of neural networks using Fritsch and Guenther&#39;s package &lt;em&gt;neuralnet&lt;/em&gt; in R (&lt;a href=&quot;https://cran.r-project.org/web/packages/neuralnet/neuralnet.pdf&quot;&gt;https://cran.r-project.org/web/packages/neuralnet/neuralnet.pdf&lt;/a&gt;). Although the package allows for the relatively straightforward creation of neural networks trained through backpropagation with the neuralnet() function, it was more expedient to create a set of wrapping functions for training an ANN with the package that could easily be deployed using our data:&lt;/p&gt;
&lt;p&gt;&lt;script src=&quot;https://gist.github.com/erikfredner/3f29542d59723e7641cdbbae64b57766.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;This function takes a training set (raw.data), a withheld test sample (test.set) and a vector of column identifiers for both the input features (variable.vec) and the response variables (output.var). The response variables expected by the neural net packages in R are different when compared to other classification models: the neuralnet package expects and classifies on one or more continuous variables. The flag cat=T calls the class.ind() function from a separate package (&lt;em&gt;nnet&lt;/em&gt;), which converts a single categorical variable into a binary matrix, where the columns correspond to the levels of the categorical variable as a factor and each row contains a 1 in its corresponding column. In other words, the function converts a single categorical response variable to some number of binary response variables based on how many categorical values were in the initial variable (in this case, 2: suspense and unsuspense). Note that in the training step, the error handling of the function allows the model to fail: this often occurs when the model converges to a local, rather than global, minimum.&lt;/p&gt;
&lt;p&gt;The wrapping function above does not just create a neural network model, it also cross-validates that model against the withheld test sample with the function netClass():&lt;/p&gt;
&lt;p&gt;&lt;script src=&quot;https://gist.github.com/erikfredner/dbd402240e804ac631e09e84a532d541.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;The function netClass takes a trained ANN model, a withheld test sample, and a set of correct class assignments. After validating the model by classifying the test sample against the known assignments, it returns the overall error rate for the validation, as well as a classification table and a confusion plot showing the relative classification success for each class. These are returned as part of the output list, along with the trained model, by the trainNeuralNet() function.&lt;/p&gt;
&lt;p&gt;Like a topic model that requires a pre-established number of topics, the neuralnet package requires a pre-determined internal structure: some number of hidden levels, each with some number of hidden nodes per level. The training process assigns weights to these nodes, but the actual numbers are fed to the algorithm when it is initialized, here through a vector the same length as the number of hidden levels; the values of each element indicate the number of hidden nodes per level.&lt;a href=&quot;https://litlab.stanford.edu/virtual-readers/#ftn4&quot;&gt;[4]&lt;/a&gt; Determining the number of nodes is highly subjective, particularly for a shallow ANN with a relatively small data set (886 scored passages in our initial sample). In our project, then, we created a series of backpropagation-trained ANNs, varying the number of levels between 1 and 4 and the number of nodes per level between 5 and 8. We iterated each level and node combination 5 times and averaged the error statistics across the iterations (creating, in total, 420 different models). These relatively small networks offered, at least initially, better results than much larger networks, and the small size allowed us to create and store a number of iterations.&lt;/p&gt;
&lt;p&gt;In our first results, the best performing network was a simple single-layer neural network with just five hidden nodes. Plotting it, we can easily see its structure (click any image to view the full-resolution version):&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/PlotofNet.jpg&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/PlotofNet.jpg&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/virtual_readers_Confusion-Plot_2.jpg&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/virtual_readers_Confusion-Plot_2.jpg&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Not only did this success rate outperform logistic regression (65% correct) and SVM (62% correct), but its accuracy overall astonished us. We had expected to observe and draw conclusions from its failures, but here, the successes are far more informative. Based solely on a set of semantic fields, culled from topics models and distinctive words, our virtual reader was able to correctly predict a human rater’s response to a passage four out of five times.&lt;/p&gt;
&lt;p&gt;Although one the limitations on using a neural network is our inability to reconstruct the logic behind the weights it assigns to individual nodes (and thus the exact mechanism of its classification decisions), it is still possible to peek into the black box and see the relative magnitude of importance of each variable (if not its precise relationship to the other features or the direction of its weight). And, by bringing our critical understanding of the data to the weights, we can begin to productively unpack the classifier. For the model above, we were able to use a garson plot from the package &lt;em&gt;NeuralNetTools&lt;/em&gt; to see the relative importance of each feature in deciding whether a passage was suspenseful or unsuspenseful:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/WeightsofNN_suspense.jpg&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/WeightsofNN_suspense.jpg&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;From the neural network then, we were able to not only ascertain that there is a predictive relationship between at least some of the formal features that we identified, and whether the passage was suspenseful or unsuspenseful, but also what features were most strongly associated with suspenseful topics (either in their presence, as we suspect for the topics that we termed “Physical Pain” and “American Military”, or in their absence, as we suspect for “Drawing Room Conversation” or “Sentimental Romanticism”).&lt;/p&gt;
&lt;p&gt;The neural network, then, became a virtual participant in the project, an overlay of all of the individual reading experiences of all of the human participants. But it was a participant that was also able to associate patterns of topic and distinctive and difficult word usage  with suspenseful or unsuspenseful passages across all of our texts regardless of period or genre. By reading this composite reader, whose level of detail is unmatched by any of our human participants, we were finally able to start drawing the conclusions about the relationship between semantics and suspense that had eluded us during the initial stages of the project. Our unexpected turn from exploratory clusters to supervised classification, from a digitally assisted hermeneutics, to a radically altered meta-observational method, finally provided some answers to many of our initial questions about suspense as a formal property, even as it began to make these questions obsolete by pushing us in new, unlooked for, directions. In this, then, lies both the limits and the promise, the frustration and the excitement, of our work at the Lab.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;ftn1&quot;&gt;[1]&lt;/a&gt; This, for example is what Stephen Ramsay calls “algorithmic criticism” in his book &lt;em&gt;Reading Machines&lt;/em&gt;, when he describes how computational methods “deform” the texts.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;ftn2&quot;&gt;[2]&lt;/a&gt; Ted Underwood’s recent work using the success and failure of logistic regression to identify genre consistency and identity over time is an excellent example of this process in action.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;ftn3&quot;&gt;[3]&lt;/a&gt; In ‘measuring’ the features in passages, we abandoned the posterior probabilities of the topic model, or the p-values of the distinctive words and instead calculated the percentages of words in each passage that came from each field, normalizing the results by the expected value of that field in the overall corpus.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;ftn4&quot;&gt;[4]&lt;/a&gt; So, for example, the vector (9,9,5,5) would describe a network with four hidden levels and nine hidden nodes in each of the first two levels and five nodes in the final two levels.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Humanities Without Humanists</title>
    <link href="https://litlab.stanford.edu/humanities-without-humanists/"/>
    <updated>2016-08-25T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/humanities-without-humanists/</id>
    <content type="html">&lt;p&gt;The humanities are used to feeling embattled, and, consequently, to making excuses for their existence. We should be allowed to study literature, one such line of argument goes, because reading and writing make students better employees or better citizens or more empathic human beings --- because literature has some merit that is not &lt;em&gt;merely&lt;/em&gt; aesthetic. Take, for instance, a widely publicized article in &lt;em&gt;Science&lt;/em&gt; affirming that &amp;quot;&lt;a href=&quot;http://science.sciencemag.org/content/342/6156/377.full&quot;&gt;Reading Literary Fiction Improves Theory of Mind&lt;/a&gt;&amp;quot;: the psychologist authors, while acknowledging the &amp;quot;difficulty in precisely quantifying literariness,&amp;quot; casually reify that &amp;quot;literariness&amp;quot; in their effort to demonstrate its virtues, concluding that reading &amp;quot;prize-winning&amp;quot; texts seems more beneficial, psychologically &lt;em&gt;healthier&lt;/em&gt;, than reading &amp;quot;popular&amp;quot; fiction. The fact that literary scholars would be likely to reject the precise terms of their distinction --- which places Dashiell Hammett and Robert Heinlein in the same category as Danielle Steel and Christian novelist William Paul Young, and which ignores variables like translation, publication date, and author demographics --- was trumped, when the article was picked up in the popular media, by the suggestion that we should be grateful that these scientists have taken an interest in our object of study.&lt;/p&gt;
&lt;p&gt;In a sense, I am. Among literary study&#39;s dizzying series of 21st-century &amp;quot;turns,&amp;quot; the &lt;a href=&quot;http://arcade.stanford.edu/blogs/am-i-turning-empirical&quot;&gt;empirical one&lt;/a&gt; strikes me as the most intellectually stimulating and the likeliest to last, providing a means for literary scholars to generate and test falsifiable hypotheses without abandoning the interpretive rigor that characterizes our discipline. And digital humanities, of course, is the star of this new empiricism; the question is --- why? What is it about large-scale quantitative analysis that strikes us as so invigorating and so promising? This question may sound like a softball, but I think a prolonged discussion would reveal that many of us in DH actually disagree on the answer. I have one, of course, that I think is right --- but before I get to it, I want to explore one especially common &lt;em&gt;wrong&lt;/em&gt; answer.&lt;/p&gt;
&lt;p&gt;A few weeks ago, an article started showing up in my Twitter feed and my email inbox. The title was admirably straightforward --- &amp;quot;&lt;a href=&quot;http://arxiv.org/abs/1606.07772&quot;&gt;The emotional arcs of stories are dominated by six basic shapes&lt;/a&gt;&amp;quot; --- and the authors were all mathematicians or computer scientists, mostly based in the Computational Story Lab at the University of Vermont. Having generated &amp;quot;emotional arcs&amp;quot; by assigning sentiment scores within a 10000-word sliding window (using a tool delightfully named the Hedometer), the team then classified the arcs into six major &amp;quot;modes&amp;quot; using Singular Value Distribution. Because the team was able to duplicate these &amp;quot;modes&amp;quot; as clusters via both hierarchical clustering and unsupervised machine learning, and because randomly shuffled &amp;quot;word salad&amp;quot; versions of the texts did not generate similar arcs, the authors feel justified in their conclusion that &amp;quot;these specific arcs are uniquely compelling as stories written by and for &lt;em&gt;homo narrativus&lt;/em&gt;.&amp;quot;&lt;/p&gt;
&lt;p&gt;Well --- ok. This is &lt;em&gt;extraordinarily&lt;/em&gt; similar to Matt Jockers&#39;s &lt;a href=&quot;http://www.matthewjockers.net/2015/02/02/syuzhet/&quot;&gt;Syuzhet&lt;/a&gt; project (right down to the identification of 6-7 basic plot types), which the authors never mention by name in the body of the article; but perhaps the conventions of citation are sufficiently different for computer scientists that this does not read to &lt;em&gt;them&lt;/em&gt; as unethical.&lt;a href=&quot;https://litlab.stanford.edu/humanities-without-humanists/#ftn1&quot;&gt;[1]&lt;/a&gt; It&#39;s a huge and essentializing claim, which makes many humanities scholars uneasy; but what is DH for if not to push us toward more ambitious arguments? There may be issues with generalizing about all stories from what is at best a corpus limited by time and nationality (Gutenberg, for which I have a deep and abiding love, is nonetheless very Eurocentric); but it&#39;s still better, surely, to put forward a suggestion that future research can refine than to make the kind of contentless claim --- &amp;quot;both affirms and subverts,&amp;quot; &amp;quot;both endorses and undermines&amp;quot; --- we used to see in the bad old days of &lt;em&gt;anti&lt;/em&gt;-empiricist criticism, the kind of claim that aims for nuance but flounders in vagueness. Right?&lt;/p&gt;
&lt;p&gt;Right, I think, in principle. But then one looks at the authors&#39; lists of the top 5 texts associated with the various arcs, and notices something curious.&lt;a href=&quot;https://litlab.stanford.edu/humanities-without-humanists/#ftn2&quot;&gt;[2]&lt;/a&gt; Here&#39;s &lt;em&gt;A Christmas Carol&lt;/em&gt;, sure, and &lt;em&gt;A Hero of Our Time&lt;/em&gt;; here&#39;s &lt;em&gt;The Adventures of Tom Sawyer&lt;/em&gt; and something called &lt;em&gt;Tarzan the Terrible&lt;/em&gt;; here&#39;s --- &lt;em&gt;Fundamental Principles of the Metaphysic of Morals&lt;/em&gt;. Kant? A one-off mistake, perhaps? But wait, it&#39;s &lt;em&gt;Notes on Nursing&lt;/em&gt; by Florence Nightingale, and Lucretius&#39;s &lt;em&gt;On the Nature of Things&lt;/em&gt;; &lt;em&gt;The Economic Consequences of the Peace&lt;/em&gt; and &lt;em&gt;Cookery and Dining in Imperial Rome&lt;/em&gt;, which is quite literally a book of &lt;em&gt;recipes&lt;/em&gt; --- and all these, mind you, drawn from the top five most representative examples of each story arc (although one finds plenty more nonfictional works when one examines the hierarchical tree in the appendix). Kant, for instance, is apparently the fifth most perfect instance of the &amp;quot;Man in a Hole&amp;quot; narrative template (a designation the authors borrowed from &lt;a href=&quot;https://www.youtube.com/watch?v=oP3c1h8v2ZQ&quot;&gt;Kurt Vonnegut&lt;/a&gt; --- possibly by way of Jockers again, who cites the same Vonnegut talk); this has a certain hilarious aptness, but is ultimately hard to fathom.&lt;/p&gt;
&lt;p&gt;A significant subset of the &amp;quot;stories&amp;quot; analyzed by the authors, then, appear not even to have been &lt;em&gt;narratives&lt;/em&gt;, let alone fictional ones. But there&#39;s a more subtle problem with some of the fictional works as well. One might see the name of Balzac or Poe and assume that we are here at least dealing with appropriate narrative texts. But one of those Balzac &amp;quot;narratives&amp;quot; is &lt;em&gt;The Human Comedy: Introductions and Appendix&lt;/em&gt;, while Poe&#39;s collected &lt;em&gt;Works&lt;/em&gt; make an appearance; there are also anthologies of works that aren&#39;t even by a single author, like &lt;em&gt;Fifty Famous Stories Retold&lt;/em&gt; or &lt;em&gt;Humour, Wit, and Satire of the Seventeenth Century&lt;/em&gt;. (The authors identify &lt;em&gt;A Primary Reader&lt;/em&gt; as &amp;quot;among the most categorical tragedies [they] found,&amp;quot; which would surely alarm the well-intentioned elementary school teacher who composed this story collection for her first-grade charges.) If you try to track an &amp;quot;emotional arc&amp;quot; across one of these anthologies, you may well get a pattern that seems recognizable, but it will be invalidated by the presence of discrete narratives &lt;em&gt;within&lt;/em&gt; the text: &amp;quot;The Murders in the Rue Morgue&amp;quot; and &amp;quot;The Balloon-Hoax,&amp;quot; for instance, have distinct and unrelated arcs, but would show up by the authors&#39; methods as mere moments in a larger &amp;quot;narrative&amp;quot; that does not, in fact, exist --- not in authorial intention, and not in readerly experience.&lt;/p&gt;
&lt;p&gt;Here you may object, reasonably, that I&#39;m piling on, dismantling an analysis that isn&#39;t necessarily worth the trouble; after all, digital humanists know that we&#39;re supposed to clean up our corpora and retain metadata on texts&#39; genres, whereas the authors of this paper seem not to have recognized that these steps were important. And it&#39;s true, they surely didn&#39;t realize; but this is exactly my point. For researchers with any sort of background in literary studies, the results obtained here would have provided valuable feedback about the accuracy of the &amp;quot;Hedometer&amp;quot; tool: if your sentiment analysis algorithm is finding emotional arcs in Kant and cookbooks, it is effectively broken.&lt;a href=&quot;https://litlab.stanford.edu/humanities-without-humanists/#ftn3&quot;&gt;[3]&lt;/a&gt; Precisely because the authors didn&#39;t begin by conceptualizing fiction and non-fiction, or narrative and non-narrative, as discrete categories within &amp;quot;the literary,&amp;quot; the discovery that the two showed the same characteristics under &amp;quot;Hedometer&amp;quot; did not register as problematic or even surprising --- as, objectively, it should be.&lt;/p&gt;
&lt;p&gt;The same goes for countless other morsels of expert knowledge that would, in a successful DH project, provide a sanity check on the plot arc results. Does the sentiment analysis tool adequately deal with irony --- and, if not, can we assume that this washes out in a large data set, or do we need to exercise special care with particular authors or genres? Might focalization choices have predictable effects on textual sentiment that would add more nuance to the Hedometer&#39;s judgments? How do these story templates, if they prove accurate, interact with more prescriptive theories of plot structure (for instance, the Aristotelian model that the authors mention in their introduction)? These are not follow-up questions, building on the data generated by this research project; they are integral to determining that data&#39;s validity and meaning. If the authors so thoroughly miss an opportunity to say something worthwhile about the humanities, it is because they ignore the concepts and tacit knowledge internal to literary study; rather than &lt;a href=&quot;https://litlab.stanford.edu/LiteraryLabPamphlet6.pdf&quot;&gt;operationalizing&lt;/a&gt; literary concepts --- character, canonicity, personification, poetic meter --- with the help of big(gish) data, these &amp;quot;emotional arcs&amp;quot; are built on shaky conceptual foundations, making even the basic information they purport to provide practically unusable.&lt;/p&gt;
&lt;p&gt;I&#39;ve been focusing my criticism on the applied mathematicians who wrote this paper, which may seem to suggest that I believe &lt;em&gt;we&lt;/em&gt; humanists would never do this; &lt;em&gt;we&#39;d&lt;/em&gt; never let our love for an analytical tool blind us to the weak results it produced, or elide humanistic knowledge in the interest of arresting visualizations. It might be more accurate, though, to say that &lt;em&gt;when&lt;/em&gt; we do this, it can&#39;t necessarily be explained by mere ignorance. Rather, DH projects that treat the humanities as a mere data set --- rather than a robust mode of inquiry with protocols of its own --- are making a calculated decision to use science as a &lt;em&gt;legitimizing&lt;/em&gt; tool rather than a truly investigative one. The implication, I think, is that your average non-digital humanist is simply a subpar scientist, and that one can therefore learn more by applying quantitative tools to raw texts than by generating hypotheses and writing programs on the basis of humanistic knowledge. Contrary to many non-DH practitioners who seem to think that this devaluation is DH&#39;s ultimate agenda, I&#39;d argue that nothing could be more fatal to the future of the digital humanities. What a truly empirical humanities would suggest, after all, is that literature (for instance) is worth investigating for its own sake, beyond the potential for monetizing a particular plot structure or trading on cultural capital; as a complex form of human behavior, literature deserves the same conceptual rigor and expertise that we would have no qualms about bringing to, say, the study of traffic planning or coral colonies. (Would a mathematician consider analyzing coral reef distribution without consulting a marine biologist, or indeed even &lt;em&gt;reading&lt;/em&gt; any active marine biologists?) To allow papers like this one to represent DH --- as, flattered and hopeful, we often do --- is to mistake &lt;em&gt;un&lt;/em&gt;disciplinary for &lt;em&gt;inter&lt;/em&gt;disciplinary research, or indeed to forget that we have a discipline at all. It&#39;s short-term visibility, bought with long-term extinction.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;ftn1&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt; The authors critique Jockers’s project, linking to his work in a footnote without explicitly mentioning him, when they claim that “other work” on sentiment analysis within texts has confused “the emotional arc and the plot of a story.” Given that the authors label their own emotional arcs with plot-type designations like “Rags-to-riches,” “Cinderella,” “Tragedy,” and “Icarus,” one might suspect that they themselves are not entirely scrupulous about this distinction. Ben Schmidt, in his &lt;a href=&quot;http://sappingattention.blogspot.com/2016/07/plot-arceology-emotion-and-tension.html&quot;&gt;blog post&lt;/a&gt; responding to the paper, also notes the similarity to Jockers’s work.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;ftn2&quot;&gt;&lt;sup&gt;[2]&lt;/sup&gt;&lt;/a&gt; Again, Schmidt &lt;a href=&quot;http://sappingattention.blogspot.com/2016/07/plot-arceology-emotion-and-tension.html&quot;&gt;noticed&lt;/a&gt; this too, in a blog post I became aware of after writing this one. He points out, correctly, that the authors’ means of separating fiction from non-fiction — length and download count — are “*terrible* inputs into a fiction/nonfiction classifier.”&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;ftn3&quot;&gt;&lt;sup&gt;[3]&lt;/sup&gt;&lt;/a&gt; As David McClure pointed out to me in conversation, there is another explanation for this phenomenon: it could be that these non-narrative texts &lt;em&gt;do&lt;/em&gt; in fact have emotional arcs that the tool is picking up accurately, and that the authors have discovered something about the affective structure of nonfiction. This is certainly possible, but I think one could only make the argument if the authors had derived their six “emotional arcs” from a corpus that they knew to contain only narratives; in that case, finding evidence of narrative structures in a non-narrative text would suggest that the text &lt;em&gt;did&lt;/em&gt; in fact have some narrative component. Because the non-narratives were baked into the corpus from the beginning, though, their arcs presumably influenced the outcome of the SVD analysis, making it hard to argue that these arcs reflect anything distinctive about stories per se.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Counting words in HathiTrust with Python and MPI</title>
    <link href="https://litlab.stanford.edu/counting-words-in-hathitrust-with-python-and-mpi/"/>
    <updated>2016-08-26T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/counting-words-in-hathitrust-with-python-and-mpi/</id>
    <content type="html">&lt;p&gt;In recent months we&#39;ve been working on a couple of projects here in the Lab that are making use of the &lt;a href=&quot;https://analytics.hathitrust.org/features&quot;&gt;Extracted Features data set&lt;/a&gt; from HathiTrust. This is a fantastic resource, and I owe a huge debt of gratitude to everyone at HTRC for putting it together and maintaining it. The extracted features are essentially a set of very granular word counts, broken out for each physical page in the corpus and by part-of-speech tags assigned by the OpenNLP parser. For example, we can say -- on the first page of &lt;em&gt;Moby Dick&lt;/em&gt;, &amp;quot;Call&amp;quot; appears 1 time as a &lt;code&gt;NNP&lt;/code&gt;, &amp;quot;me&amp;quot; 5 times as a &lt;code&gt;PRP&lt;/code&gt;, and &amp;quot;Ishmael&amp;quot; 1 time as a &lt;code&gt;NNP&lt;/code&gt;, etc. What&#39;s missing, of course, is the syntagmatic axis, the actual sequence of words -- &amp;quot;Call me Ishmael.&amp;quot; This means that it&#39;s not possible to do any kind of analysis at the level of the sentence or phrase. For instance, we couldn&#39;t train a &lt;a href=&quot;https://en.wikipedia.org/wiki/Word2vec&quot;&gt;word2vec&lt;/a&gt; model on the features, since word2vec hooks onto a fairly tight &amp;quot;context window&amp;quot; when learning vectors, generally no more than 5-10 words. But, with just the per-page token counts, it is possible to do a really wide range of interesting things -- tracking large-scale changes in word usage over time, looking at how cohorts of words do or don&#39;t hang together at different points in history, etc. It&#39;s an interesting constraint -- the macro (or at least meso) scale is more strictly enforced, since it&#39;s harder to dip back down into a chunk of text that can actually be read, in the regular sense of the idea.&lt;/p&gt;
&lt;p&gt;The real draw of this kind of data set, though, is the sheer size of the thing, which is considerable -- 4.8 million volumes, 1.8 billion pages, and many hundreds of billions of words, packed into 1.2 terabytes of compressed JSON files. These numbers are dizzying. I always try to imagine what 5 million books would look like in real life -- how many floors of the stacks over in Green Library, here at Stanford? How many pounds of paper, gallons of ink? In the context of literary studies, data at this scale is fascinating and difficult. When we make an argument based on an analysis of something like Hathi -- what&#39;s the proper way to frame it? What&#39;s the epistemological status of a truth claim based on 5 million volumes, as opposed to 2 million, 1 million, a hundred thousand, or ten? Surely there&#39;s a difference -- but how big of a difference, and what type of difference? Is it categorical or continuous? What&#39;s the right balance between intellectually capitalizing on the scale of the data -- using it to make claims that are more ambitious than would be possible with smaller corpora -- and also avoiding the risk of over-generalizing, of mistaking a (large) sample for the population?&lt;/p&gt;
&lt;p&gt;These are wonderful problems to have, of course. In addition to the philosophical challenges, though, we quickly realized that the size of the corpus also poses some really interesting technical difficulties. The type of code that I&#39;m used to writing for smaller corpora will often bounce right off a terabyte of data -- or at least, it might take many days or weeks to inch through it all. To help kick off the lab&#39;s new &lt;em&gt;Techne&lt;/em&gt; series, I wanted to take a look at some of the parallel programming patterns we&#39;ve been working with that make it possible to spread out these kinds of big computations across many hundreds or thousands of individual processors -- namely, a protocol called the &amp;quot;&lt;a href=&quot;https://en.wikipedia.org/wiki/Message_Passing_Interface&quot;&gt;Message Passing Interface&lt;/a&gt;&amp;quot; (MPI), a set of programming semantics for distributing programs in large computing grids. This is under-documented, and can feel sort of byzantine at times. But it&#39;s also incredibly powerful, and, from a standpoint of programming craft, it introduced me to a whole new way of structuring programs that I had never encountered before.&lt;/p&gt;
&lt;p&gt;Now, I&#39;d be remiss not to mention that HathiTrust actually provides a platform that makes it possible to run custom jobs on their computing infrastructure. (You can sign up for an account &lt;a href=&quot;https://analytics.hathitrust.org/signup&quot;&gt;here&lt;/a&gt;.) This is extremely cool, though we&#39;ve run into a number of situations recently -- both with Hathi and with other data sets -- where we found ourselves needing to write this type of code, so I wanted to figure out how to do it in-house. The extracted features seemed like an obvious place to start.&lt;/p&gt;
&lt;h4&gt;The simple way -- loop through everything, one-by-one&lt;/h4&gt;
&lt;p&gt;So, we&#39;ve got 5 million bzipped JSON files. Generally, to pull something of interest out of the corpus, we need to do three things -- decompress each file, do some kind of analysis on the JSON for the volume, and then merge the result into an aggregate data structure that gets flushed to disk at the end of the process.&lt;/p&gt;
&lt;p&gt;Say we&#39;ve got a Python class that wraps around an individual volume file in the corpus:&lt;/p&gt;
&lt;p&gt;&lt;script src=&quot;https://gist.github.com/davidmcclure/3d45c6ca3cdd17c535e7abc3fb704356.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;This just reads the file, parses the JSON, and sets the data on the instance. Here, we&#39;ve got a &lt;code&gt;token_count()&lt;/code&gt; method, which steps through each page and adds up the total number of tokens in the book.&lt;/p&gt;
&lt;p&gt;And, similarly, say we&#39;ve got a &lt;code&gt;Manifest&lt;/code&gt; class, which wraps around a local copy of the &lt;code&gt;pd-basic-file-listing.txt&lt;/code&gt; from Hathi, which provides an index of the relative locations of each volume file inside the pairtree directory. &lt;code&gt;Manifest&lt;/code&gt; just joins the relative paths onto the location of the local copy of the features directory, and provides a &lt;code&gt;paths&lt;/code&gt; attribute with absolute paths to all 5M volumes:&lt;/p&gt;
&lt;p&gt;&lt;script src=&quot;https://gist.github.com/davidmcclure/bd4701a5b44db60089f81d40462644c0.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;To run code on the entire corpus, the simplest thing is just to loop through the paths one-by-one, make a volume instance, and then do some kind of work on it. For example, to count up the total number of tokens in all of the volumes:&lt;/p&gt;
&lt;p&gt;&lt;script src=&quot;https://gist.github.com/davidmcclure/ca85a90decc0606b293870f6713f5f9e.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;This kind of approach is often good enough. Even if it takes a couple hours on a larger set of texts, it often makes more sense to keep things simple instead of putting in the effort to speed things up, which itself takes time and tends to make code more complex. With Hathi, though, the slowness is a deal-breaker. If I point this at the complete corpus and let it run for an hour, it steps through 14,298 volumes, which is just 0.2% of the complete set of 4.8 million, meaning it would take 335 hours -- just shy of 14 days -- just to loop through the pages and add up the pre-computed token counts, let alone do any kind of expensive computation.&lt;/p&gt;
&lt;p&gt;Why so slow? Reading out the raw contents of the files is fast enough, but, once the data is in memory, there&#39;s a cost associated with decompressing the .bz2 format and then parsing the raw JSON string, which, for an entire book&#39;s worth of pages, is long. But, since neither of these steps are IO-bound -- the costly work is being done by the CPU, not the disk -- this is ripe for parallelization. Out of the gate, though, Python isn&#39;t great for parallel programming. Unlike some more recent languages like Go, for example -- which bakes &lt;a href=&quot;https://www.golang-book.com/books/intro/10&quot;&gt;concurrency primitives&lt;/a&gt; right into the core syntax of the language -- Python programs always run on a single CPU core, and the much-maligned &amp;quot;global interpreter lock&amp;quot; means that only a single thread is allowed to do work at any given moment, regardless of the resources available on the machine.&lt;/p&gt;
&lt;h4&gt;The better way -- multiple cores on the same machine&lt;/h4&gt;
&lt;p&gt;To work around this limitation, though, Python has a nice module called &lt;code&gt;multiprocessing&lt;/code&gt; that makes it easy to make use of multiple cores -- the program is duplicated into separate memory spaces on different CPUs, work is spread out across the copies, and then the results are gathered up by a controller process at the end. The API is &lt;a href=&quot;https://docs.python.org/3.5/library/multiprocessing.html&quot;&gt;fairly large&lt;/a&gt;, but it&#39;s generally easiest to use the &lt;code&gt;Pool&lt;/code&gt; class, which basically provides parallel implementations of &lt;code&gt;map&lt;/code&gt; in a couple of different flavors. For example, with the Hathi data -- we can write a worker function that takes a file path and returns a token count, and then use the &lt;code&gt;imap_unordered&lt;/code&gt; function to map this across the list of paths from the manifest:&lt;/p&gt;
&lt;p&gt;&lt;script src=&quot;https://gist.github.com/davidmcclure/a7aff2ed46406d37becb70674bc31ab6.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;This produces a really nice speedup -- now, over an hour, a 16-core node steps through 117,951 volumes, an 8x speedup from the single-process code. But, the numbers are still forbidding when scaled up to the full set of 5M volumes -- even at ~120k volumes an hour, it would still take about 40 hours to walk through the corpus. And again, this is just the bare minimum of adding up the total token count -- a more expensive task could run many times slower.&lt;/p&gt;
&lt;p&gt;Can we just keep cranking up the number of processes? In theory, yes, but once we go past the number of physical CPU cores on the machine, the returns diminish fairly quickly, and beyond a certain point the performance will actually drop, as the CPU cores start scrambling to juggle all of the processes. One solution is to find a massive computer with lots of CPUs -- Amazon Web Services, for example, now offers a gigantic &amp;quot;X1 32xlarge&amp;quot; instance with 128 cores. But this is pretty much the upper limit.&lt;/p&gt;
&lt;h4&gt;MPI -- multiple cores on multiple machines&lt;/h4&gt;
&lt;p&gt;So, only so many cores can be stuffed into a single machine -- but there isn&#39;t really a limit to the number of computers that can be stacked up next to each other on a server rack. How to write code that can spread out work across multiple computers, instead of just multiple cores?&lt;/p&gt;
&lt;p&gt;There are few different approaches to this, each making somewhat different assumptions. On the one hand there are &amp;quot;MapReduce&amp;quot; frameworks like &lt;a href=&quot;http://hadoop.apache.org/&quot;&gt;Hadoop&lt;/a&gt; and &lt;a href=&quot;http://spark.apache.org/&quot;&gt;Spark&lt;/a&gt;, which grew up around the types of large, commodity clusters that can be rented out from services like Amazon Web Services or Google&#39;s Compute Engine. In this context, the inventory is often enormous -- there are lots and lots of servers -- but it&#39;s assumed that they&#39;re connected by a network that&#39;s relatively slow and unreliable. This leads to a big focus on fault tolerance -- if a node goes offline, the job can shuffle around resources and recover. And, since it&#39;s slow to move data over a slow network, Hadoop is really invested in the notion of &amp;quot;data locality,&amp;quot; the idea that each node should always try to work on a subset of the data that&#39;s stored physically nearby in the cluster -- in RAM, on an attached disk, on another machine on the same server rack, etc.&lt;/p&gt;
&lt;p&gt;Meanwhile, there&#39;s an older approach to the problem called the &amp;quot;Message Passing Interface&amp;quot; (MPI), which is used widely in scientific and academic contexts. MPI is optimized for more traditional HPC architectures -- grids of computers wired up over networks that are fast and reliable, where data can be transferred quickly and the risk of a node going offline is smaller. MPI is also more agnostic about programming patterns than MapReduce frameworks, where it&#39;s sometimes necessary to formulate a problem in a fairly specific way to make it fit with the map-reduce model. MPI is lower-level, really just a set of primitives for exchanging data between machines.&lt;/p&gt;
&lt;p&gt;From the perspective of the programmer, MPI flattens out the distinction between different cores and different computers. Programs get run on a set of nodes in a computing cluster, and, depending on the resources available on the nodes, the program is allocated a certain number of &amp;quot;ranks,&amp;quot; which are essentially parallel copies of the program that can pass data back and forth. Generally, one rank gets mapped onto each available CPU core on each node. So, if a job runs on 32 nodes, each with 16 cores, the program would get replicated across 512 MPI ranks.&lt;/p&gt;
&lt;p&gt;Writing code for MPI was a bit confusing for me at first because, unlike something like a multiprocessing &lt;code&gt;Pool&lt;/code&gt;, which is functional at heart -- write a function, which gets mapped across a collection of data -- with MPI the distinction between code that &lt;em&gt;does work&lt;/em&gt; and code that &lt;em&gt;orchestrates work&lt;/em&gt; is accomplished with in-line conditionals that check to see which rank the program is running on. You just write a single program that runs everywhere, and that program has to figure out for itself at runtime which role it&#39;s been assigned to. MPI provides two basic pieces of information that makes this possible -- the &lt;code&gt;size&lt;/code&gt;, the total number of available ranks, and the &lt;code&gt;rank&lt;/code&gt;, a offset between 0 and &lt;code&gt;size&lt;/code&gt; that identifies this particular copy of the program. To take a trivial example -- say we&#39;ve got 5 MPI ranks, and we want to write a program to compute the square root of 4 numbers. Rank 0 -- the controller rank -- broadcasts out each of the numbers, and then ranks 1, 2, 3, and 4 each receive a number and do the computation:&lt;/p&gt;
&lt;p&gt;&lt;script src=&quot;https://gist.github.com/davidmcclure/c6b283a26c9198def30363e074ab695a.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;&lt;script src=&quot;https://gist.github.com/davidmcclure/2d446e4c9ada195458c6f747a187494f.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;Beyond this kind of simple “point to point” communication – one ranks sends some data, another receives it – MPI also has a number of synchronization utilities that make it easier to coordinate work across groups of ranks. Unlike the code above, most MPI programs have just two branches – one for rank 0, which is responsible for splitting up the task into smaller pieces of work, and another for all of the other ranks, each of which uses the same code to pull instructions from rank 0. For example, the scatter and gather utilities make it possible to split a set of input data into N pieces, “scatter” each piece out to N ranks, wait until all of the worker ranks finish their computations, and then “gather” the results back into the controller rank. Eg, for the square roots:&lt;/p&gt;
&lt;p&gt;&lt;script src=&quot;https://gist.github.com/davidmcclure/3fd74edb99f428614cdacdddf2543c64.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;This is starting to look like the kind of approach we’d want for a data set like Hathi – just replace the integers with volume paths, and the square roots with some kind of analysis on the feature data. In essence, something like:&lt;/p&gt;
&lt;p&gt;&lt;script src=&quot;https://gist.github.com/davidmcclure/f6ae97c08e599490fbba11f533bfe327.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;This works like a charm. Here’s a complete program that counts up the total number of tokens in the corpus:&lt;/p&gt;
&lt;p&gt;&lt;script src=&quot;https://gist.github.com/davidmcclure/8becd4679c99ada94f3fce05293274db.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;(Simplified just a bit for readability -- see the full version &lt;a href=&quot;https://github.com/davidmcclure/hathi-mpi/blob/master/jobs/scatter_list.py&quot;&gt;here&lt;/a&gt;, along with the benchmarking programs for all the other code in this post.)&lt;/p&gt;
&lt;p&gt;On 16 nodes on Stanford&#39;s Sherlock cluster, this runs in about 140 minutes -- ~2.3 hours -- a 18x speedup over the &lt;code&gt;multiprocessing&lt;/code&gt; solution on a 16-core machine and 145 times faster than the original single-threaded code. And, this scales roughly linearly with the number of nodes -- 32 nodes would finish in just over an hour, 64 in half an hour, etc. This approach has served us well with the first projects we&#39;ve been using the Hathi data for -- a look at the history of the word &amp;quot;literature,&amp;quot; in collaboration with a group at Paris-Sorbonne. Though, I&#39;m still new to this type of programming, and my guess is that there are ways that this could be improved pretty significantly.&lt;/p&gt;
&lt;p&gt;One question I&#39;m still unsure about -- instead of decompressing the volumes on-the-fly during jobs, would it make sense to just do this once, write the inflated files back to the disk, and then run jobs against the regular JSON? I think this would speed up the jobs themselves -- the decompression step accounts for about 40% of the time that it takes to materialize a volume. (Though, we&#39;d also be pulling more data off the filesystem, which takes time -- so I&#39;m not sure.) I haven&#39;t gone down this road, though, because it seems like there are other costs, if only in terms of data management and programming hassle. It would take up much more disk space, for one thing -- about 9 terabytes, on top of the 1.2 for the original files. And, it would mean that we&#39;d have to remember to re-run this step if Hathi updates the corpus, etc. As a rule of thumb -- I never really love the idea of creating &amp;quot;downstream&amp;quot; versions of data sets when it can be avoided, since I think it often adds surface area for mistakes and makes things harder to reproduce down the line. If the MPI-ified job can run in 2 hours against the original .bz2&#39;s, I&#39;m not sure it would be worth adding complexity to the code just to get it down to 1 hour, or whatever. I guess this might make sense if we were running lots and lots of jobs, but I doubt we&#39;ll be doing that.&lt;/p&gt;
&lt;p&gt;So, how many tokens in Hathi? We count 814,317,177,732 -- which, I have to pinch myself to remember, is 80% of a trillion. This is actually quite a bit more than the 734 billion number &lt;a href=&quot;https://www.hathitrust.org/htrc-releases-massive-dataset&quot;&gt;reported by Hathi&lt;/a&gt; back in 2015. Maybe it&#39;s grown a billion-odd words in the last year? Or, we might be counting differently -- we&#39;re just adding up the top-level &lt;code&gt;tokenCount&lt;/code&gt; keys on each page, which I believe include all the OCR errors that would get filtered out in real projects.&lt;/p&gt;
&lt;p&gt;Either way -- Hathi is a kind of Borgesian dream. More to come.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Working in the Lab, Part 1</title>
    <link href="https://litlab.stanford.edu/working-in-the-lab-part-1/"/>
    <updated>2016-12-07T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/working-in-the-lab-part-1/</id>
    <content type="html">&lt;p&gt;On my first day of work, I looked up the term &amp;quot;operationalize&amp;quot; in the dictionary. A mixture of curiosity and sheer pragmatism led me to do this; after all, the project I was about to embark on aimed to &amp;quot;operationalize time.&amp;quot; More specifically, the ultimate goal was to create a computer program that might track the progression of time in fiction (that is, in novels and short stories). I thought it wise to have at least &lt;em&gt;some&lt;/em&gt; sense of what this actually entailed. To my utter dismay, however, what I found online was not of much help. According to &lt;em&gt;Merriam-Webster&lt;/em&gt;, the transitive verb means &amp;quot;to make operational.&amp;quot; (Boy, was that instructive!) I did learn a fun, random fact, though: in terms of popularity, &amp;quot;operationalize&amp;quot; is in the bottom 30% of searched words. Filled with both frustration and excitement, I decided to heed my project leader&#39;s advice and read a Literary Lab pamphlet written by English professor (and Lit Lab founder), Franco Moretti. His fifteen-page paper, titled &amp;quot;&#39;Operationalizing&#39;: or, the function of measurement in modern literary theory,&amp;quot; clarified my doubts tremendously. (Also, an important lesson was learned: &lt;em&gt;always&lt;/em&gt; follow older and smarter people&#39;s advice.) Here is the general gist:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;quot;Operationalizing means building a bridge from concepts to measurement, and then to the world. In our case: from the concepts of literary theory, through some form of quantification, to literary texts.&amp;quot; (Moretti, 1)&lt;/p&gt;
&lt;p&gt;&amp;quot;Taking a concept, and transforming it into a series of operations.&amp;quot; (Moretti, 2)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Armed with a better understanding of &amp;quot;operationalize,&amp;quot; I was ready to start tagging my first work of fiction. I read Ernest Hemingway&#39;s &amp;quot;The Killers.&amp;quot; This is an excellent story, but before I dive into the process of tracing its advancement of time, allow me to restate, in more detailed terms, the look and feel of my research. The process is fairly straightforward: choose a novel or short story. Break it up into discrete scenes (e.g. a conversation between two characters would be separate from the description of a house). It helps to imagine what a film adaptation of the given fictional piece might look like---that is, what set of actions would constitute a single shot or a related series of them. Lastly, ascribe a time duration to every single scene, in units of minutes. Alright, now back to Hemingway. I am extremely glad his story was the first one I got to tag; the experience proved to be quite enjoyable. The narrative was linear, meaning there were no unexpected flashbacks or flash-forwards to consider. In other words, I could easily partition the tale into standalone scenes. In order to figure out the durations, I read aloud the dialogue between the characters and timed myself with my cellphone&#39;s chronometer. Unfortunately, the next stories I read---William Faulkner&#39;s &amp;quot;A Rose for Emily&amp;quot; and Henry James&#39;s &amp;quot;Jolly Corner&amp;quot;---were not as simple to tag. In the case of Faulkner, the difficulty came from the fact that most paragraphs were expository; they read as descriptions (of settings, for instance, or characters&#39; backstories) as opposed to plot advancements. Would a paragraph indicating the look of a living room account for any time in a movie? Probably not, since the living room could just be shown instantly, in a shot. The reason James&#39;s story was tricky was the abundance of Spencer Brydon&#39;s thoughts. After all, how long does a thought (or a chain of thoughts) last? More often than not, I went with my gut when tagging these works of fiction. I read Arthur Conan Doyle&#39;s &lt;em&gt;The Hound of the Baskervilles&lt;/em&gt; and Toni Morrison&#39;s &lt;em&gt;The Bluest Eye&lt;/em&gt;. For both of these novels, the breakup of scenes part was easy, while the assignment of time durations was not. Still, I am glad I took part in the experiment. As Erik Fredner pointed out to me: &amp;quot;not only is precision impossible in this context since texts are imprecise in their timing,&amp;quot; but &amp;quot;precision isn&#39;t even the goal.&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ena Alvarado is a Stanford junior from Caracas, Venezuela studying English. She may love reading and writing a little too much.&lt;/em&gt;&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Working in the Lab, Part 2</title>
    <link href="https://litlab.stanford.edu/working-in-the-lab-part-2/"/>
    <updated>2016-12-08T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/working-in-the-lab-part-2/</id>
    <content type="html">&lt;p&gt;I first became familiar with the Literary Lab when I took a class on literary text mining in R with Mark Algee-Hewitt last winter. From discussing the philosophies behind the digital humanities to constructing cluster dendrograms (plus lots of other cool graphs) of Poe&#39;s short stories, I loved the class and was excited to start working at the lab!&lt;/p&gt;
&lt;p&gt;The first project I contributed to was the Identity project, which investigates the discourse on race in American literary texts from the late 18th century until the mid-20th century. Much of my time was devoted to reading novels and replacing references to black characters&#39; names with fixed tags. I personally read and tagged &lt;em&gt;The Sound and the Fury&lt;/em&gt;, &lt;em&gt;Their Eyes Were Watching God&lt;/em&gt;, &lt;em&gt;Our Nig&lt;/em&gt;, &lt;em&gt;The Adventures of Huckleberry Finn&lt;/em&gt;, &lt;em&gt;The Conjure Woman&lt;/em&gt;, &lt;em&gt;Westward Ho!&lt;/em&gt;, &lt;em&gt;Three Lives&lt;/em&gt;, and &lt;em&gt;Uncle Remus: His Songs and Sayings&lt;/em&gt;. These character-tagged texts would later be used in a collocate analysis.&lt;/p&gt;
&lt;p&gt;The tagging seemed simple enough, but I soon ran into problems with pronouns. Should a pronoun reference have the same tag as a named reference? And moreover, should we distinguish between reflexive and personal pronoun tags? Using a single, fixed tag could streamline future analyses, but using multiple tags could lead to potentially more revelatory results. After conferring with the principal researchers, we decided to alter tags based on the type of pronoun. As I was reading through the texts, I noticed that sentences containing reflexive pronouns sometimes revealed how characters conceived of their own identities, often as they related to race. The collocate analysis done by the principal researchers produced some surprising results---for example, a common word found in collocates of reflexive pronouns was &amp;quot;value&amp;quot;.&lt;/p&gt;
&lt;p&gt;From another project, I learned first-hand that projects often run into obstacles early on, and that&#39;s valuable---it can help clarify goals and shed light on new ways of doing. About a month before I started at the lab I&#39;d been thinking a lot about nebulous genre words such as &amp;quot;postmodern&amp;quot;. I thought about doing an empirical content analysis of texts generally considered postmodern to see what their defining characteristics are according to a computer. I ran into a challenge right away---how could I construct a corpus that was free of bias? The other members of the lab and I came up with a set of 50 novels we considered postmodern, and I then verified the accuracy of the label by finding peer-reviewed articles describing the novels as postmodern. Yet all the same we were still coming up with a set of novels ourselves to fit within a corpus of arbitrary size. We did, however, decide to make a control corpus which will consist of a random set of novels from the lab&#39;s 20th century corpus with the same date distributions as those in the postmodern corpus. The randomness inherent in that corpus could control for bias better.&lt;/p&gt;
&lt;p&gt;I planned to work in R and use topic models and keywords in context. At the start, I ran a trial topic model on a subset of the corpus and found it a little hard to make sense of (if only it would clearly assign names to topics!). And I soon realized that my goals were constrained by the tools I was using. Topic models and keywords in context are useful for understanding thematic postmodernism---that is, the specific words and topics that make up postmodern texts. Yet postmodern novels often experiment with form, from employing extensive commentaries such as in &lt;em&gt;Pale Fire&lt;/em&gt; to extensive footnotes such as in &lt;em&gt;Infinite Jest&lt;/em&gt;. A text file of &lt;em&gt;Infinite Jest&lt;/em&gt; in which the footnotes blend together with the actual text could give us distorted results about the most prominent words in the novel, yet it&#39;s hard to dispute that the footnotes are essential to the experience of reading the novel. For making sense of stylistically postmodern characteristics---things often rendered best in print form---my best bet was reading the texts themselves or having knowledge of the form beforehand. After all, there&#39;s no one way of being experimental in form, and such experimentation is not necessarily easy for a computer to recognize.&lt;/p&gt;
&lt;p&gt;Going forward, I&#39;m thinking of trying out more methods in R such as clustering or most distinctive words. I&#39;m also considering revamping my corpus to only include novels with the highest number of &amp;quot;postmodern&amp;quot; affirmations in peer-reviewed articles; I suspect this could alleviate bias in the corpus. And I&#39;ll read up more on postmodern literature and theory so that I can better interpret the results in R as they compare to widely-held views. Digital tools can help maneuver wide-ranging literary questions, but it&#39;s really a solid knowledge of literature that makes all the data visualizations and topic models the most meaningful. I&#39;ll continue to learn!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sarah Thomas is a sophomore at Stanford majoring in English. In her spare time she hosts a radio show called Life Aquatic on KZSU.&lt;/em&gt;&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Working in the Lab, Part 3</title>
    <link href="https://litlab.stanford.edu/working-in-the-lab-part-3/"/>
    <updated>2016-12-09T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/working-in-the-lab-part-3/</id>
    <content type="html">&lt;p&gt;This was my sophomore summer with the Literary Lab. I started the summer ready to capitalize on my veteran knowledge and pick up where I left off. I did just that when I spent the the first weeks of summer working on what we in-house called the Identity Project, but what is officially known as &amp;quot;Representations of Race and Ethnicity in American Fiction, 1789-1964&amp;quot;. My fellow RAs from last year and I were part of the project&#39;s nascent stages. We helped gather the 193 texts for the project&#39;s handpicked corpus (this process included more OCR-ing than I&#39;d ever like to do again). But, the idea for the project was fascinating and I was excited to see how it would develop. Some of my primary academic interests are investigating race in the context of American society, media and history. When I returned this summer, the project had developed significantly. We had already garnered --- no exaggeration --- hundreds of thousands of collocates for our twelve ethnic groups. The next phase of the project, which I was primarily involved with, focused on examining how the discourse about characters who embodied our ethnic groups may or may not have differed from the general discourse surrounding these groups. My task was to read novels and tag certain black characters for collocate analysis. I personally read &lt;em&gt;Uncle Tom&#39;s Cabin&lt;/em&gt; and &lt;em&gt;Native Son.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;While my task was at times painstakingly repetitive (replacing ambiguous pronouns for specific characters with a unique ID can only be so riveting), I was excited to see the results of my work and engage in the analytical process. I sat in on my first results meeting at the lab and got a peek behind the curtain to see more of what goes into producing high level scholarly work. It was fascinating to sit in and listen to the kind of discussions that came about, like why &amp;quot;minstrel&amp;quot; was a specific collocate for &amp;quot;ethiopian&amp;quot; or how the Eastern European target group collocates consisted primarily of orchestral-related and political terms. It was also reassuring to see how the principal investigators were just as overwhelmed with the scale of results we produced as I was. The data from this project is fertile enough to produce years and years of scholarly work. Ultimately, that&#39;s what I&#39;ll take pride in the most: weeks of my effort helped to create this incredible set of information that can be a starting point for so much diverse and intriguing academic inquiry.&lt;/p&gt;
&lt;p&gt;It&#39;s actually kind of funny; when I initially applied to work at CESTA [the Center for Spatial and Textual Analysis], I applied for a specific spatial history project, but I instead found my way to the Literary Lab. I had some reservations; I was worried about being the only non-English major, woefully under-qualified to work at a literary focused group. But, with this project and my overall experience with the Lit Lab, I&#39;ve really learned that humanistic inquiry, in all its forms, is something that I have found and will always find captivating. I look forward to where the Lab takes me next!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Asha Isaacs is a junior in the psychology department at Stanford, and is fascinated by all forms of humanistic inquiry.&lt;/em&gt;&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>How many novels have been published in English? (An Attempt)</title>
    <link href="https://litlab.stanford.edu/how-many-novels-have-been-published-in-english-an-attempt/"/>
    <updated>2017-03-14T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/how-many-novels-have-been-published-in-english-an-attempt/</id>
    <content type="html">&lt;p&gt;Not for the first time, I find myself wanting to know how big the field of the novel is. Granted, finding the precise number of novels published in English is impossible. And even if we had an exact figure, the number of published novels doesn&#39;t directly address the question of the genre&#39;s cultural extent since it wouldn&#39;t account for self-publishing, personal writing shared among friends, fan fiction, etc. Nevertheless, having an approximate answer to this question seems useful for two reasons: First, I genuinely didn&#39;t know at what order of magnitude the field of the novel operates. Is the number of novels in the tens or hundreds of millions? Or is it shockingly modest---maybe just a few hundred thousand? Second, this question is worth asking because the order of magnitude matters. We know that we only study a tiny portion of the novel field, and that what we do study is deliberately nonrepresentative. Knowing the scope of our reading in comparison to the field as a whole gives us a better sense of how circumscribed our claims about &amp;quot;the novel&amp;quot; are. Asking about the &amp;quot;representativeness&amp;quot; of our samples connotes a quantitative humility.&lt;/p&gt;
&lt;p&gt;So, an attempt: According to Bowker&#39;s Books in Print, there were 2,714,409 new books printed in English in 2015.&lt;a href=&quot;https://litlab.stanford.edu/how-many-novels-have-been-published-in-english-an-attempt/#ftn1&quot;&gt;[1]&lt;/a&gt; Of these, just 221,597 (8.2%) were classified as fiction. This alone surprised me---I had always assumed that fiction controlled a significantly larger portion of publishing considering how much of the global conversation about books is driven by it. But, based on a Nielsen report,&lt;a href=&quot;https://litlab.stanford.edu/how-many-novels-have-been-published-in-english-an-attempt/#ftn2&quot;&gt;[2]&lt;/a&gt; the ratio of fiction releases to sales is not one-to-one; even though only about 8% of publishing is fiction, the category accounts for 23% of all book sales. (Also worth noting here is another surprise from that report, at least from the perspective of someone cloistered in an English department: just 47% of Americans&lt;a href=&quot;https://litlab.stanford.edu/how-many-novels-have-been-published-in-english-an-attempt/#ftn3&quot;&gt;[3]&lt;/a&gt; buy books of any kind in any format, and a huge number of them were adult coloring books last year.)&lt;/p&gt;
&lt;p&gt;Bowkers&#39;s 2015 ratio (8.2% of publishing categorized as &amp;quot;fiction&amp;quot;) does not seem to have been too far outside of the norm for the last six years:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/books-per-year-by-type.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/books-per-year-by-type.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As this chart shows, the data Bowkers collects on book sales has varied dramatically over the last sixteen years, starting with that sharp uptick in 2009-10 before declining again. It&#39;s hard to know whether that spike accurately reflects a year of unprecedented book publication, or if instead it measures a change in Bowkers&#39;s counting methodology. After all, 2009-10 seems like it would have been a bad time economically to quintuple your book printing. But it also came near the beginning of on-demand printing---those physical reprints of out-of-copyright texts by no-name publishers, sometimes literally just printing off scanned page images from Archive.org and gluing them together.&lt;a href=&quot;https://litlab.stanford.edu/how-many-novels-have-been-published-in-english-an-attempt/#ftn4&quot;&gt;[4]&lt;/a&gt; This could have greatly inflated the number of &amp;quot;new&amp;quot; books being printed, but it&#39;s hard to tell what percentage of the texts from those years and after fall into that category.&lt;/p&gt;
&lt;p&gt;Thankfully for our purposes here, the absolute variance in Bowkers&#39;s data does not particularly matter since what we need is not a count of books but rather a ratio of fiction to total print production.&lt;a href=&quot;https://litlab.stanford.edu/how-many-novels-have-been-published-in-english-an-attempt/#ftn5&quot;&gt;[5]&lt;/a&gt; During this period, fiction was never more than 16.3% (2004) nor less than 2.5% (2010) of a given year&#39;s printing. On average, about 11% of books published in a given year were fiction. Without the outlier years, that dips slightly to 10.6%.&lt;/p&gt;
&lt;p&gt;That gives us a ratio of fiction to nonfiction production within the contemporary book market. Roughly 1 in 10 books printed will be categorized &amp;quot;fiction,&amp;quot; a set that contains a range of materials, including novel-length literary fiction, novellas, short story collections, young adult novels, romance, science fiction, fantasy, translations, etc.&lt;/p&gt;
&lt;p&gt;To the best of my knowledge we lack a reliable means of estimating historical fiction/nonfiction print ratios. So, my first major assumption will be to map the average contemporary ratio of fiction to total print production from Bowkers onto a measure of total print production. Clearly this will produce a *very *rough result. But if we can assume that the contemporary moment reflects an average or lower-than-average ratio of fiction to nonfiction print production, then using the current ratio will point us toward a larger goal of this exercise: estimating the number of novels in English without overshooting the mark.&lt;/p&gt;
&lt;p&gt;Given the contemporary ratio of fiction to nonfiction in print, we now need to know how many printed books we ought to be considering. If you filter Google Books for English language works today, the search engine returns an estimated 189 million books, a 146% increase from Google&#39;s 2010 estimate of total extant volumes globally.&lt;a href=&quot;https://litlab.stanford.edu/how-many-novels-have-been-published-in-english-an-attempt/#ftn6&quot;&gt;[6]&lt;/a&gt; Of course, this too underestimates the field since it presumably only references the collections Google has access to. Applying the ratio of fiction production we derived from Bowker&#39;s to this measure of total published output would leave us about 18.9 million books in English that Bowkers would categorize as fiction.&lt;a href=&quot;https://litlab.stanford.edu/how-many-novels-have-been-published-in-english-an-attempt/#ftn7&quot;&gt;[7]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&amp;quot;Books&amp;quot; turns out to be a key word in that last sentence. Bowkers&#39;s &amp;quot;Books in Print&amp;quot; is precisely what it sounds like: a database about books published, not works written. Because it privileges the book-as-product, the dataset does not allow us to easily differentiate between new novels and new versions of an extant novel. I use &amp;quot;versions&amp;quot; rather than &amp;quot;editions&amp;quot; or &amp;quot;printings&amp;quot; advisedly since Bowkers tracks paper-and-ink books, on-demand printing, digital copies of trade books, Kindle Direct Publishing (Amazon&#39;s self-publishing wing), etc. Localization also plays a major role in counting: &lt;em&gt;The Color Purple&lt;/em&gt; and &lt;em&gt;The Colour Purple&lt;/em&gt; count as two books, though I&#39;m quite sure we would think of them as one novel. Worse, there is no way to readily decide from the metadata whether two different editions of a book titled &lt;em&gt;The Portrait of a Lady&lt;/em&gt; both contain the same Henry James novel. We could assume based on a set of fuzzy title-author matches, but that becomes immediately ambiguous: Could we set parameters to reliably find that an item titled *The Portrait of a Lady: A Novel *is the same as &lt;em&gt;Portrait&lt;/em&gt; &lt;em&gt;of a Lady&lt;/em&gt;? Or, as a more genuine question of literary history rather than one about metadata, should we count *The Portrait of a Lady: New York Edition *as the same novel as &lt;em&gt;The Portrait of a Lady&lt;/em&gt;?&lt;a href=&quot;https://litlab.stanford.edu/how-many-novels-have-been-published-in-english-an-attempt/#ftn8&quot;&gt;[8]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To suss out the contents of the fiction category we need a sample of the total texts. I copied the first 500 records (the max allowed by Bowkers) from the 2015 fiction works, sorted by the date the record was last updated. Of the sorting options, this seemed to offer the greatest degree of randomness, though a truly random sample from the 222,686 records would of course have been much better.&lt;/p&gt;
&lt;p&gt;Reading through those records, I only recognized a few by title: &lt;em&gt;Blood Meridian&lt;/em&gt; (McCarthy), &lt;em&gt;Plainsong&lt;/em&gt; (Haruf), &lt;em&gt;The Diaries of Adam and Eve&lt;/em&gt; (Twain), &lt;em&gt;The Savage Detectives&lt;/em&gt;, and* 2666*. The fact that these last two are by Bolaño shows one clear limit of relying on date edited as a randomizing field.&lt;/p&gt;
&lt;p&gt;To give you a sense of the range, here are a few other titles from that group:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;The Book that Proves Time Travel Happens&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Rio de Janeiro! #5&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Chicken and Pickle: Get a Baby&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Everything is Teeth&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A huge number of books on the list were movie and television tie-ins from franchises like &lt;em&gt;Star Wars&lt;/em&gt;, &lt;em&gt;The Princess Diaries&lt;/em&gt;, &lt;em&gt;The Minions&lt;/em&gt;, &lt;em&gt;The Avengers&lt;/em&gt;, &lt;em&gt;Walking Dead&lt;/em&gt;, &lt;em&gt;Shrek&lt;/em&gt;, &lt;em&gt;Madagascar&lt;/em&gt;, and &lt;em&gt;Doctor Who&lt;/em&gt;, among others. But there were also a few other titles that had been classed as fiction, but seem to be &lt;em&gt;about&lt;/em&gt; fiction rather than fiction themselves:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Japanese Science Fiction: Views of a Changing Society&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;The Transhuman Antihero: Split-Natured Protagonists in Speculative Fiction from Mary Shelley to Richard Morgan&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;The Angel and the Cad: Love, Loss and Scandal in Regency England&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of those 500 items, 211 (42%) were duplicate entries referencing the same work (i.e. *Colour Purple */ &lt;em&gt;Color Purple&lt;/em&gt;). Duplicate entries seem to primarily be the result of localizations and book type (hardcover vs. softcover vs. ebook). If we subtract those duplicates and the titles like the ones above about fiction rather than fictional works themselves, that leaves us with 285 possibilities. Of those, if we cut from there with a top-level BISAC code of Fiction, we&#39;re left with 128 possible novels. This seems reasonable if we&#39;re interested in the novel as distinct from &amp;quot;juvenile fiction&amp;quot; (275 of the items in the sample).&lt;/p&gt;
&lt;p&gt;The genre breakdown of that group in this sample is as follows:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/bisac-fiction-in-sample-by-first-sub-cat.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/bisac-fiction-in-sample-by-first-sub-cat.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A few items to note about this chart. BISAC Fiction includes a wide range of categories not represented in this sample.&lt;a href=&quot;https://litlab.stanford.edu/how-many-novels-have-been-published-in-english-an-attempt/#ftn9&quot;&gt;[9]&lt;/a&gt; This includes non-novel fiction like short stories, anthologies, classics (as in Greek tragedy, not Penguin Classics), etc. So if we assume that the ratio of BISAC Fiction to Bowkers Fiction holds over the set, there would still be some percentage of non-novel Fictions, though they do seem to be rare. The subcategory also includes many forms of genre fiction, which, taken together, outweigh so-called General fiction, frequently the label used for literary fiction. Notably, the BISAC Code Fiction / Literary did not appear once in the sample, though some literary fictions did (e.g. *Blood Meridian *was categorized as Western, &lt;em&gt;2666&lt;/em&gt; as General, etc.)&lt;/p&gt;
&lt;p&gt;Based on this sample, roughly 25% of everything Bowkers categorized as Fiction could possibly be a novel. That takes the initial figure of 18.9 million possible fictional works down to 4.8 million. If cutting that proportion to get from &amp;quot;books&amp;quot; to &amp;quot;works&amp;quot; seems outlandish, consider this: According to Bowkers&#39;s, Zora Neale Hurston, James Joyce, and Henry James published a combined 123 books of fiction in 2015, 55 years after the youngest of them died. Over the time-period that Bowkers&#39;s full database covers, Henry James has been listed as the author of 14,829 books of fiction. I repeat: Based on the way Bowkers counts, Henry James &amp;quot;authored&amp;quot; 14,829 books since the 1990s. Compared to the 23 novels included in the &lt;em&gt;Library of America&lt;/em&gt;&#39;s complete printing of James&#39;s novels, the ratio of unique book-length works by James to books attributed to James is about 0.16%.&lt;/p&gt;
&lt;p&gt;Among novelists who have had large numbers of unique books printed, James seemed to me like he ought to rank quite highly considering his prolific output. I was curious who, if anyone, had more books to their name, so I looked up some of the highly ranked names from the Literary Lab&#39;s Popularity/Prestige project:&lt;a href=&quot;https://litlab.stanford.edu/how-many-novels-have-been-published-in-english-an-attempt/#ftn10&quot;&gt;[10]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/books-attributed-to-author.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/books-attributed-to-author.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Predictably, Shakespeare appears at the top of the list. But quite a few of these placements surprised me: Twain has been printed far more than other 19th century novelists like Austen, Eliot, and Melville, who also wrote a fairly large number of books. Faulkner and Hemingway appear surprisingly far down the list, being published more like J.K. Rowling than Fitzgerald, Woolf, and Cather.&lt;/p&gt;
&lt;p&gt;To get at the problem of the &amp;quot;unique novel&amp;quot; we need to cut aggressively against the 4.8 million printed novels proposed earlier to account for authors with many print runs, but here we run to the end of our rope. If we had the data, we could get novel counts for a list of highly printed novelists against their total number of novels and subtract overprinting from the total.&lt;/p&gt;
&lt;p&gt;But running up against this blockade (or, rather, the inflection point between diminishing returns and increasingly dubious assumptions) allowed me to pause and reflect on what we have learned at this point. We&#39;re within an order of magnitude: the total number of novels in English is closer to 5 million than 500,000 or 50 million. We also know that the floor is in the hundreds of thousands since the Library of Congress holds more than 207,000 fiction items&lt;a href=&quot;https://litlab.stanford.edu/how-many-novels-have-been-published-in-english-an-attempt/#ftn11&quot;&gt;[11]&lt;/a&gt; and the British Library returns over 390,000 books containing &amp;quot;fiction&amp;quot; anywhere in the object description.&lt;a href=&quot;https://litlab.stanford.edu/how-many-novels-have-been-published-in-english-an-attempt/#ftn12&quot;&gt;[12]&lt;/a&gt; For &amp;quot;novel,&amp;quot; those numbers are 139,000 and 66,000 respectively---surprisingly small considering the size of the corpora we have become accustomed to working with in the Lab.&lt;/p&gt;
&lt;p&gt;I also reflected on whether and how this figure relates to my initial question, considering the data that is actually available. I started off this post by asking, &amp;quot;How many novels have been published in English?&amp;quot; Based on the data I wound up using in the attempt, I rewrote the initial question to see what this process actually &amp;quot;answered.&amp;quot; This is as close as I got: &amp;quot;Based on a ratio of fiction to nonfiction production---derived in part from a sampling methodology that selected for works that are likely novels that is then then applied to a rough measure of total publication and subselected against non-novel fictions---how many novels have been published in English, within an order of magnitude?&amp;quot; Self-flagellation by qualification.&lt;/p&gt;
&lt;p&gt;Imprecise, presentist, and biased toward the published and the archived as it may be, what does having an order of magnitude tell us about the genre of the novel?&lt;/p&gt;
&lt;p&gt;To answer that question, it helps to think about that number from the perspective of the reader who opened this essay. If you were something quite a bit more than voracious in your reading, and managed to get through a new novel every day for 50 years without letup, you would have read more than 18,000 &amp;quot;loose, baggy monsters,&amp;quot; which is 8% of our lowest estimate and 0.3% of the highest. Literary critics, by contrast with this imagined reader, might know 200 novels quite well, giving them purchase on somewhere between 0.1% and 0.004% of the field. Even as we specialize by nation and century, the comprehensiveness of critics&#39; reading only increases by portions of a percent.&lt;a href=&quot;https://litlab.stanford.edu/how-many-novels-have-been-published-in-english-an-attempt/#ftn13&quot;&gt;[13]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The question that emerges (and one that cannot be addressed in this space) is whether so little is, in fact, enough. The books that get read in literature departments may exist a space marginal to the marketplace of books that Bowkers tallies, but not in one entirely disconnected from it. When you&#39;re trying to understand a phenomenon like the novel from a sample that&#39;s both that small and &lt;em&gt;deliberately&lt;/em&gt; nonrepresentative, does knowing its broadest dimension oblige us to ask about the other 99.9%?&lt;a href=&quot;https://litlab.stanford.edu/how-many-novels-have-been-published-in-english-an-attempt/#ftn14&quot;&gt;[14]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;ftnref1&quot;&gt; [1] This methodology expands on the data set used in Figure 1 of Pamphlet 8, &amp;quot;Between Canon and Corpus&amp;quot; and continues an interrogation from the section on archival bias and representativeness broached in Pamphlet 11, &amp;quot;Canon/Archive.&amp;quot; Lastly, it extends a few ideas from a post by Matthew Wilkens: &lt;a href=&quot;https://mattwilkens.com/2009/10/14/how-many-novels-are-published-each-year/&quot;&gt;https://mattwilkens.com/2009/10/14/how-many-novels-are-published-each-year/&lt;/a&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Queries were performed at &lt;a href=&quot;http://www.booksinprint.com/&quot;&gt;http://www.booksinprint.com/&lt;/a&gt; in December of 2016 and should be reproducible with small variance from changes in the database since.&lt;/p&gt;
&lt;p&gt;Query structure: Date range 2015-01-01 to 2015-12-31; Language: English. The same date and language filters were applied to each search (changing the years as needed), and adding or removing the Fiction book type.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;ftnref2&quot;&gt;[2] Page 32 of &lt;a href=&quot;https://quantum.londonbookfair.co.uk/RXUK/RXUK_PDMC/documents/9928_Nielsen_Book_Research_In_Review_2015_The_London_Book_Fair_Quantum_Conference_2016_DIGITAL_FINAL.pdf?v=635995987941118341&quot;&gt;this report&lt;/a&gt;.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;ftnref3&quot;&gt;[3] Unfortunately, the Nielsen report doesn&#39;t disaggregate the category of &amp;quot;Americans&amp;quot; to help us better understand this alarming statistic. For instance, does &amp;quot;Americans&amp;quot; include young children who likely don&#39;t buy anything?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;ftnref4&quot;&gt;[4] See, as an example of the on-demand publishing form, the &amp;quot;Paige M. Gutenberg&amp;quot; machine in the Harvard bookstore: http://www.harvard.com/clubs_services/books_on_demand/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;ftnref5&quot;&gt;[5] Notably, the fiction/print ratio in the first five years of this dataset is significantly higher than in the last five years. Hard to say if this reflects a shift in the publishing industry or Bowkers&#39;s counting.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;ftnref6&quot;&gt;[6] Query performed in November, 2016. As of now on the Google frontend, you cannot filter the Google Books corpus for works tagged by language without a character-based query, so this number is an estimate of books in English containing the word &amp;quot;the.&amp;quot; As the most common word in English, &amp;quot;the&amp;quot; should put us within the margin of error, though there exist English books without it (e.g. Gilbert Adair&#39;s 1995 translation of Georges Perec&#39;s &lt;em&gt;A Void&lt;/em&gt;, a novel written entirely without the letter &amp;quot;e&amp;quot; both in Perec&#39;s French and Adair&#39;s English.)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;ftnref7&quot;&gt;[7] This is quite crude, though changing to an average ratio over the short period of time covered by Bowkers does not seem better.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;ftnref8&quot;&gt;[8] For those not familiar, in his later years James edited many of his novels for release in a set known as &lt;em&gt;The New York Edition&lt;/em&gt;. Many novels underwent fairly substantial changes during the editing process (frequently for the worse in the view of some James scholars).&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;ftnref9&quot;&gt;[9] http://bisg.org/page/Fiction&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;ftnref10&quot;&gt;[10] It should be pointed out here that this chart represents all books printed and documented by Bowkers&#39;s in all languages for a given author, not just English.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;ftnref11&quot;&gt;[11] https://www.loc.gov/search/index/subject/?q=the&amp;amp;all=true&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;ftnref12&quot;&gt;[12] &lt;a href=&quot;http://explore.bl.uk/primo_library/libweb/action/search.do?dscnt=0&amp;amp;vl(10130439UI0)=any&amp;amp;vl(drStartDay4)=00&amp;amp;vl(drEndMonth4)=00&amp;amp;scp.scps=scope%3A%28BLCONTENT%29&amp;amp;tab=local_tab&amp;amp;dstmp=1482018469320&amp;amp;srt=rank&amp;amp;mode=Advanced&amp;amp;vl(drEndDay4)=00&amp;amp;vl(1UIStartWith1)=contains&amp;amp;tb=t&amp;amp;indx=1&amp;amp;vl(41497491UI2)=any&amp;amp;vl(freeText0)=fiction&amp;amp;fn=search&amp;amp;vid=BLVU1&amp;amp;vl(freeText2)=&amp;amp;vl(drEndYear4)=Year&amp;amp;frbg=&amp;amp;ct=search&amp;amp;vl(drStartMonth4)=00&amp;amp;vl(10130438UI1)=desc&amp;amp;vl(1UIStartWith2)=contains&amp;amp;dum=true&amp;amp;vl(1UIStartWith0)=contains&amp;amp;vl(46690061UI3)=all_items&amp;amp;Submit=Search&amp;amp;vl(freeText1)=&amp;amp;vl(drStartYear4)=Year&quot;&gt;http://explore.bl.uk/primo_library/libweb/action/search.do?dscnt=0&amp;amp;vl(10130439UI0)=any&amp;amp;vl(drStartDay4)=00&amp;amp;vl(drEndMonth4)=00&amp;amp;scp.scps=scope%3A(BLCONTENT)&amp;amp;tab=local_tab&amp;amp;dstmp=1482018469320&amp;amp;srt=rank&amp;amp;mode=Advanced&amp;amp;vl(drEndDay4)=00&amp;amp;vl(1UIStartWith1)=contains&amp;amp;tb=t&amp;amp;indx=1&amp;amp;vl(41497491UI2)=any&amp;amp;vl(freeText0)=fiction&amp;amp;fn=search&amp;amp;vid=BLVU1&amp;amp;vl(freeText2)=&amp;amp;vl(drEndYear4)=Year&amp;amp;frbg=&amp;amp;ct=search&amp;amp;vl(drStartMonth4)=00&amp;amp;vl(10130438UI1)=desc&amp;amp;vl(1UIStartWith2)=contains&amp;amp;dum=true&amp;amp;vl(1UIStartWith0)=contains&amp;amp;vl(46690061UI3)=all_items&amp;amp;Submit=Search&amp;amp;vl(freeText1)=&amp;amp;vl(drStartYear4)=Year&lt;/a&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;ftnref13&quot;&gt;[13] Randall Munroe of xckd has also addressed this question from the perspective of reading a subset of authors rather than a genre like the novel, finding that if you read 16 hours a day, you could keep up with &amp;quot;400 living Isaac Asimovs:&amp;quot; https://what-if.xkcd.com/76/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;ftnref14&quot;&gt;[14] Or: Wouldn&#39;t Sisyphus have wanted to know how high the hill was?&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Distributions of words across narrative time in 27,266 novels</title>
    <link href="https://litlab.stanford.edu/distributions-of-words-27k-novels/"/>
    <updated>2017-07-10T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/distributions-of-words-27k-novels/</id>
    <content type="html">&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/how-often.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/how-often.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Over the course of the last few months here at the Literary Lab, I&#39;ve been working on a little project that looks at the distributions of individual words inside of novels, when averaged out across lots and lots of texts. This is incredibly simple, really -- the end result is basically just a time-series plot for a word, similar to a historical frequency trend. But, the units are different -- instead of historical time, the X-axis is what Matt Jockers calls &amp;quot;narrative time,&amp;quot; the space between the beginning and end of a book.&lt;/p&gt;
&lt;p&gt;In a certain sense, this grew out of a project I worked on a couple years ago that did something similar in the context of an individual text -- I wrote a program called &lt;a href=&quot;http://dclure.org/essays/mental-maps-of-texts/&quot;&gt;Textplot&lt;/a&gt; that tracked the positions of words inside of novels and then found words that &amp;quot;flock&amp;quot; together, that show up in similar regions of the narrative. This got me thinking -- what if you did this with lots of novels, instead of just one? Beyond any single text -- are there general trends that govern the positions of words inside of novels at a kind of narratological level, split away from any particular plot? Or would everything wash out in the aggregate? Averaged out across thousands of texts -- do individual words rise and fall across narrative time, in the way they do across historical time? If so -- what&#39;s the &amp;quot;shape&amp;quot; of narrative, writ large?&lt;/p&gt;
&lt;p&gt;This draws inspiration, of course, from a bunch of really interesting projects that have looked at different versions of this question in the last couple years. Most well-known is probably Matt Jockers&#39; &lt;a href=&quot;http://www.matthewjockers.net/2015/02/02/syuzhet/&quot;&gt;Syuzhet&lt;/a&gt;, which tracks the fluctuation of &amp;quot;sentiment&amp;quot; across narrative time, and then clusters the resulting trend lines into a set of archetypal shapes. Andrew Piper, &lt;a href=&quot;http://muse.jhu.edu/article/581921&quot;&gt;writing in New Literary History&lt;/a&gt;, built a model of the &amp;quot;conversional&amp;quot; narrative -- based on the &lt;em&gt;Confessions&lt;/em&gt;, in which there&#39;s a shift in the semantic register at the moment of conversion -- and then traced this signature forward through literary history. And, here at Stanford, a number of projects have looked at the movement of different types of literary &amp;quot;signals&amp;quot; across the X-axis of narrative. The suspense project has been using a neural network to score the &amp;quot;suspensefulness&amp;quot; of chunks of novels; in &lt;a href=&quot;https://litlab.stanford.edu/LiteraryLabPamphlet7.pdf&quot;&gt;Pamphlet 7&lt;/a&gt;, Holst Katsma traces the &amp;quot;loudness&amp;quot; of speech across chapters in &lt;em&gt;The Idiot&lt;/em&gt;; and Mark Algee-Hewitt has looked at the dispersion of words related to the &amp;quot;sublime&amp;quot; across long narratives.&lt;/p&gt;
&lt;p&gt;Methodologically, though, the closest thing is probably Ben Schmidt&#39;s fascinating &amp;quot;&lt;a href=&quot;http://sappingattention.blogspot.com/2014/12/fundamental-plot-arcs-seen-through.html&quot;&gt;Plot Arceology&lt;/a&gt;&amp;quot; project, which does something very similar except with movies and TV shows -- Schmidt trained a topic model on a corpus of screenplays and then traced the distributions of topics across the scripts, finding, among other things, the footprint of the prototypical cop drama, a crime at the beginning and a trial at the end. I was fascinated by this, and immediately started to daydream about what it would look like to replicate it with a corpus of novels. But, picking up where Textplot left off -- what could be gained by taking a kind of zero-abstraction approach to the question and just looking at individual words? Instead of starting with a higher-order concept like sentiment, suspense, loudness, conversion, sublimity, or even topic -- any of which may or may not be the most concise way to hook onto the &amp;quot;priors&amp;quot; that sit behind narratives -- what happens if you start with the smallest units of meaning, and build up from there?&lt;/p&gt;
&lt;p&gt;It&#39;s sort of the lowest-level version of the question, maybe right on the line where literary studies starts to shade into corpus linguistics. Which words are most narratologically &amp;quot;focused,&amp;quot; the most distinctive of beginnings, endings, middles, ends, climaxes, denouements? How strong are the effects? Which &lt;em&gt;types&lt;/em&gt; of words encode the most information about narrative structure -- is it just the predictable stuff like &amp;quot;death&amp;quot; and &amp;quot;marriage,&amp;quot; or does it sift down into the more architectural layers of language -- articles, pronouns, verb tenses, punctuation, parts-of-speech? Over historical time -- do words &amp;quot;move&amp;quot; inside of narratives, migrating from beginnings to middles, middles to ends, ends to beginnings? Or, to pick up on a question &lt;a href=&quot;https://twitter.com/Ted_Underwood/status/873884672383647744&quot;&gt;Ted Underwood posed recently&lt;/a&gt; -- would it be possible to use this kind of word-level information to build a predictive model of narrative sequence, something that could reliably &amp;quot;unshuffle&amp;quot; the chapters in a book? It&#39;s like a &amp;quot;basic science&amp;quot; of narratology -- if you just survey the interior axis of literature in as simple a way as possible, what falls out?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Stacking texts&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One nice thing about this question is that the feature extraction step is really easy -- we just need to count words in a particular way. At the lowest level -- for a given word in a single text, we can compute its &amp;quot;dispersion&amp;quot; across narrative time, the set of positions where the word appears. For example, &amp;quot;death&amp;quot; in Moby Dick:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/death-moby-dick.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/death-moby-dick.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So, just eyeballing it, maybe “death” leans slightly towards the end, especially with that little cluster around word 220k? Once we can do this with a single text, though, it’s easy to merge together data from lots of texts. We can just compute this over and over again for each novel, map the X-axis onto a normalized 0-1 scale, and then stack everything up into a big vertical column. For example, in 100 random novels:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/death-100.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/death-100.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Or, in 1,000:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/death-1000.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/death-1000.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And then, to merge everything together, we can just sum everything up into a big histogram, basically – split the X-axis into 100 evenly-spaced bins, and count up the number of times the word appears in each column. It’s sort of like one of those visualizations of probability distributions where marbles get dropped into different slots – each word in each text is a marble, and its relative position inside the text determines which slot the marble goes into. At the end, everything stacks up into a big multinomial distribution that captures the aggregate dispersion of the word across lots of texts. Eg, from the sample of 1,000 above:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/death-1000-bars.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/death-1000-bars.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This can then be expanded to all 27,266 texts in our corpus of American novels – 18,177 from Gale’s American Fiction corpus (1820-1930), and another 9,089 from the Chicago Novel Corpus (1880-2000), which together weigh in at about 2.5 billion words. For death:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/death-27k.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/death-27k.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So it’s true! Not a surprise, but useful as a sanity check. Now, with “death,” it happened that we could already see a sort of blurry, pixelated version of this just with a handful of texts. The nice thing about this, though, is that it also becomes possible to surface well-sampled distributions even for words that are much more infrequent, to the degree that they don’t appear in any individual text with enough frequency to infer any kind of general tendency. (By Zipf’s law, this is a large majority of words, in fact – most words will just appear a handful of times in a given novel, if at all.) For example, take a word like “athletic,” which appears exactly once in Moby Dick, about half-way through:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/athletic-moby-dick.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/athletic-moby-dick.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And, even in the same sample of 1,000 random novels, the data is still very sparse:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/athletic-1000.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/athletic-1000.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you squint at this, maybe you can see something, but it’s hard to say. With the full corpus, though, the clear trend emerges:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/athletic-27k.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/athletic-27k.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&amp;quot;Athletic&amp;quot; -- along with a great many words used to describe people, as we&#39;ll see shortly -- concentrates really strongly at the beginning of the novel, where characters are getting introduced for the first time. If someone is athletic, it tends to get mentioned the first time they appear, not the second or third or last.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Significance, &amp;quot;interestingness&amp;quot;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Are these trends significant, though? For &amp;quot;death&amp;quot; and &amp;quot;athletic&amp;quot; they certainly look meaningful, just eyeballing the histograms. But what about for a word like &amp;quot;irony&amp;quot;:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/irony.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/irony.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Which looks much more like a flat line, with some random noise in the bin counts? If we take the null hypothesis to be the uniform distribution – a flat line, no relationship between the position in narrative time and the observed frequency of the word – then the simple way to test for significance is just a basic chi-squared test between the observed distribution and the expected uniform distribution, where the frequency of the word is spread out evenly across the bins. For example, “death” appears 593,893 times in the corpus, so, under the uniform assumption, we’d expect each of the 100 bins to have 1/100th of the total count, ~5,938 each. For “death,” then, the chi-squared statistic between this theoretical baseline and the observed counts is ~16,925, and the pvalue is so small that it literally rounds down to 0 in Python. For “athletic,” chi-squared is ~1,467, with p also comically small at 3.15e-242. Whereas, for “irony” – chi-squared is 98, with p at 0.49, meaning we can’t say with confidence that there’s any kind of meaningful trend.&lt;/p&gt;
&lt;p&gt;Under this, it turns out that almost all words that appear with any significant frequency are non-uniform. If we skim off the set of all words that appear at least 10,000 times in the corpus (of which there are 10,908 in total) – of these, 10,513 (96%) are significantly different from the uniform distribution at p&amp;lt;0.05, 10,227 (94%) at p&amp;lt;0.01, and 9,815 (90%) at p&amp;lt;0.001. And even if we crank the exponent down quite a lot - at p&amp;lt;1e-10, 7,830 (72%) words still reject the null hypothesis. What to make of this? I guess there&#39;s some kind of very high (or low) level insight here - narrative structure does, in fact, manifest at the level of individual words? Which, once the numbers pop up in the Jupyter notebook, is one of those things that seems either interesting or obvious, depending on how you look at it. But, from an interpretive standpoint, the question becomes - if everything is significant, then which words are the most significant? Which words are the most non-uniform, the most surprising, which encode the most narratological information? Where to focus attention? How to rank the words, essentially, from most to least interesting?&lt;/p&gt;
&lt;p&gt;This seemed like a simple question at first, but over the course of the last couple months I’ve gone around and around on it, and I’m still not confident that I have a good way of doing this. I think the problem, basically, is that this notion of “interestingness” or “non-uniformity” is actually less cut-and-dry that it seemed to me at first. Specifically, it’s hard to meaningfully compare words with extremely different overall frequencies in the corpus. It’s easy to compare, say, “sleeping” (which is basically flat) and “ancient” (which has a huge spike at the beginning), both of which show up right around 100,000 times. But, it’s much harder to compare “ancient” and “the,” which appears over 100 million times, and represents slightly less than 5% of all words in all of the novels. This starts to feel sort of apples-to-oranges-ish, in various ways (more on this later), and is further confounded by the fact that the data gets so sparse at the very top of the frequency chart – since a small handful of words have exponentially larger frequencies than everything else, it’s harder to say what you’d “expect” to see with those words, since there are so few examples of them.&lt;/p&gt;
&lt;p&gt;To get a sense of how this plays out across the whole dictionary – one very simple way to quantify the “non-uniformity” of the distributions is just to take the raw variance of the bin counts, which can then be plotted against frequency:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/variance-linear.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/variance-linear.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is unreadable with the linear scales because the graph gets dominated by a handful of function words; but, on a log-log scale, a fairly clean power law falls out:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/variance-log-log.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/variance-log-log.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;But, with that really wide vertical “banding,” which is basically the axis of surprise that we care about – at any given word frequency on the X-axis, words at the bottom of the band will have comparatively “flat” distributions across narrative time, and words at the top will have the most non-uniform / narratologically interesting distributions.&lt;/p&gt;
&lt;p&gt;The nice thing about just using the raw variance to quantify this is that it’s easy to compare it against what you’d expect, in theory, at any given level of word frequency. For a multinomial distribution, the variance you’d expect to see for the count in any one of the bins is n * p (1-p), where n is the number of observations (the word count, in this context) and p is 1/100, the probability that a word will fall into any given bin under the uniform assumption. Almost all of the words fall above this line:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/variance-exp.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/variance-exp.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Which, to loop back, corresponds to the fact that almost all words, under the chi-squared test, appear not to come from the uniform distribution – almost all have higher variances than you’d expect if they were uniformly distributed.&lt;/p&gt;
&lt;p&gt;I’m unsure about this, but – to get a crude rank ordering for the words in a way that (at least partially) controls for frequency, I think it’s reasonable just to divide the observed variances by the expected value at each word’s frequency, which gives a ratio that captures how many times greater a word’s actual variance is than what you’d expect if it didn’t fluctuate at all across narrative time. (Mathematically, this is basically equivalent to the original chi-squared statistic.)&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/variance-bands.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/variance-bands.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So, then, if we skim off the top 500 words:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/variance-500.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/variance-500.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Does this make sense, statistically? I don’t love the fact that it still correlates with frequency – almost all of the highest-frequency words make the cut. But that might also just be surfacing something true about the high frequency words, which do clearly rise up higher above the line. (Counterintuitively?) There are other ways of doing this – namely, if you flatten everything out into density functions – that reward low-frequency words that have the most “volatile” or “spiky” distributions. But, these are problematic in other ways – this is a rabbit hole, which I’ll try circle back to in a bit.&lt;/p&gt;
&lt;p&gt;Anyway, under this (imperfect) heuristic, here are the 500 most narratologically non-uniform words, ordered by the “center of mass” of the distributions across narrative time, running from the beginning to the end. There’s way too much here to look at all at once, but, in the broadest sense, it looks like – beginnings are about youth, education, physical size, (good) appearance, color, property, hair, and noses? And endings – forgiveness, criminal justice, suffering, joy, murder, marriage, arms, and hands? And the middle? I gloss it as a mixture of dialogism and psychological interiority – “say,” “think,” “feel,” and all the contractions and dialog punctuation marks – though it’s less clear-cut.&lt;/p&gt;
&lt;p&gt;The really wacky stuff, though, is in the stopwords – which deserve their own post.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/500-1.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/500-1.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The feature extraction code for this project can be found &lt;a href=&quot;https://github.com/davidmcclure/literary-interior/tree/shuffle&quot;&gt;here&lt;/a&gt;, and the analysis code is &lt;a href=&quot;https://github.com/davidmcclure/lint-analysis&quot;&gt;here&lt;/a&gt;. Thanks Mark Algee-Hewitt, Franco Moretti, Matt Jockers, Scott Enderle, Dan Jurafsky, Christof Schöch, and Ryan Heuser for helping me think through this project at various stages. Any mistakes are mine!&lt;/em&gt;&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>A hierarchical cluster of words across narrative time</title>
    <link href="https://litlab.stanford.edu/hierarchical-cluster-across-narrative-time/"/>
    <updated>2017-07-31T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/hierarchical-cluster-across-narrative-time/</id>
    <content type="html">&lt;p&gt;I wanted to pick back up quickly with that list of the 500 most &amp;quot;non-uniform&amp;quot; words at the end of the &lt;a href=&quot;https://litlab.stanford.edu/distributions-of-words-27k-novels/&quot;&gt;last post about word distributions across narrative time in the American novel corpus&lt;/a&gt;. Before, I just put these into a big ordered list, arranged by the center of mass of each word&#39;s distribution, which gives a pretty good sense of the conceptual gradient from beginning to end. But, it&#39;s easy to see that the center-of-mass metric is papering over some interesting differences. For example, &amp;quot;girls&amp;quot; sits right next to &amp;quot;brick,&amp;quot; both of which are clearly beginning words, in the sense that they&#39;re high at the beginning and low at the end:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/girls-brick.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/girls-brick.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;But, in terms of the actual shape of the distribution, “girls” looks much closer to “manners,” which sits nine positions to the left, or “liked,” seventeen positions to the right:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/manners-liked.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/manners-liked.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&amp;quot;Brick&amp;quot; has a huge spike at the very beginning, presumably being used in the context of describing houses, but then is basically flat across the rest of the text; whereas &amp;quot;girls&amp;quot; maybe actually peaks just &lt;em&gt;after&lt;/em&gt; the beginning, and then fall off linearly across the narrative. This seems like a meaningful difference -- I&#39;d suppose, &amp;quot;girls&amp;quot; is somehow contributing less to the work of &amp;quot;world-building,&amp;quot; the process of rendering the fictional world into existence at the very start, and marking something else. (But what?)&lt;/p&gt;
&lt;p&gt;How to hook onto these kinds of distinctions more precisely? Beyond the (literally) one-dimensional notion of a &amp;quot;center&amp;quot; or &amp;quot;mean&amp;quot; of a word, how to slice the dictionary into groups and cohorts, little rivulets that run through the prototypical narrative? To circle back to the original question from Textplot, back in 2014, but now applied to a big stack of 27k novels instead of just one -- which words &amp;quot;flock&amp;quot; together across narrative time, ebb and flow in the most exactly similar patterns? Again, at a conceptual level, this is largely a replication of &lt;a href=&quot;http://sappingattention.blogspot.com/2014/12/fundamental-plot-arcs-seen-through.html&quot;&gt;Ben Schmidt&#39;s analysis of the distributions of topic models across screenplays&lt;/a&gt;, and &lt;a href=&quot;https://mimno.infosci.cornell.edu/novels/plot.html&quot;&gt;David Mimno&#39;s experiments with plotting topic models across novels&lt;/a&gt; (using data from Matt Jockers&#39; &lt;em&gt;Macroanalysis&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Though, coming from a slightly different direction, and also drawing inspiration from what Ryan Heuser and Long Le-Khac did back in &lt;a href=&quot;https://litlab.stanford.edu/LiteraryLabPamphlet4.pdf&quot;&gt;Pamphlet 4&lt;/a&gt; with the &amp;quot;Correlator&amp;quot; program. Instead of starting with topics in the normal sense -- groups of words that tend to show up in close proximity in individual texts -- I was curious what would happen if I clustered words just on the basis of their cumulative distributions across lots and lots of texts, regardless of whether they actually hang together inside of individual novels. (Similar to Ryan and Long&#39;s notion of a &amp;quot;semantic cohort,&amp;quot; a group of words that correlate in overall frequency across historical time.) If we just start with individual words, and build up from there -- which groupings of words are the most &amp;quot;cohesive,&amp;quot; at a narratological level? What pops out most strongly, which combinations of words do the most narratological work?&lt;/p&gt;
&lt;p&gt;One simple approach is just to do a basic hierarchical cluster of the distributions. Here, I converted the raw percentile counts for each word into density functions, and then just took the raw euclidean distance between each pair of words. (As I mentioned before, this is iffy when comparing words with very large differences in overall frequency, and, in this case, has the effect of lumping in all of the high-frequency words with the &amp;quot;middle&amp;quot; words, even when they actually have really interestingly skewed distributions. But again -- this is another can of worms.) Then, I handed the distance matrix over to SciPy&#39;s &lt;code&gt;[dendrogram](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.dendrogram.html)&lt;/code&gt; function to compute the hierarchical cluster, using the &amp;quot;ward&amp;quot; linkage method. I ended up running this on the 1,000 most non-uniform words, not just the top 500, which gives a bit more specificity to some of the clusters. The final render is huge (click to show full size):&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/dendrogram.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/dendrogram.png&quot; width=&quot;400px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The nice thing about hierarchical clustering is that you don’t have to make many decisions going in – just the distance and linkage metrics. But, the downside is that interpreting this is a bit subjective, mainly because there’s no hard-and-fast answer as to where you should “cut” the dendrogram, how high up the tree you should go before breaking off a cluster of words to look at. Here, I’ve set the coloring threshold at 0.01, meaning that any words / groups that were merged together with a distance of less than 0.01 (under the “ward” metric) get the same color. This seems to give fairly sensible clusters for this data – here are plots for all of these clusters with at least 3 words. (Important to note that the Y-axes are different here, which I dislike, but I think is the lesser of two evils in this case, since a handful of huge effects would otherwise make it hard to see more subtle but still very strong trends.)&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/cohorts.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/cohorts.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Really this is probably a bit too granular, since there are a number of groups that have similar profiles and seem to hang together at some kind of topical / conceptual level. But, I think it&#39;s useful to err in the direction of too granular instead of too coarse, since there are some cases where the lower threshold splits out groups that seem interestingly different (for example, the &amp;quot;family&amp;quot; and &amp;quot;dialogue&amp;quot; words, discussed below).&lt;/p&gt;
&lt;h2&gt;Beginnings&lt;/h2&gt;
&lt;p&gt;Here&#39;s my less-than-scientific gloss on this, where, in the interest of shortness, I&#39;ve often merged a handful of groups back together into higher-order groups. To start, at the beginning -- there&#39;s a big cohort of words that all have something to do with the description of people and objects -- age (sixteen, seventeen, eighteen, younger, older), body size (tall, stout, slender), personal qualities (graceful, educated), physical materials (wooden, leather, cotton), etc. These peak out in the first percentile, fall off sharply in the first ~10%, and then decline more gradually across the middle, with some spiking slightly again at the very end:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/descriptions-2.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/descriptions-2.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This makes sense – the fictive world has to get sketched into existence at the start; a movie or play can just show things, but a novel has to explicitly describe them, the novelist has to manually “render” the world of the story.&lt;/p&gt;
&lt;p&gt;Education also happens at the very beginning, though the word “education” itself also ticks up slightly at the end, which I’m not sure about – maybe education in a more general sense, the protagonist having received an education of some sort over the course of the plot, not education in the literal sense of school, college?&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/education.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/education.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Beyond words that just show up at the beginning – there’s an interesting set of clusters that are strong at the very beginning, flat across the middle of the text, but then also high at the very end. These all have to do with physical setting, essentially – cardinal directions (north, south, east, west, northern, southern), features of the physical landscape (mountains, fields, hills, waters), actual place names (America, American, England, York), and the sort of human experience of the outdoors (sky, horizon, wide, vast, distant).&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/geography.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/geography.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Which I guess goes hand-in-hand with the descriptions of people and things -- this is the narrative hammering together the &amp;quot;stage&amp;quot; of the narrative, in the broadest sense, the &lt;em&gt;place&lt;/em&gt; of the text. Though, the spike at the ending is less clear to me. Why return to the physical setting at the very end, once when the world has already been blocked into existence by the beginning? (And surely, at the end, the narrative isn&#39;t crossing into new fictive territory that needs to be described for the first time?) Maybe, to use film as an analogy -- if we think of the narrative as a kind of &amp;quot;camera&amp;quot; onto the world, this is the camera panning out at the end, pulling back into a wide shot of the landscape of the story -- the characters gazing out contemplatively over the mountains, plains, valleys, seas? A kind of reciprocal &amp;quot;wide shot&amp;quot; to match the scene-setting of the beginning? Like in &lt;em&gt;Howards End&lt;/em&gt;, maybe, when, at the very end, everyone goes outside and looks at the &amp;quot;field&amp;quot; and &amp;quot;meadow&amp;quot; behind the house. The last two paragraphs:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;From the garden came laughter. &amp;quot;Here they are at last!&amp;quot; exclaimed Henry, disengaging himself with a smile. Helen rushed into the gloom, holding Tom by one hand and carrying her baby on the other. There were shouts of infectious joy.&lt;/p&gt;
&lt;p&gt;&amp;quot;The &lt;strong&gt;field&lt;/strong&gt;&#39;s cut!&amp;quot; Helen cried excitedly--&amp;quot;the big &lt;strong&gt;meadow&lt;/strong&gt;! We&#39;ve seen to the very end, and it&#39;ll be such a crop of hay as never!&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or, to stick with Forster, the last paragraph of &lt;em&gt;A Room with a View&lt;/em&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Youth enwrapped them; the song of Phaethon announced passion requited, love attained. But they were conscious of a love more mysterious than this. The song died away; they heard the &lt;strong&gt;river&lt;/strong&gt;, bearing down the snows of winter into the &lt;strong&gt;Mediterranean&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Along the same lines -- &amp;quot;sun,&amp;quot; &amp;quot;sunlight,&amp;quot; and &amp;quot;sunshine&amp;quot; are high at the beginning and the end:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/sunny.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/sunny.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;But also “clouds,” “snow,” and “shadows,” which, unlike sunshine, are flat across the middle, instead of falling off to a low point around 80%. (Not sure how much can be made of those kinds of small differences, which could always just be noise in the sample. Though also not sure that nothing should be made of them, since N here isn’t small.)&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/cloudy.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/cloudy.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Words related to family are also strong at the beginning, though with a twist – unlike the description words, which are highest in the very first percentile, family words start relatively lower and then spike up to a peak in the second, third, fourth percentiles, before falling off more gradually across the first half of the narrative and then rising slightly at the end. As if – the narrative starts with world-building, and then turns to characters, where the first order of business is to lay out the family tree, fill in the edges of the graph?&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/family.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/family.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This also seems to have something to do with childhood and youth, with “mama,” “mamma,” “papa.” Though interestingly, words explicitly about children and childhood break off cleanly into a separate cluster, which rises much higher at the end:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/children.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/children.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Again, &amp;quot;childhood&amp;quot; at the beginning makes sense -- narratives map onto lives, are told in chronological order, etc. And I suppose that &amp;quot;childhood&amp;quot; at the end is, basically, the consummation of the marriage plot? &amp;quot;Childhood&amp;quot; in the first percentile is the childhood of the protagonist, and &amp;quot;childhood&amp;quot; in the 99th percentile is the childhood of his/her children?&lt;/p&gt;
&lt;h2&gt;10%&lt;/h2&gt;
&lt;p&gt;Next in order, moving away from the start -- a series of clusters of words related to entertainment, fashion, social interaction, and, notably, women, all of which rise quickly at the start, reach a relatively broad peak around 10%, and then fall of in a sort of convex arc. First, with the highest peak, around 10-15% -- &amp;quot;girls,&amp;quot; &amp;quot;amused,&amp;quot; &amp;quot;amusement&amp;quot;:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/girls-amusement.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/girls-amusement.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Or, “girl,” “pretty,” “wear,” “dress”:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/girl-pretty.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/girl-pretty.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And, with somewhat less of that initial rise in the first couple percent, “women,” “clothes,” “table,” “manner”:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/women-clothes.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/women-clothes.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Along with women and fashion, this is also very notably the arc of food, which creeps in above with “fish” and “table.” Also peaking around 10% – “breakfast,” “supper,” “meal,” “beer,” “coffee,” “tea”:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/breakfast-supper.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/breakfast-supper.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And, interestingly, a fairly distinct second food cluster, which rises more gradually to a peak around 20% – “lunch,” “wine,” “eat,” “dinner,” and also words like “conversation,” and “dance” – which make me think of soirees, dinner parties, balls, gatherings that are more formal, that take place at night?&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/lunch-dinner.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/lunch-dinner.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So, a kind of anatomy of beginnings starts to come into view. First, descriptions of physical setting, places, things, weather, and (the appearance of) people; then family relations and childhood. And then, once the work of world-building is complete, once the stage and props and characters have been described into existence -- it&#39;s as if the narrative kicks off in earnest with some kind of social gathering or a meal, a first scene or set piece where the characters (mostly women?) are shown dressed fashionably, sitting around the table, at ease and enjoying themselves, before the complications of the plot ensue -- Anna Pávlovna&#39;s soiree, at the start of &lt;em&gt;War and Peace&lt;/em&gt;, the first dinner at the Bertolini in &lt;em&gt;A Room with a View&lt;/em&gt;? A meal at 10%, a dance at 20% -- how many individual novels do this?&lt;/p&gt;
&lt;h2&gt;90%&lt;/h2&gt;
&lt;p&gt;Meanwhile, at the end -- first of all, sort of mirroring the food / amusement cohorts that peak just after the start -- a series of clusters that peak just before the end, around 90-95%, all having something to do with violence. The highest peak is murder, generally with guns:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/guns-murder.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/guns-murder.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And the trial, which looks very similar to what Ben Schmidt saw with the TV scripts, here probably driven by suspense and mystery novels:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/trial.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/trial.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And I think this is war – “enemy,” “attack,” “guard,” “escape”?&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/war.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/war.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I guess these peaks are the &amp;quot;climax,&amp;quot; basically?&lt;/p&gt;
&lt;h2&gt;Ends&lt;/h2&gt;
&lt;p&gt;Finally, at the very end, words that peak in the 99th percentile -- some endings are happy, filled with joy, happiness, tender embraces, kisses, eternal faithfulness:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/happy-endings.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/happy-endings.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Related, the marriage plot, which is the single strongest signal of any of these, just by the height of the spike:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/marriage.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/marriage.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And, Brooks / Barthes / Kermode and friends were right, endings also tend to be about death:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/sad-endings.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/sad-endings.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Middles?&lt;/h2&gt;
&lt;p&gt;What about the middle? Middles are relatively sparse, in this set of 1,000 words, though, in part, I think this is a function of how I&#39;m skimming off words to look at, and how the counts were tallied up in the first place. Really just two things pop out, and, in both cases, it&#39;s not so much that the clusters are &amp;quot;peaking&amp;quot; in the middle, and more just that they&#39;re missing at the beginning and end. The most exactly middle-heavy cluster is probably this one, which clearly corresponds to &lt;em&gt;dialogue&lt;/em&gt; -- quotation marks and contractions, and those &lt;code&gt;&amp;quot;&lt;/code&gt; tokens, which are actually errors, places where the OpenNLP tokenizer is failing to split the quotation mark away from the first word in the sentence.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/dialogue1.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/dialogue1.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And this cluster, which is a little more ambiguous, but I think also marks conversation – “question,” “talk,” “why,” and more contractions:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/dialogue2.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/dialogue2.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Interestingly, there’s a third dialogue cluster, which looks broadly similar but peaks significantly later, around 80%, and, in addition to the dialogue tokens, also includes a set of words that seem to describe mental states or intentions – “understand,” “try,” “believe,” “know,” “feel,” “wish,” “want”:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/mental-states.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/mental-states.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Though it&#39;s interesting that &amp;quot;think&amp;quot; gets clustered with that first group, which peaks right in the middle.&lt;/p&gt;
&lt;p&gt;So, middles are -- speaking and thinking, dialogue and psychological interiority? That rings true, but I also think it&#39;s probably not the full picture. First, it might be that the most middle-heavy words aren&#39;t making it into this cut of 1,000 words, which, as I mentioned before, is based on one particular scoring metric (among many possible alternatives) that tends to reward relatively frequent words.&lt;/p&gt;
&lt;p&gt;More broadly, I also wonder if the process of normalizing the text lengths has the effect of sort of &amp;quot;blurring out&amp;quot; the structure of the middle, and giving clear pictures of just the beginning and the end. For example, imagine that there were some kind of event / function / trope that tends to happen at a &lt;em&gt;fixed&lt;/em&gt;, not relative, distance from the beginning or end -- eg, a thunderstorm always happens right around 10,000 words from the beginning, or whatever. If the novel is 50,000 words long, this would get mapped onto the 20% marker. But, if the novel is 100,000 words long, it would get put at 10%, etc. This seems clearly problematic, but I also don&#39;t think there&#39;s an easy solution that doesn&#39;t introduce other problems. For example, if we don&#39;t normalize the lengths at all, and compare the 50,000 word novel directly to the 100,000-word novel -- then, words at the very end of the 50k novel would get compared to words at the exact middle of the 100k novel, which also seems weird. (Though I&#39;d be curious to try this anyway, and see what happens, possibly starting from different fixed &amp;quot;anchor points&amp;quot; in the narrative -- plot time-series trends for words in terms of raw distance from the beginning, moving forward; from the end, moving backwards; or from the 25% / 50% / 75% markers, moving backwards and forwards, etc.)&lt;/p&gt;
&lt;p&gt;Anyway though, most of this isn&#39;t super surprising. Where it gets more interesting, I think, is with the really high-frequency function words, which turn out to have very irregular trends across the narrative, often in ways that seem to give a kind of keyhole view onto something more fundamental, a kind of underlying narrative &amp;quot;physics&amp;quot; that sits below the birth / death / murder / marriage conventions. I&#39;ll write more about this next, but quickly -- check out &amp;quot;and&amp;quot; and &amp;quot;or&amp;quot;:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/and-or.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/and-or.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>The (weird) distributions of function words across novels</title>
    <link href="https://litlab.stanford.edu/distributions-of-function-words/"/>
    <updated>2017-08-10T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/distributions-of-function-words/</id>
    <content type="html">&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/a-an-the-big.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/a-an-the-big.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Last week I looked at some of the &lt;a href=&quot;https://litlab.stanford.edu/hierarchical-cluster-across-narrative-time/&quot;&gt;clusters of words that fluctuate together across narrative time&lt;/a&gt; in the Lab&#39;s corpus of ~27k American novels. A lot of these are pretty semantically &amp;quot;legible,&amp;quot; in the sense that it&#39;s not hard to map them back onto the experience of actually reading novels. For example, it&#39;s easy to reason about what&#39;s going on with cluster 139 (student, students, school) or 37 (pistol, bullet, gun). Which, as &lt;a href=&quot;https://twitter.com/Ted_Underwood/status/892445002978250752&quot;&gt;Ted&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/scottenderle/status/892466165343293440&quot;&gt;Scott&lt;/a&gt; pointed out on Twitter, might tell us more about the presence of different genres in the corpus than about &amp;quot;narrative,&amp;quot; in any kind of general sense.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/school-guns.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/school-guns.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;But, what to make of something like cluster 10? This seems more muddled, and, notably, includes a number of stopwords – “a,” “an,” “or,” “than,” “these”.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/a-an-cluster.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/a-an-cluster.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Which, it turns out, isn’t a fluke. Function words tend to have very strong trends across narrative time. In fact, stronger than almost anything else in the dictionary. Take a look again at this graph from a couple weeks ago, which plots the non-uniformity of each word as a function of its frequency:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/variance.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/variance.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The Y-axis is just the variance of the word&#39;s frequency across each percentile of the novel, which gives a crude measure of the unevenness of the word, the degree to which it&#39;s rising and falling across the narrative. The black line represents the null hypothesis, basically -- the amount of variance that we&#39;d expect under the uniform distribution, if everything were just random noise. By dividing the observed variance by this theoretical baseline, we can get a simple score for the narratological volatility of the word, in a way that controls for the expected correlation with frequency.&lt;/p&gt;
&lt;p&gt;Before, I focused on the fact that almost all of the words fall above this line, which corresponds to the fact that most words have some kind of statistically significant trend across the novel. But, beyond that, it&#39;s also clear that the slope of the data is steeper than the slope of the line -- &lt;em&gt;words rise higher above the line as frequency increases&lt;/em&gt;. On the left side of the graph, the highest-scoring words sit about 2 orders of magnitude above the baseline; at the right side, this rises to about 3. Words seem to become &lt;em&gt;more&lt;/em&gt; narratologically volatile as they become more frequent, even after adjusting for the expected relationship between frequency and variance.&lt;/p&gt;
&lt;p&gt;Indeed, when we use this metric to skim off a set of the most non-uniform words, we end up getting a large majority of the most frequent words and a much smaller slice of the less frequent words. If we take the 200 highest-scoring words, for example, we get 60 out of the 100 most frequent words in the corpus, here in bold:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;end, &lt;strong&gt;you&lt;/strong&gt;, chapter, &lt;strong&gt;a&lt;/strong&gt;, &lt;strong&gt;of&lt;/strong&gt;, &lt;strong&gt;the&lt;/strong&gt;, &lt;strong&gt;i&lt;/strong&gt;, &lt;strong&gt;.&lt;/strong&gt;, chapter, &lt;strong&gt;?&lt;/strong&gt;, i., ii, years, &lt;strong&gt;&amp;quot;&lt;/strong&gt;, &lt;strong&gt;!&lt;/strong&gt;, &lt;strong&gt;said&lt;/strong&gt;, &lt;strong&gt;me&lt;/strong&gt;, iii, &lt;strong&gt;&amp;quot;&lt;/strong&gt;, &lt;strong&gt;him&lt;/strong&gt;, young, &lt;strong&gt;she&lt;/strong&gt;, 2, 3, &lt;strong&gt;he&lt;/strong&gt;, &lt;strong&gt;to&lt;/strong&gt;, &lt;strong&gt;her&lt;/strong&gt;, its, &lt;strong&gt;&amp;quot;&lt;/strong&gt;, &lt;strong&gt;will&lt;/strong&gt;, i., father, school, &lt;strong&gt;what&lt;/strong&gt;, &lt;strong&gt;and&lt;/strong&gt;, hair, mother, god, age, &lt;strong&gt;&amp;quot;&lt;/strong&gt;, tall, ), death, love, the, &lt;strong&gt;that&lt;/strong&gt;, (, &lt;strong&gt;,&lt;/strong&gt;, year, brown, &lt;strong&gt;have&lt;/strong&gt;, happy, &lt;strong&gt;now&lt;/strong&gt;, life, &lt;strong&gt;it&lt;/strong&gt;, &lt;strong&gt;not&lt;/strong&gt;, &lt;strong&gt;we&lt;/strong&gt;, &lt;strong&gt;your&lt;/strong&gt;, &lt;strong&gt;in&lt;/strong&gt;, &lt;strong&gt;&amp;quot;&lt;/strong&gt;, happiness, &lt;strong&gt;an&lt;/strong&gt;, new, joy, boy, &lt;strong&gt;or&lt;/strong&gt;, 5, &lt;strong&gt;if&lt;/strong&gt;, wedding, vii, dead, 6, again, &lt;strong&gt;&amp;quot;&lt;/strong&gt;, old, family, &lt;strong&gt;do&lt;/strong&gt;, blue, small, &lt;strong&gt;which&lt;/strong&gt;, viii, l, iv, heart, large, &lt;strong&gt;did&lt;/strong&gt;, published, book, gun, miss, girls, arms, do, 9, girl, &lt;strong&gt;would&lt;/strong&gt;, ix, &lt;strong&gt;could&lt;/strong&gt;, &lt;strong&gt;his&lt;/strong&gt;, tell, &lt;strong&gt;&amp;quot;&lt;/strong&gt;, kill, bride, told, &lt;strong&gt;was&lt;/strong&gt;, college, always, black, letter, &lt;strong&gt;has&lt;/strong&gt;, 8, don&#39;t, &lt;strong&gt;can&lt;/strong&gt;, &lt;strong&gt;back&lt;/strong&gt;, tears, know, author, last, forgive, shall, asked, go, &lt;strong&gt;be&lt;/strong&gt;, die, mr., saw, &lt;strong&gt;had&lt;/strong&gt;, broad, handsome, hand, gray, must, &lt;strong&gt;then&lt;/strong&gt;, 7, &lt;strong&gt;about&lt;/strong&gt;, fiction, think, &lt;strong&gt;n&#39;t&lt;/strong&gt;, edition, books, boys, &lt;strong&gt;little&lt;/strong&gt;, &lt;strong&gt;my&lt;/strong&gt;, &lt;strong&gt;is&lt;/strong&gt;, cried, &lt;strong&gt;their&lt;/strong&gt;, us, younger, summer, older, prisoner, love, xii, 1, green, &lt;strong&gt;by&lt;/strong&gt;, &amp;quot;you, stranger, vi, beauty, dying, grave, done, novel, village, readers, xiii, world, youth, 22, children, york, kissed, born, think, know, love, 13, mrs., town, rich, killed, story, 23, 21, still, 12, &amp;amp;, whose, very, wife, &lt;strong&gt;from&lt;/strong&gt;, went, &lt;strong&gt;some&lt;/strong&gt;, nose, 24, v., pain, see, &lt;strong&gt;like&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What to make of this? It&#39;s kind of perplexing, and runs against what I expected at the start. I assumed that function words would be basically flat -- maybe with some very slight ups and downs -- since I don&#39;t really think of them as having any kind of narratological &amp;quot;valence&amp;quot; that would cause them to attach to beginnings / middles / ends in the way that things like death and marriage do. I thought they&#39;d probably be negative examples of what I was looking for, almost -- the connective tissue of the language, always just there, never ebbing or flowing. (Though I also remembered Matt Jockers&#39; finding from &lt;em&gt;Macroanalysis&lt;/em&gt; that the word &amp;quot;the&amp;quot; rises and falls across historical time, and a little voice in my head wondered if there might be similar effects across narrative time.)&lt;/p&gt;
&lt;p&gt;Usually, when something correlates with frequency like this, it feels like a red flag, the worry being that you&#39;re somehow just reproducing the fact that frequent words are frequent, infrequent words are infrequent. As a sanity check, and to get a sense of what the null hypothesis would look like, I re-ran the same feature extraction job on the corpus, but this time, before pulling out the percentile-sliced word counts for each text, I randomly shuffled the words to destroy any kind of narratological ordering. Sure enough:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/random-variance.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/random-variance.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So – as far as I can tell, I think there is actually some meaningful way in which the highest frequency words are some of the most skewed across the narrative, the most uneven, the most narratologically charged? This seemed really weird to me at first, then I convinced myself that it wasn’t actually that weird, and now I’m back to being surprised by it. But, I’m not sure. The effect is so strong, it makes me wonder – is it somehow inevitable, is there some kind of fundamental linguistic / literary / information-theoretic pressure that would make it impossible for this not to be the case, in some way?&lt;/p&gt;
&lt;p&gt;Part of the issue, I think, is that this question of whether a word is narratologically “uneven” is actually less cut-and-dry than it seemed to me at first, and it gets caught up in interesting ways with the overall frequency of the word. For example, take “gun” and “a”:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/a-gun-hist.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/a-gun-hist.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;“Gun” appears 174,286 times; “a” appears 44,510,387 times, about 255 times as often. Which of these is more “surprising”? At a kind of visual / intuitive level, “gun” obviously has the more dramatic trend – a huge spike around the 95% marker, where it literally doubles in volume relative to the baseline across the first half of the narrative. Indeed, if you convert them into probability density functions – throwing out any information about the overall frequencies – and then compare them to a uniform distribution using pretty much any goodness-of-fit test or distance metric, “gun” will always score higher by a large margin. Just using the Euclidean distance – “a” has a distance of 0.004 from the uniform distribution, whereas “gun” is 0.02, over 5x higher.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/a-gun-series.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/a-gun-series.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;But, when you remember the actual footprint of &amp;quot;a&amp;quot; in the corpus -- 44 million occurrences, which represents about 1.8% of all words in all 27k novels -- the total quantity of linguistic &amp;quot;mass&amp;quot; that&#39;s getting displaced is sort of incredible. Here it is again, plotted this time with an error bar around the expected value of the uniform distribution -- if &amp;quot;a&amp;quot; had no trend across the plot, 95% of the bin counts would fall inside the gray band, which gets dwarfed by the actual data. In the first percentile, &amp;quot;a&amp;quot; appears 72 &lt;em&gt;thousand&lt;/em&gt; times more than we&#39;d expect under the uniform distribution, and 40 thousand fewer times in the last percentile. Which correspond to zscores of 109 and -61, respectively, which are absurdly large. (In fact so large, as Scott Enderle pointed out, that the uniform distribution almost feels like a meaningless / poorly-chosen null hypothesis. But, I&#39;m not really sure what the alternative would be.)&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/a-zscore.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/a-zscore.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Whereas for “gun,” the high-water mark at 95% has a zscore of 37, which of course is still huge:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/gun-zscore.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/gun-zscore.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;“A” is flatter, but since it’s so frequent, it represents a kind of massive, tectonic displacement of words, sort of like the gravity of the moon pulling the tide in an out – the water only rises and falls a couple of feet, but in order for that to happen the entire mass of the ocean has to get moved around. The amount of narratological “energy” needed to produce the “a” trend seems much larger than for “gun,” from this perspective.&lt;/p&gt;
&lt;p&gt;So, “gun” beats “a” in one way, but “a” beats “gun” in other ways. Which of this is more true? I spent some time going around in circles on this, but, as Dan Jurafsky and Ryan Heuser pointed out, there might not be a single right answer. Maybe more accurate just to say that there are different types of “surprise” at play, and that they operate differently at n=10^5 than at n=10^8?&lt;/p&gt;
&lt;p&gt;To get a broader set of how that plays out across lots of words – out of the 100 most frequent words in the corpus, 90 appear in the list of the 1,000 most uneven words, under the metric from above. Here’s this list of 90, sorted from the most uneven to the least uneven, starting with the most narratologically skewed:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/multiples.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/multiples.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;A vs. the&lt;/h2&gt;
&lt;p&gt;There’s too much here to go through all of it, but quickly – what’s up with “a”? There’s a remarkable symmetry to it – high at the very beginning, a fast falloff in the first 10%, a slower decline across the middle, and then a nosedive again at the very end. “An” is almost identical, though with more noise in the sample, since it’s less frequent:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/a-an.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/a-an.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There&#39;s a pretty easy explanation for this, though I&#39;m kind of fascinated by the fact that it seems to show up at the scale of the entire narrative, and not just inside of individual passages -- &amp;quot;a&amp;quot; is used when an object is introduced for the first time, when an entity makes its first appearance in some context. For example, we might first say -- &amp;quot;a man was walking down the street&amp;quot; -- but then after that, once the man has been placed on the narrative stage, we&#39;d switch to the definite article -- &amp;quot;&lt;em&gt;the&lt;/em&gt; man walked into a shop,&amp;quot; etc. (Franco pointed out that Benveniste makes exactly this point in &lt;em&gt;Problems in General Linguistics&lt;/em&gt;.)&lt;/p&gt;
&lt;p&gt;So, this is totally speculative, but -- maybe one way to think about this is to say that &amp;quot;a&amp;quot; is a proxy for the &lt;em&gt;rate at which newness is getting introduced into the text&lt;/em&gt;? Most quickly at the start, as the fictional world is getting introduced for the first time. Then, over the course of the middle, the plot continues to move into new fictional space -- new people, new places, new objects -- but more slowly than at the beginning. And then slowest at the very end, where the plot doesn&#39;t have any space left to introduce new things. &amp;quot;A,&amp;quot; in other words, gives a kind of empirical X-ray of the &amp;quot;speed&amp;quot; of the novel, in one sense of the idea -- the degree to which it&#39;s moving into new fictional contexts that have to be introduced for the first time, as opposed to standing still inside of contexts that have already been introduced? (Sort of like those old RPGs from the late 90s like Baldur&#39;s Gate or Icewind Dale, where by default the entire world of the game is black, and thing only come into view as your character moves around the map, as the spotlight falls onto new territory for the first time -- the moment of &amp;quot;a&amp;quot;?)&lt;/p&gt;
&lt;p&gt;Is this the right explanation? I think it seems sensible, but I don&#39;t really know. The funny thing, though, is that it&#39;s not totally clear to me how you&#39;d &amp;quot;prove&amp;quot; this, either at a linguistic or a literary register. Usually the next step would be to dip back down into individual texts and start spot-checking passages, but with a word like &amp;quot;a,&amp;quot; which will appear literally millions of times in virtually all contexts, this seems like sort of a losing game. I guess the first thing would be to look at words that follow &amp;quot;a,&amp;quot; and see if some kind of pattern falls out? Eg, count up all &amp;quot;a __&amp;quot; bigrams, and then find words that come after &amp;quot;a&amp;quot; most distinctively in the first percentile, as compared to the last percentile?&lt;/p&gt;
&lt;p&gt;&amp;quot;The&amp;quot; is interestingly different:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/a-an-the.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/a-an-the.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Also very high at the start, a fast falloff in the first 10% (faster than “a”), comparatively low through the middle, and then a gradual rise at the end, starting at around 60%. So, “a” and “the” – flip sides of the same coin, grammatically – seem to do different work at a narratological level? Both seem to mark beginnings and ends, but in different ways. “A” shows something about how they are different – beginnings are building worlds, ends are inhabiting those worlds? Whereas, “the” is high at both the beginning and the end, and so, I guess, is marking something about how they are similar, a way in which the end is some kind of return to the beginning? But, in what sense?&lt;/p&gt;
&lt;p&gt;I’m not sure about this, especially the ending. Weirdly, if we compare “the” to the combined trend for all nouns in the corpus, the ending doesn’t match up:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/the-nn.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/the-nn.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;No idea, really. Again, to really get at this, we&#39;d need to look at context -- the &lt;em&gt;what&lt;/em&gt;? What words are following &amp;quot;the,&amp;quot; in different parts of the narrative?&lt;/p&gt;
&lt;h2&gt;Determiners&lt;/h2&gt;
&lt;p&gt;Beyond &amp;quot;a&amp;quot; and &amp;quot;the&amp;quot; -- the other determiners are interesting:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/this-that-these-those.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/this-that-these-those.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So, at a narratological level, they basically pair up on the basis of singular / plural, not near / far. “This” and “that” are low at the beginning, peak around 70%, and then fall off at the end:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/this-that.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/this-that.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Whereas “these” and “those” are very high at the start, flat across the middle, and then split at the end, with “those” going up and “these” falling off:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/these-those.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/these-those.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;JD pointed out that “this” and “that” look a lot like the dialogue clusters from last week, with the wide peak across the middle. As for “these” and “those” – I’m not sure why plurals would be so high at the beginning, but in this case it does seem to generally match up with the trend for nouns, where plurals are much higher at the start:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/nn-ns.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/nn-ns.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Though, the question then just becomes -- why plural nouns at the start? The divergence at the end is also interesting -- why does &amp;quot;those&amp;quot; spike up, and &amp;quot;these&amp;quot; fall off? Again, all of this needs much more careful attention, but -- picking up on the &amp;quot;geography&amp;quot; words from the last post, which spiked at the end -- this fits with the idea that the end is a sort of &amp;quot;zooming out,&amp;quot; if we think of the narrative as a camera onto the fictional world? At the end, the narrative pans out into a wide shot of the surrounding mountains / fields / valleys, it makes itself &lt;em&gt;distant&lt;/em&gt; from the action of the plot -- the domain of &amp;quot;those,&amp;quot; not &amp;quot;these&amp;quot;?&lt;/p&gt;
&lt;p&gt;Check out the &amp;quot;how much&amp;quot; determiners -- &amp;quot;all,&amp;quot; &amp;quot;some,&amp;quot; and &amp;quot;no&amp;quot; (&amp;quot;no&amp;quot; specifically as a determiner, in the sense of &amp;quot;there were no people in the room&amp;quot;):&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/all-some-no.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/all-some-no.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;“All” peaks at the end, the moment of generalization, completeness, closure? I’m less sure what to make of the fact that “some” peaks at 20%, but “no” at 80%:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/some-no.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/some-no.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Meanwhile, to close out the determiners – “each” skyrockets at 99%, whereas “every” stays low:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/each-every.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/each-every.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;And vs. or&lt;/h2&gt;
&lt;p&gt;Conjunctions are also fascinating:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/and-or2.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/and-or2.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Again, there’s a tidy explanation for the split at the end, though I’m still sort of bewildered that this stuff actually shows up so strongly and at such a low level. “Or” introduces a potential branch in the narrative, a state of indeterminacy – Robert will blow the bridge, or he’ll die trying; Lucy will marry Cecil, or she will marry George; etc. And so, as the plot moves towards a close, “or” has to fall off as the ending is revealed, as uncertainty is replaced by certainty, as the plot gets sealed up as a unity and the Jamesian “circle” comes to a close?&lt;/p&gt;
&lt;h2&gt;To be&lt;/h2&gt;
&lt;p&gt;Moving on to verbs – a handful of “to be” verbs show up in the list of 90 above. Here’s everything together:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/to-be.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/to-be.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Which splits really cleanly into the present tense:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/to-be-present.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/to-be-present.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And the past:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/to-be-past.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/to-be-past.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So – the beginning is in the past tense, the middle in the present tense (dialogue, again?), but then the past tense peaks again around 95% (the climax, in some sense?), and then the present shoots back up at the very end.&lt;/p&gt;
&lt;h2&gt;Pronouns&lt;/h2&gt;
&lt;p&gt;Pronouns are also really varied. Subject and object pronouns are low at the start, rise gradually across the middle, and then kind of scramble at the end. Though, overall, the subject pronouns seem to plateau around 80%, whereas the object pronouns start to curve up a bit:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/prp-subject-object.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/prp-subject-object.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The absence at the beginning, I guess, just corresponds that things first need to be introduced with regular nouns, before they can be referred to with pronouns. Eg, “a man named Robert” has to come before “he”?&lt;/p&gt;
&lt;p&gt;Whereas, possessives are all over the place at the start:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/prp-possessive.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/prp-possessive.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;With “its” and “your” kind of playing foils to each other.&lt;/p&gt;
&lt;p&gt;Breaking these out by grammatical “persons” – for the first- and third-person singulars, the possessive is always highest at the start, followed by the subject, then the object. But endings are more mixed, maybe with some interesting gender patterns – “he,” “him,” and “his” all fall off dramatically at the end, whereas “her” (as a possessive) and “my” spike up:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/prp-1st-3rd.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/prp-1st-3rd.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Meanwhile, for the third-person plural – “their,” the possessive, is super strong at the beginning and end:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/prp-3rd-plural.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/prp-3rd-plural.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And, with the first-person plurals, “our” rises highest at the end:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/prp-1st-plural.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/prp-1st-plural.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It’s also interesting that, for “he” / “him” and “she” / “her,” the object gradually overtakes the subject. Especially with “he” and “him,” both of which are almost exactly linear across the middle, but with “him” rising faster and higher:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/he-him-she-her.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/he-him-she-her.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So, as the narrative moves forward – things increasingly happen to people, they increasingly become grammatical objects, instead of subjects?&lt;/p&gt;
&lt;h2&gt;Punctuation&lt;/h2&gt;
&lt;p&gt;This is going on way too long, but quickly – also really zany are the punctuation tokens, which, usefully in this context, get broken out by the OpenNLP tokenizer as separate words:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/punctuation.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/punctuation.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Questions and dialogue happen in the middle, and endings are exclamatory:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/question-exclamation.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/question-exclamation.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Periods and commas also fascinate me:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/period-comma.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/period-comma.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The period, I assume, is basically a proxy for sentence length, where more periods mean shorter sentences? So, sentences are longest at the very beginning, and shortest just shy of the end, around 97%. I guess – long, descriptive sentences at the start, and short, staccato, action-filled sentences at the end? And it makes sense that commas would be inversely correlated – fewer periods means longer sentences, which means more commas? (And, they kiss at the end!)&lt;/p&gt;
&lt;p&gt;Anyway, there’s sort of an infinity of stuff to look at here, and it’s hard to know where to start. I’m writing code right now to look at all of these in the context of higher-order ngrams – eg, what words are following “the” in the 99th percentile? And beyond that, I think most interesting – would it be possible to use these kinds of trends in really high-frequency words to train classifiers that could “unshuffle” novels, a predictive model of narrative sequence?&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on the New Yorker</title>
    <link href="https://litlab.stanford.edu/news/2018-02-05-presentation-on-the-new-yorker/"/>
    <updated>2018-02-06T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2018-02-05-presentation-on-the-new-yorker/</id>
    <content type="html">&lt;p&gt;On February 6th, 2018, Nichole Nomura led a discussion about the New Yorker in the Literary Lab.&lt;/p&gt;
&lt;p&gt;This was somewhat different from ordinary Lab meetings in that we developed multiple research questions that could benefit from the New Yorker corpus, and discussed how we want to tag and sort the corpus to facilitate answering those questions.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on “Typicality”</title>
    <link href="https://litlab.stanford.edu/news/2018-02-26-presentation-on-typicality/"/>
    <updated>2018-02-27T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2018-02-26-presentation-on-typicality/</id>
    <content type="html">&lt;p&gt;On February 27th, 2018, Mark Algee-Hewitt and Erik Fredner presented their new project on typicality in literature.&lt;/p&gt;
&lt;p&gt;Do literary critics need to know what the typical novel is like? Critics routinely turn to moments where novels violate our expectations of the form, expectations that have been developed by reading, writing about, and discussing novels of all sorts. We may intuitively reject the idea of any given novel’s typicality—each is, in some sense, unlike any other—yet paradoxically rely on a conception of the typical novel as a heuristic for other works in the genre. We know when our expectations in a novel have been undercut, but in writing about a given novel, we tend to focus more on what that undercutting does than the origins of the expectation. This project shifts the critical focus from the former to the latter.&lt;/p&gt;
&lt;p&gt;The problem of typicality poses a question that is at once deeply historical (what is the typical novel of, for example, the 1890s?) and simultaneously unmoored from history (what is the most novel-like novel?) Methods of quantitative analysis can help us get new purchase on these questions: for the first time, we can measure averages, medians, and other statistical expressions of typicality in many different ways across large datasets. But what are the benefits and consequences of thinking about the literary field in this way? How might we measure literary averages, and, assuming that we can find a few, what light would the typical novel shed on our understanding of the literary novel? In this new project, we begin to explore the intersections of quantitative and qualitative typicality as a methodological intervention into literary criticism.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>The Space of Poetic Meter</title>
    <link href="https://litlab.stanford.edu/hooddistance/"/>
    <updated>2018-03-05T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/hooddistance/</id>
    <content type="html">&lt;p&gt;One of the goals of the Techne blog as a whole is to highlight technical issues in Digital Humanities---the kinds of in-the-weeds ideas that are interesting to specialists but don&#39;t necessarily make the cut of a final paper. It&#39;s easy to think of &amp;quot;technical issues&amp;quot; as the domain of the digital half of DH; but I think it&#39;s important to emphasize the technical nature of the humanities as well. Sometimes it&#39;s easy to forget in the new and complex DH world, or just in the STEM-centric environment of U.S. academia, that humanities researchers have *technical *expertise, and DH is best served when it tries to make advances in &lt;em&gt;those&lt;/em&gt; areas as well as finding cool new digital methods.&lt;/p&gt;
&lt;p&gt;To that end, I wanted to discuss an issue that came up a few years ago in the course of the Transhistorical Poetry Project, an early Literary Lab collaboration that included Ryan Heuser, Mark Algee-Hewitt, Jonny Sensenbaugh, Justin Tackett, and me. It started as a technical exploration of both programming and poetics, and led us to one of the most generative ideas of the project as a whole.&lt;/p&gt;
&lt;p&gt;The original goal of the Transhistorical Poetry Project was to automate the detection of poetic form, loosely defined. The first benchmark was detecting the number of metrical feet in each poetic line, followed by extrapolating the &amp;quot;metrical scheme&amp;quot; of a given poem (both of which depended on metrical parsing software called &amp;quot;Prosodic,&amp;quot; developed by Heuser, Josh Falk, and Arto Anttila).&lt;a href=&quot;https://litlab.stanford.edu/hooddistance/#_ftn1&quot;&gt;[1]&lt;/a&gt; That means, in the first instance, determining that the line &amp;quot;The brain is wider than the sky&amp;quot; has four feet, and that the Dickinson poem containing it alternates between four-foot and three-foot lines:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The Brain---is wider than the Sky---&lt;/p&gt;
&lt;p&gt;For---put them side by side---&lt;/p&gt;
&lt;p&gt;The one the other will contain&lt;/p&gt;
&lt;p&gt;With ease---and You---beside---&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In other words, the program was basically good enough to identify ballad meter, though we mostly stuck to labeling poems as &amp;quot;consistent&amp;quot; (like a sonnet, always five feet), &amp;quot;alternating&amp;quot; (like ballad meter) or &amp;quot;irregular&amp;quot; (everything else).&lt;a href=&quot;https://litlab.stanford.edu/hooddistance/#_ftn2&quot;&gt;[2]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Just this level of detail already unlocks new ways to study poetic history; in forthcoming papers, we&#39;ll be looking at, for instance, the history of pentameter in English poetry since the 16th century. But it&#39;s instantly clear to anyone who works on poetry that the results described above are a little unusual. Standard prosody doesn&#39;t really discuss &amp;quot;alternating 4 and 3&amp;quot;, much less a generic &amp;quot;irregular&amp;quot; tag. Instead, it uses long-standing terminology for the metrical patterns within lines; sonnets, for example, are in &lt;em&gt;iambic&lt;/em&gt; pentameter.&lt;/p&gt;
&lt;p&gt;Since several of the members of the project work on poetry, we were especially curious about how our results could interact with four of the most common meters: &lt;em&gt;iambic&lt;/em&gt;, &lt;em&gt;trochaic&lt;/em&gt;, &lt;em&gt;anapestic&lt;/em&gt;, and &lt;em&gt;dactylic&lt;/em&gt;. We don&#39;t pretend that these are the only or best ways to categorize meter in English; but to us the balance of critical usage, and the poetic practice influenced by that usage, makes these categories worth exploring on their own terms.&lt;a href=&quot;https://litlab.stanford.edu/hooddistance/#_ftn3&quot;&gt;[3]&lt;/a&gt; In other words, we wanted to engage with the technical questions raised by ordinary prosody.&lt;/p&gt;
&lt;p&gt;The first step, technical in the metrical sense, is to determine how these feet relate to each other. Essentially, they can be sorted by two criteria: 1. Does the stress come on the first part of the foot, or the last? 2. Does the foot have two beats, or three? These questions organize the feet like so :&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/FourFeet.jpeg&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/FourFeet.jpeg&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(I couldn’t think of an anapestic animal.)&lt;/p&gt;
&lt;p&gt;The next step, technical in the digital sense, was to operationalize these criteria. That is, on a programmatic basis, how can we tell: 1. Whether a foot is head-initial (falling) or head-final (rising)? 2. Whether a foot is binary or ternary?&lt;/p&gt;
&lt;p&gt;Prosodic is pretty good at detecting which syllables in a line are stressed, which is the key to question one. Lines with a head-final rhythm should, generally speaking, start with an unstressed syllable, whereas lines with a head-initial rhythm should start with a stressed one. However, trochaic inversions are common enough that they could throw off the calculations for many poems that a human reader would call iambic. To get around that, we asked Prosodic about the fourth syllable in each line. This worked well, as you’ll see below, though it changes up the square a little.&lt;a href=&quot;https://litlab.stanford.edu/hooddistance/#_ftn4&quot;&gt;4&lt;/a&gt; For question two, we simply check the frequency (in percentage terms) of consecutive weak syllables. Since ternary meters have two unstressed syllables per foot and binary meters only have one, we should see far more consecutive weak syllables in ternary meters.&lt;a href=&quot;https://litlab.stanford.edu/hooddistance/#_ftn5&quot;&gt;5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To test how well these operations worked, we assembled 205 poems that fit comfortably in each of the four meters—that is, they were consistently iambic, anapestic, etc. Sensenbaugh, Holst Katsma, and Zoya Lozoya scanned each of these poems line by line to confirm their metrical regularity. The colors in Figure 2 reflect these initial metrical tags.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/meterscope-canon-byron.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/meterscope-canon-byron.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The lines that cut the graph into quadrants are based on empirical observation; they divide the space in the way that maximizes the machine&#39;s accuracy relative to the initial tags. That accuracy is remarkably high: In this sample, we correctly classified 202 of the 205 poems, or 98.5%.&lt;a href=&quot;https://litlab.stanford.edu/hooddistance/#_ftn6&quot;&gt;[6]&lt;/a&gt; Even the borderline cases are often successes here: Richard Crashaw&#39;s &amp;quot;Upon the Infant Martyrs&amp;quot;, for example, lies exactly between the iambic and trochaic quadrants; its first two lines are perfectly iambic, and its second two lines are perfectly trochaic:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To &lt;strong&gt;see&lt;/strong&gt; both &lt;strong&gt;blen&lt;/strong&gt;ded &lt;strong&gt;in&lt;/strong&gt; one &lt;strong&gt;flood&lt;/strong&gt;,&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;mo&lt;/strong&gt;thers&#39; &lt;strong&gt;milk&lt;/strong&gt;, the &lt;strong&gt;chil&lt;/strong&gt;dren&#39;s &lt;strong&gt;blood&lt;/strong&gt;,&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Makes&lt;/strong&gt; me &lt;strong&gt;doubt&lt;/strong&gt; if &lt;strong&gt;heaven&lt;/strong&gt; will &lt;strong&gt;ga&lt;/strong&gt;ther&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ro&lt;/strong&gt;ses &lt;strong&gt;hence&lt;/strong&gt;, or &lt;strong&gt;li&lt;/strong&gt;lies &lt;strong&gt;ra&lt;/strong&gt;ther.&lt;a href=&quot;https://litlab.stanford.edu/hooddistance/#_ftn7&quot;&gt;[7]&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;At the beginning of the project, we joked that we wanted Prosodic to be as accurate as an undergraduate, and we feel that we succeeded---either inspiring or depressing, depending on your perspective.&lt;/p&gt;
&lt;p&gt;At the same time, Crashaw&#39;s poem is exemplary of a key lesson from this graph. I&#39;ve been speaking of the division of poems into four metrical &lt;em&gt;categories&lt;/em&gt;, but there are not four positions here; there are hundreds, nearly as many as there are poems. Tennyson&#39;s &amp;quot;Lady of Shalott&amp;quot;  and Byron&#39;s &amp;quot;She Walks in Beauty&amp;quot; may both be iambic, but they clearly differ from each other according to the criteria we used to determine their iambic-ness---the distance between the two is substantial, even within the iambic quadrant. The &amp;quot;categories&amp;quot; are far more nuanced than four simple designations; to be precise, what we have here is a metrical &lt;em&gt;space&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;When we turned our attention to a larger archive of poems, we serendipitously discovered a new way to think about this metrical profusion:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/Four-Quadrants-of-Meter-Shewing-All-Poems.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/Four-Quadrants-of-Meter-Shewing-All-Poems.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Every dot is a poem; there are 6,400 in total on this graph. From these, we selected 238 at random and scanned them by hand; our conclusions are reflected in the colors you see here. Compared to our human tags, the program correctly identified 94% of poems.&lt;/p&gt;
&lt;p&gt;With even more metrical variety in front of us, we noticed one poem the graph &lt;em&gt;doesn&#39;t&lt;/em&gt; categorize. At the center of the dividing lines, sitting in none of the four quadrants, is Thomas Hood&#39;s &amp;quot;Lines to Miss F. Kemble, On the Flower Scuffle at Covent Garden Theatre,&amp;quot; a comic poem published in 1832 in the &lt;em&gt;Athenaeum&lt;/em&gt;. As it happens, this poem is maniacally unmetrical (or perhaps it&#39;s the more philosophical &amp;quot;ametrical&amp;quot;)---even moreso than the free verse poems in our sample (in black), which often displayed a particular metrical tendency, even if &amp;quot;accidental.&amp;quot; Here is the first stanza:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Well---this flower-strewing I must say is sweet&lt;/p&gt;
&lt;p&gt;And I long, Miss Kemble, to throw myself considerably at your feet;&lt;/p&gt;
&lt;p&gt;For you&#39;ve made me a happy man in the scuffle when you jerk&#39;d about the daisies;&lt;/p&gt;
&lt;p&gt;And ever since the night you kiss&#39;d your hand to me and the rest of the pit, I&#39;ve been chuck full of your praises!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The rest of the poem (&lt;a href=&quot;https://litlab.stanford.edu/assets/Hood_Kemble.rtf&quot;&gt;available here&lt;/a&gt;) continues very much in the same vein, structured by rhyme but hilariously difficult to read with any metrical regularity.&lt;a href=&quot;https://litlab.stanford.edu/hooddistance/#_ftn8&quot;&gt;[8]&lt;/a&gt; And this is just the exemplar of a general rule: If a poem is perfectly metrical according to these classical feet, it shows up near the corners of the graph---a perfectly trochaic poem will &lt;em&gt;never&lt;/em&gt; have an accented fourth foot or two consecutive weak syllables. If a poem is all over the place, like Hood&#39;s, it will show up near his. If it is somewhere in between---like most poems---it will appear in the space between. And we find this to be true, empirically; the closer a poem is to the center, the more apt it is to be free verse or some other irregular form. Thus the &lt;em&gt;Hood Distance&lt;/em&gt; of a poem---literally its distance from Hood&#39;s &amp;quot;Lines to Miss F. Kemble&amp;quot; on this graph---is a measure of the extent to which it exhibits the characteristics of one of these four basic feet, *but not the others---*that is, its metricality, or metrical regularity.&lt;a href=&quot;https://litlab.stanford.edu/hooddistance/#_ftn9&quot;&gt;[9]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;On one hand, Hood Distance is a great example of a lesson that we encounter again and again in quantitative literary criticism: Things tend not to just &lt;em&gt;be&lt;/em&gt; A or B, but to be some percentage A or B. That&#39;s what a trochaic inversion, for instance, really indexes in the first place---most sonnets aren&#39;t 100% iambic (or, in a sense, 100% &lt;em&gt;sonnet&lt;/em&gt;). This graph gives us a way to see meter in particular not as a quality (&lt;em&gt;either&lt;/em&gt; iambic &lt;em&gt;or&lt;/em&gt; trochaic) but as a space of possibilities.&lt;/p&gt;
&lt;p&gt;On the other hand, we believe that Hood Distance amounts to more than added nuance for an existing concept. It articulates a &lt;em&gt;new&lt;/em&gt; concept, the range of formal variation from poems with a strongly defined base meter (like iambic ballads) to those with no particular base meter at all (like free verse poems). In short, Hood Distance means that the &lt;em&gt;metricality&lt;/em&gt; of a poem, in this complex metrical space with its hundreds of realized possibilities, can be measured. We can track the history not just of things like pentameter, or even iambic pentameter, but metricality in general---the tendency of the poems of an era (or at any rate, the poems we can get from an era) to adhere to the prosodic rules that evolved alongside them. For example, here is the variation in Hood Distance we see over the course of some of the eras tagged in the Chadwyck-Healey poetry corpus:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/Regularity-2-Metrical-Typicality.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/Regularity-2-Metrical-Typicality.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Each dot here is a poem; the gray bars contain 50% of the poems in a given era, and the spot where they change color is the median Hood Distance for that era. The colors of the poems reflect the traditional, categorical designations of meter, but the changes in the distribution, the rise and fall of the median, show a new kind of poetic history: The rise and fall of metrical regularity. Starting in the Tudor period, Hood Distance increases through the eighteenth century; the Romantic Era then initiates a decline in regularity that persists as long as the data does, as clearly iambic and trochaic poems give way to more freewheeling meters. Like so many DH findings, this tracks roughly with what we &amp;quot;already knew&amp;quot;---but &lt;em&gt;did&lt;/em&gt; we know it? Are you sure you wouldn&#39;t have picked the Augustan Age for peak regularity? What would you have said about the Victorian Era as against the Restoration? If knowledge is (more or less) true justified belief, we now have more empirical justification and a more precise sense of the truth, where neither were quite as possible before.&lt;/p&gt;
&lt;p&gt;In the end, Hood Distance gives us a new lens for measuring a technical feature of poetics on the scale provided by digital research. And conceptually speaking, Hood Distance is not the only thing we can measure this way. In theory, we could track the distance from any poet, defining meter in terms of, say, a Dickinson distance. Or we could change the axes, putting, say, Hood Distance itself on one, and the percentage of line-ending rhymes on the other, to measure the license that rhyme gives (if any) to metricality. Meter is a complex structure that has varied widely in theory and practice over time, but this concept of spatial arrangement derived from programmatic analysis of thousands of poems enables us to construct frameworks for describing poetic history with the usual DH mixture of the very broad and the very specific. It&#39;s the D and the H as a dialectic; this metrical space is the synthesis of two kinds of technical exploration.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;_ftn1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Ryan Heuser, the primary programmer during this project, has written two approachable summaries of Prosodic for those who wish to use it, &lt;a href=&quot;https://github.com/quadrismegistus/litlab-poetry&quot;&gt;one&lt;/a&gt; more specific to this project, and &lt;a href=&quot;https://github.com/quadrismegistus/prosodic&quot;&gt;one&lt;/a&gt; (more frequently updated) for more general use. For a more linguistics-oriented introduction to the software, see: Anttila, Arto, and Ryan Heuser. &amp;quot;Phonological and Metrical Variation across Genres.&amp;quot; &lt;em&gt;Proceedings of the Annual Meeting on Phonology&lt;/em&gt; 3 (2016). Web. Linguistic Society of America. Washington D.C., 2015. The paper is &lt;a href=&quot;https://journals.linguisticsociety.org/proceedings/index.php/amphonology/article/view/3679&quot;&gt;available here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;_ftn2&quot;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; I wanted to use the more thematically-appropriate &amp;quot;Split the lark and you&#39;ll find the music&amp;quot;, but it&#39;s more typically Dickinsonian---i.e., too irregular to serve as an example of &amp;quot;perfect&amp;quot; meter.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;_ftn3&quot;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; We thought of including other feet, but as Jonny Sensenbaugh writes: &amp;quot;Although other classical metrical feet such as the spondee and the amphibrach exist, the existence of truly spondaic poems is doubted in linguistic theory (*Princeton Encyclopedia of Poetry and Poetics, *1352), and that of English amphibrachic poems both exceedingly rare and difficult to distinguish from anapestic or dactylic meter (45)&amp;quot;.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;_ftn4&quot;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; Iambs and dactyls are stressed on the fourth syllable (-/-/ and /--/, respectively), whereas trochees and anapests are unstressed there (/-/- and --/-, respectively).&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;_ftn5&quot;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; Here&#39;s a simplified explanation showing why this works: In Prosodic, a perfectly iambic line (with, let&#39;s say, four feet) would read &amp;quot;w-s-w-s-w-s-w-s&amp;quot;; a perfectly trochaic line would read &amp;quot;s-w-s-w-s-w-s-w&amp;quot;. The &amp;quot;w-w&amp;quot; pair (the minimum unit of consecutive weak syllables) never appears in either line; even if an iambic or trochaic poem had a &amp;quot;mistake&amp;quot; here or there, that &amp;quot;w-w&amp;quot; pair would still be relatively rare. Compare a perfect anapestic line, &amp;quot;w-w-s-w-w-s-w-w-s-w-w-s&amp;quot;, or a perfect dactylic line, &amp;quot;s-w-w-s-w-w-s-w-w-s-w-w&amp;quot;. These are rife with the &amp;quot;w-w&amp;quot; pair, and even with variations here and there in the poem, the pair should still occur frequently.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;_ftn6&quot;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; On a line-by-line level, the accuracy was lower---59%. The accuracy for a poem as a whole, however, depends only on getting &lt;em&gt;most&lt;/em&gt; of the lines right, so two or three lines of a sonnet can be mischaracterized as trochaic and the program will still place the poem in the iambic quadrant. In a loose sense, then, the overall metrical scheme of the poem influences the &amp;quot;interpretation&amp;quot; of individual lines---not too dissimilar from human reading.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;_ftn7&quot;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; Note that in British poetry of this period, &amp;quot;heaven&amp;quot; is usually treated as a one syllable word.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;_ftn8&quot;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt; Note the italicized &amp;quot;&lt;em&gt;is&lt;/em&gt;&amp;quot; in the first line. To a human, the italics probably indicate that the word is stressed; Prosodic, though, does not see typeface. To my ear, the poem also follows a loose ballad meter, with the 4/3 beat structure rendered as 7 stresses per line---but it is so irregular that it does not match very well with any of the four metrical feet Prosodic tracks. These are just two examples of metrical complexity that our project does not capture.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;_ftn9&quot;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt; Properly speaking, the most accurate measure would be a Hood &lt;em&gt;Vector&lt;/em&gt;. Vectors include information about direction, so they would show whether a poem was far from Hood by virtue of occupying the iambic corner, the dactylic corner, the boundary between anapest and dactyl, etc. With this accuracy, however, comes a tradeoff in ease of understanding and use; e.g., although we can envision Hood Distance as a very promising pedagogical tool in explaining how meter works, it loses some of its appeal if English students are expected to calculate vectors.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on “Operationalizing the Change. Literary Transition in Poland Viewed Through Bibliographical Data (1989-2002)”</title>
    <link href="https://litlab.stanford.edu/news/2018-03-12-presentation-on-operationalizing-the-change-literary-transition-in-poland-viewed-through-bibliographical-data-1989-2002/"/>
    <updated>2018-03-13T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2018-03-12-presentation-on-operationalizing-the-change-literary-transition-in-poland-viewed-through-bibliographical-data-1989-2002/</id>
    <content type="html">&lt;p&gt;On March 13th, 2018, Maciej Maryl presented his work on literary transition in Poland from 1989-2002.&lt;/p&gt;
&lt;p&gt;Operationalizing the Change. Literary Transition in Poland Viewed Through Bibliographical Data (1989-2002)&lt;/p&gt;
&lt;p&gt;I will be presenting preliminary results of the quantitative research into transitions of Polish literary life 1989-2002. The abundance of scholarly and critical writing about this period makes it interesting from the perspective of data-driven research, allowing for validation of critical claims on the basis of the existing data. I will focus on operationalising and plotting key qualitative hypotheses about this period, namely dispersion and recentralisation of literary life after 1989, debut-centrism of the literary criticism in the early 1990s and the subsequent “return of old masters” in the second part of the decade.&lt;/p&gt;
&lt;p&gt;The research is based on the Polish Literary Bibliography, a comprehensive database of Polish literary and cultural life, which indexes not only literary books but also other instances of literary life and reception. The bibliographical dataset I compiled for this study includes metadata about 24,025 writers and poets, 22,503 critics, 188,633 creative works (incl. 37,294 books) and 90,539 critical writings (e.g. reviews, interviews, profiles). Since big datasets tend to cause big problems, this talk will also address the pitfalls of data provenance.&lt;/p&gt;
&lt;p&gt;Maciej Maryl, Ph.D., assistant professor at and the founding head of the Digital Humanities Centre at the &lt;a href=&quot;http://chc.ibl.waw.pl/en/&quot;&gt;Institute of Literary Research of the Polish Academy of Sciences&lt;/a&gt;. He chairs a COST action New Exploratory Phase in &lt;a href=&quot;http://www.cost.eu/COST_Actions/ca/CA16213&quot;&gt;Research on East European Cultures of Dissent&lt;/a&gt; and coordinates the data project: &lt;a href=&quot;http://chc.ibl.waw.pl/en/projects/pbl-lab/&quot;&gt;Polish Literary Bibliography&lt;/a&gt; – a knowledge lab on contemporary Polish culture. He is also involved in &lt;a href=&quot;https://dariahre.hypotheses.org/working-groups/digital-methods-practices-and-ontologies&quot;&gt;DARIAH Digital Methods and Practices Observatory WG (DiMPO)&lt;/a&gt;, &lt;a href=&quot;http://www.allea.org/working-groups/overview/working-group-e-humanities/&quot;&gt;ALLEA E-humanities Working Group&lt;/a&gt;, &lt;a href=&quot;http://operas.hypotheses.org/&quot;&gt;OPERAS&lt;/a&gt; Core group, and &lt;a href=&quot;http://openmethods.dariah.eu/&quot;&gt;OpenMethods&lt;/a&gt; Editorial Board. He is currently a Fulbright Visiting Scholar at Stanford Literary Lab.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on “Reading the Norton Anthologies of American Literature”</title>
    <link href="https://litlab.stanford.edu/news/2018-05-06-presentation-on-reading-the-norton-anthologies-of-american-literature/"/>
    <updated>2018-05-07T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2018-05-06-presentation-on-reading-the-norton-anthologies-of-american-literature/</id>
    <content type="html">&lt;p&gt;On May 7th, 2018, J.D. Porter and Erik Fredner presented &amp;quot;Reading the Norton Anthologies of American Literature”.&lt;/p&gt;
&lt;p&gt;As one of the most prominent commercial literary anthologies and a pedagogical tool commonly used at both undergraduate and graduate levels, Norton has been in the business of binding literary canons for more than half a century. How, then, has Norton constructed its literary canon, and how has it changed over time? This project analyzes every text and excerpt selected for every edition of the Norton Anthologies of American Literature (NAAL) published to date. We explore the trajectories not only of individual authors and works, but broader trends of inclusion and exclusion in the Norton’s canon. Finally, this project reflects more generally on the 20th century construction and renovation of the American literary canon, focusing not on the well-known interventions of the early 20th century that canonized writers like Melville and Hawthorne, but rather on the representation of the American literary canon since 1979, when the first edition of the NAAL was released.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on “Identity”</title>
    <link href="https://litlab.stanford.edu/news/2018-05-30-presentation-on-identity/"/>
    <updated>2018-05-31T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2018-05-30-presentation-on-identity/</id>
    <content type="html">&lt;p&gt;On April 9th, 2018, Mark Algee-Hewitt, J.D. Porter and Hannah Walser presented their latest work on the Identity Project.&lt;/p&gt;
&lt;p&gt;“Representations of Race and Identity in American Fiction” explores the changing discourse of identity in the American Novel from the late eighteenth to the early nineteenth century. As the project moves toward publication, we wanted to share our latest work with the lab, including new visualizations of the discourse of the novel and a new set of findings on the “stickiness” of identity labels that have shifted the goals and outcomes of the research. We are particularly eager to think through the implication of these findings with the lab before we submit the piece for publication.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on “Microgenres”</title>
    <link href="https://litlab.stanford.edu/news/2018-05-30-presentation-on-microgenres/"/>
    <updated>2018-05-31T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2018-05-30-presentation-on-microgenres/</id>
    <content type="html">&lt;p&gt;On May 31st, 2018, there was a presentation from the Microgenres group.&lt;/p&gt;
&lt;p&gt;In this project, we explore the discursive inter-disciplinarity of novels, using machine learning to identify points at which authors incorporate the language and style of other contemporary disciplines into their narratives. How do authors signal the shift between narrative and, for example, history, philosophy or natural science? And do these signaling practices change with time or with discipline? Akin to what Bakhtin terms &amp;quot;heteroglossia,&amp;quot; these stylistic shifts indicate not only the historically contingent ways that novels are assembled from heterogeneous discourses, but they also shed light on the practices of disciplinary knowledge itself.&lt;/p&gt;
&lt;p&gt;Participants: Mark Algee-Hewitt, Michaela Bronstein, Abigail Droge, Erik Fredner, Ryan Heuser, Xander Manshel, Nichole Nomura, JD Porter, Hannah Walser&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on “Comp Titles and the Contemporary Marketplace”</title>
    <link href="https://litlab.stanford.edu/news/2018-10-10-presentation-on-comp-titles-and-the-contemporary-marketplace/"/>
    <updated>2018-10-11T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2018-10-10-presentation-on-comp-titles-and-the-contemporary-marketplace/</id>
    <content type="html">&lt;p&gt;On October 11th, 2018, Laura McGrath p her latest work on her Comp Titles and the Contemporary Marketplace project.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Popularity/Prestige: A New Canon</title>
    <link href="https://litlab.stanford.edu/pop-pres-ny/"/>
    <updated>2018-10-29T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/pop-pres-ny/</id>
    <content type="html">&lt;p&gt;Shortly after the Lab released my &lt;a href=&quot;https://litlab.stanford.edu/LiteraryLabPamphlet17.pdf&quot;&gt;recent pamphlet&lt;/a&gt; on the structure of the literary canon, &lt;em&gt;New York&lt;/em&gt; magazine &lt;a href=&quot;http://www.vulture.com/article/best-books-21st-century-so-far.html&quot;&gt;ran an article&lt;/a&gt; about the 21st century canon, in which a panel of judges pick an early version of the literary canon from the century so far.&lt;a href=&quot;https://litlab.stanford.edu/pop-pres-ny/#_ftn1&quot;&gt;[1]&lt;/a&gt; The structure of their canon is a list, approximately 100 books published (mostly) since 2000---but I wondered how it would fare under the terms laid out in the pamphlet. How does the &lt;em&gt;New York&lt;/em&gt; canon look when it comes to popularity and prestige?&lt;/p&gt;
&lt;p&gt;The basic idea behind the pamphlet is that literature becomes canonical in a variety of ways, and a structure like a list can&#39;t always capture the complexity of the canon as we actually encounter it. The two metrics I picked to remedy this are designed to show us an arrangement of the canon based on things that academic scholars write about (prestige, at least one version of it) and things that many people know about (popularity). This helps us to understand how, say, Gertrude Stein (very prestigious, less popular), Stephen King (very popular, not as prestigious), and Jane Austen (both) relate to each other within our broader notions of canonicity. But what can it tell us about, say, Zadie Smith, W.G. Sebald, and Roberto Bolaño?&lt;/p&gt;
&lt;p&gt;I think my version of popularity applies pretty smoothly to the &lt;em&gt;New York&lt;/em&gt; list: I use the number of ratings each author has on Goodreads, basically an index of how many users have interacted with the author at all. Prestige is much trickier. In the pamphlet I use the number of academic articles that feature each writer as a primary subject author (i.e., they&#39;re tagged as being highly featured in the article), according to the Modern Language Association&#39;s database of literary scholarship. I imagine this is not the kind of prestige the &lt;em&gt;New York&lt;/em&gt; panelists care about. They&#39;re writers, editors, and critics rather than academics, so they probably aren&#39;t deeply invested in the goings on at scholarly journals. It&#39;s also not clear that MLA scores can tell us much about recent work; academia moves notoriously slowly, and it can take two years or more to get from a paper idea to a publication---so any book published after about 2016 will have had very few chances to appear as the subject of an academic journal. In this case, though, I think these caveats make things more interesting, because they give us a chance to measure one kind of canon (made by people more or less in the contemporary writing business) in terms of another kind (a slower-moving and more academic one).&lt;/p&gt;
&lt;p&gt;There&#39;s one other major difference between the &lt;em&gt;New York&lt;/em&gt; canon and the one I use in the pamphlet. They chose books; I worked with authors. Since I was trying to cover a wide variety of genres over a very long time span, authors made life a lot easier; Shakespeare, Dickinson, and Hurston struck me as somewhat more comparable to each other than &lt;em&gt;Hamlet&lt;/em&gt;, &amp;quot;Because I Could Not Stop for Death&amp;quot;, and &lt;em&gt;Their Eyes Were Watching God&lt;/em&gt;. During the research phase, though, I did try the method on individual books, which led to some interesting depictions of individual authors, each of whom has a characteristic canonical bookprint. For instance, figure 1 shows Joyce, Austen, and Dickens (note that the axes are logarithmically scaled).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/JoyceAustenDickens.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/JoyceAustenDickens.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Joyce is extremely prestigious, albeit with most of the criticism about him allocated to one book. Austen is substantially less prestigious, but much more popular; modern Goodreads users still love &lt;em&gt;Pride and Prejudice&lt;/em&gt; (even in comparison with today&#39;s novels). Dickens as he appears here has achieved Austen-like canonicity, but only in the top half of his output; he also has an entire Austen-sized corpus with distinctly less-than-Austen results. As an author, he has greater total prestige than Austen, in part because he produced so many novels for scholars to analyze; but on the level of individual books, her works tend to surpass his.&lt;/p&gt;
&lt;p&gt;After all of that preface, let&#39;s get to the results for the *New York *canon. We can start with Figure 2, which uses non-logarithmic axes in order to make the outliers clear.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/New-York-Magazine-PopPres-1024x598.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/New-York-Magazine-PopPres-1024x598.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Readers are very familiar with Gillian Flynn&#39;s &lt;em&gt;Gone Girl&lt;/em&gt;, which is beating every other book depicted here by over one million ratings (none of the others even &lt;em&gt;has&lt;/em&gt; one million ratings). Meanwhile, W.G. Sebald&#39;s &lt;em&gt;Austerlitz&lt;/em&gt; is, by a pretty sizable margin, the most prestigious book, having netted 258 academic articles.&lt;a href=&quot;https://litlab.stanford.edu/pop-pres-ny/#_ftn2&quot;&gt;[2]&lt;/a&gt; The clear difference between the two speaks to the usefulness of the graph. If you took a negative view of these choices, you might say &lt;em&gt;Gone Girl&lt;/em&gt; can&#39;t make the canon because it&#39;s a flash in the pan (no one takes it seriously) or Sebald can&#39;t because he&#39;s obscure (you can&#39;t be canonical if most people have never heard of you). Yet they&#39;re clearly both quite successful, along their particular tracks. Cormac McCarthy&#39;s &lt;em&gt;The Road&lt;/em&gt; splits the difference---a book well known enough to, say, become a successful movie, but also by a Famous Important Writer.&lt;/p&gt;
&lt;p&gt;Turning to a logarithmic depiction, in Figure 3, gives us a clearer picture of the overall structure of this canon.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/New-York-Magazine-PopPres-log-1024x598.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/New-York-Magazine-PopPres-log-1024x598.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The gray lines here reflect the median values for both prestige and popularity. It&#39;s immediately clear that most of these books have received fairly little academic attention, even when we think of them as pretty prestigious. &lt;em&gt;Leaving the Atocha Station&lt;/em&gt;, for instance, is highly regarded (it was widely praised and won some awards), yet I have personally met 25% of the people who have published MLA-recognized articles about it (his name is Alexander Manshel; he&#39;s a Lit Lab member).&lt;a href=&quot;https://litlab.stanford.edu/pop-pres-ny/#_ftn3&quot;&gt;[3]&lt;/a&gt; In fact, 35 of the books listed, or a little over a third, have no MLA articles at all. That means (and for me this rings intuitively true) that getting &lt;em&gt;any&lt;/em&gt; scholarly attention is a very strong positive signal for books published in the last 18 years.&lt;/p&gt;
&lt;p&gt;This is in part a function of time, for the reasons mentioned above. As Figure 4 shows us, the MLA number is highly contingent on the book&#39;s publication date.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/MLA-per-year-1024x598.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/MLA-per-year-1024x598.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In fact, this effect is even more pronounced than I expected: The big drop-off starts ten years ago. This points to another major (and, I suppose, obvious) difference between academic thinking about literature and that of an institution like *New York *magazine. In an English department, most people work on old literature; you&#39;d virtually never see a department that had a majority of its faculty working on the contemporary. &lt;em&gt;New York&lt;/em&gt;, by contrast, probably does not employ any medievalists. Over time, this means that academics are more likely to keep talking about the same things over and over, with a rich-get-richer effect. Whoever the Ian McEwan of 1895 was, &lt;em&gt;New York&lt;/em&gt; is unlikely to mention him much these days. Meanwhile &lt;em&gt;Hamlet&lt;/em&gt; has accrued 2,169 articles since 2000---399 more than everything in Figure 4 combined.&lt;/p&gt;
&lt;p&gt;There&#39;s a conspicuous absence in the graphs so far, however, and addressing it helps to close that gap quite a bit. The &lt;em&gt;New York&lt;/em&gt; editors included several book series in their list, like Elena Ferrante&#39;s Neapolitan novels or N.K. Jemisin&#39;s Broken Earth trilogy. I didn&#39;t include those in the images above, because I wasn&#39;t quite sure how to show them. Here&#39;s the original, non-logarithmic image, with every novel from each series included.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 5&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/New-York-Magazine-PopPres-with-series-1024x596.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/New-York-Magazine-PopPres-with-series-1024x596.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In prestige terms, Margaret Atwood&#39;s &lt;em&gt;Oryx and Crake&lt;/em&gt; (part of her MaddAddam trilogy) is the big addition, coming in third overall. But an even more striking story is clearly Harry Potter. I&#39;ll admit that when I first heard about the &lt;em&gt;New York&lt;/em&gt; article, Harry Potter was my first thought---inspired, most likely, by J.K. Rowling&#39;s incredible popularity in the data for the pamphlet. The Potter series here has 18.2 million Goodreads ratings; the entire non-Potter corpus has 7.7 million. Some of that is recency; as I note in the pamphlet, the best-sellers of 1918 are mostly forgotten today, so popularity in 2018 might be equally ephemeral, if we&#39;re trying to project forward. But the first Potter novel is actually the oldest book on this list (it came out about a decade before Goodreads existed), and it has 5.6 million ratings by itself. The numbers just confirm what we all know---these books were the major popular literary event of the century so far.&lt;/p&gt;
&lt;p&gt;Oddly enough, the &lt;em&gt;New York&lt;/em&gt; panel almost didn&#39;t include them. They divided their list into four tiers: Best Book of the Century (So far); 12 New Classics; The High Canon (books chosen by at least two panelists); and The Rest. The first tier only contained one book, Helen DeWitt&#39;s &lt;em&gt;The Last Samurai&lt;/em&gt; (&lt;a href=&quot;https://en.wikipedia.org/wiki/The_Last_Samurai&quot;&gt;no relation&lt;/a&gt;), which is actually in the southwest, least canonical quadrant here, with 4,105 Goodreads reviews and 1 MLA article. I&#39;ll admit that I found their choice of that novel pretty surprising; maybe this is an index of some pretty substantial institutional differences in literary consecration. Still, it&#39;s worth remembering that making a list like this is often more an exercise in canon creation than in canon reflection. I think in DH and other empirical literary critical fields, we often think of the canon as something out there in the world for us to study, which it is; but as in many other fields, observation changes the object. For the &lt;em&gt;New York&lt;/em&gt; panel, this way of thinking about things was probably pretty paramount. You don&#39;t necessarily pick &lt;em&gt;Ghachar Ghochar&lt;/em&gt; because you think it has had a significant impact on (English language) literary culture; you pick it because you want it to.&lt;/p&gt;
&lt;p&gt;Perhaps as a result of motivations like these, the Harry Potter series made the last tier, meaning, I suppose, that only one person picked it. Could this be a prestige problem? Perhaps; I recall presenting my earlier Pop/Pres data for a French audience, and most of them scoffed at the prospect that any French academic would ever write about someone like Rowling (perhaps she should have tried &lt;a href=&quot;https://en.wikipedia.org/wiki/Mythologies_(book)&quot;&gt;professional wrestling&lt;/a&gt;). Yet Figure 5 shows that she&#39;s right up there with all sorts of prestigious novels; the first Potter novel is beating &lt;em&gt;The Corrections&lt;/em&gt; and three books by J.M. Coetzee.&lt;/p&gt;
&lt;p&gt;Of course, &lt;em&gt;Harry Potter and the Philosopher&#39;s/Sorcerer&#39;s Stone&lt;/em&gt; came out in 1997, early for this canon; but there&#39;s another factor to consider, too, the one that kept me from including series in the original images.&lt;a href=&quot;https://litlab.stanford.edu/pop-pres-ny/#_ftn4&quot;&gt;[4]&lt;/a&gt; The Potter titles sum to 137 MLA articles---respectable, but less than &lt;em&gt;Austerlitz&lt;/em&gt;, &lt;em&gt;The Road&lt;/em&gt;, or &lt;em&gt;Oryx and Crake&lt;/em&gt;. If you just look up &amp;quot;Harry Potter&amp;quot;, though, (with Rowling in another field, so we&#39;re not getting just any Harrys or Potters), you get 775 MLA articles.&lt;a href=&quot;https://litlab.stanford.edu/pop-pres-ny/#_ftn5&quot;&gt;[5]&lt;/a&gt; In other words, more people are writing about the Harry Potter series than about any one of the novels (or about any of the rest of these books) by a substantial margin. It&#39;s a bit like Sherlock Holmes; scholars might write about &lt;em&gt;Hound of the Baskervilles&lt;/em&gt; or &lt;em&gt;The Speckled Band&lt;/em&gt;, but often they just write about Holmes himself. The literary contribution is not well captured by any particular publication---which makes it difficult to depict on a graph.&lt;/p&gt;
&lt;p&gt;Nonetheless, in prestige as in popularity, Rowling is crushing it. Figure 6 is an attempt to depict each series as a single collective point; it receives the sum of the Goodreads ratings for each constituent book, plus whichever MLA score is higher between A) summing the individual volumes or B) looking up the series as a whole (using both would risk double counting articles). This isn&#39;t quite fair to the other books on the list; after all, it&#39;s surely easier to amass Goodreads ratings from your fans if you give them multiple books to rate. Nonetheless, the results are pretty stark. Seen this way, the MaddAddam trilogy is approximately as canonical as &lt;em&gt;The Road&lt;/em&gt;, and the Harry Potter series is by far the most canonical thing on the graph---in the pamphlet, I came to think of that space in the top right corner as the Shakespeare Position. In this company, Shakespeare is clearly J.K. Rowling.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 6&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/New-York-PopPres-with-Collective-Series-1024x604.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/New-York-PopPres-with-Collective-Series-1024x604.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you&#39;re like me, the fun part of all this is speculating about how this canon will age. What will the literary scholars of 2118 make of this period, or this list? I have my non-quantitative opinions, of course, often about things that didn&#39;t make the &lt;em&gt;New York&lt;/em&gt; list.&lt;a href=&quot;https://litlab.stanford.edu/pop-pres-ny/#_ftn6&quot;&gt;[6]&lt;/a&gt; I also don&#39;t think this way of measuring canonicity can offer much of a negative signal, especially given the time constraints. Helen Oyeyemi and Valeria Luiselli seem like strong candidates for the canon to me, but their work is too recent to be sure (that&#39;s not just a problem with these graphs, of course---recent work is always more difficult to evaluate for the &lt;em&gt;longue durée&lt;/em&gt;). Moreover, many metrics are just not captured here, or not capturable. I find myself thinking about Marlon James&#39;s &lt;em&gt;A Brief History of Seven Killings&lt;/em&gt; pretty often, and it&#39;s not quite in the northeastern, most canonical quadrant yet; I understood the most recent U.S. Open entirely differently, in real time, because of Claudia Rankine&#39;s &lt;em&gt;Citizen&lt;/em&gt;---how do you measure an effect like that?&lt;a href=&quot;https://litlab.stanford.edu/pop-pres-ny/#_ftn7&quot;&gt;[7]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Still, I think we can take some positive signals from what we see here. &lt;em&gt;Austerlitz&lt;/em&gt; is in great shape; this accords pretty well with my sense of Sebald&#39;s uptake among academics. It may also be noteworthy that he made this list in spite of writing in another language; there are only a few such cases here. That could be a good sign for &lt;em&gt;2666&lt;/em&gt;; add Bolaño&#39;s global reach to the information here, and his work looks formidable. McCarthy&#39;s &lt;em&gt;Road&lt;/em&gt; is doing so well that I have to imagine it will persist for a while. For me, though, since I&#39;m unfamiliar with the series, the *Oryx and Crake *results were the most surprising. In a way, Atwood has already begun to stand the test of time; &lt;em&gt;The Handmaid&#39;s Tale&lt;/em&gt; came out thirty years ago, and it clearly still has an impact today. Atwood is well positioned to stay put in the Northeast quadrant.&lt;/p&gt;
&lt;p&gt;And of course there&#39;s Harry Potter; I think those novels are already in, for the same reason Sherlock Holmes made it one hundred years earlier. At a certain point, you&#39;re so popular that people can&#39;t avoid talking about you, even if only to try and understand your popularity. If you look back at Figure 3, the &lt;em&gt;New York&lt;/em&gt; selections are concentrated in two places: In the Northeast, canonical quadrant, and scattered along the &amp;quot;no MLA articles&amp;quot; axis at the bottom. Much more than in the broad literary canon depicted in the pamphlet, the recent canon unites popularity and prestige---to get one, you really need the other. In a few cases I think various kinds of prestige probably led the charge (e.g., with Ngũgĩ wa Thiong&#39;o&#39;s &lt;em&gt;Wizard of the Crow&lt;/em&gt;, which has only 2,106 Goodreads ratings, but has already amassed 46 MLA articles.) Typically, though, it appears that readerly attention is something of a precursor for critical attention; when something is widely read (and it&#39;s clear that reading happens much faster than academically analyzing), scholars are more apt to take notice. Harry Potter is a kind of apotheosis of that principle. We can&#39;t know who else will make the canonical library, but when they arrive, Hermione will have gotten there first.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;pop-pres-ny/#_ftn1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;  Note that the link is to &lt;em&gt;Vulture&lt;/em&gt; rather than to &lt;em&gt;New York&lt;/em&gt; per se (the two are connected in some corporate structure or another). It&#39;s tempting to call this the &amp;quot;Vulture Canon&amp;quot;, but I first encountered it in the print version of *New York *magazine, and I like the way &amp;quot;New York&amp;quot; suggests the world of professional writers/critics/editors in question, so with some regrets I&#39;ll use &amp;quot;New York Canon&amp;quot; in this post.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;pop-pres-ny/#_ftn2&quot;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;  It&#39;s a little more difficult to look up books than authors, since titles often consist of common words (e.g., Ali Smith&#39;s &lt;em&gt;How to Be Both&lt;/em&gt;). Minor differences also matter more at this scale than they did in the pamphlet---being off by two articles is more significant for a book with 10, in comparison with an author who has 6,000. My method was to start with the title in the Primary Subject Work field, and the author&#39;s last name in a general field. I also tried titles in the original language where applicable, and in a few cases titles in the general field. As a rule I tried to give a book the maximum plausible number of articles I could find. I spot-checked the results; I think they will generally hold up, but it&#39;s always possible I missed something.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;pop-pres-ny/#_ftn3&quot;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;  You can read Manshel&#39;s article &lt;a href=&quot;http://post45.research.yale.edu/2017/09/the-rise-of-the-recent-historical-novel/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;pop-pres-ny/#_ftn4&quot;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; * The Corrections* came out in 2001, and one of the Coetzee novels (&lt;em&gt;Boyhood&lt;/em&gt;) is also from 1997, so Rowling doesn&#39;t have *that *much of a head start over those two.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;pop-pres-ny/#_ftn5&quot;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;  Readers of the pamphlet may notice that this is about 50% more articles than Rowling had as a whole in that data. That&#39;s because that data was a few years old; the information in this post was collected in October 2018.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;pop-pres-ny/#_ftn6&quot;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;  Here&#39;s a self-indulgent footnote: I&#39;d have included Harryette Mullen&#39;s &lt;em&gt;Sleeping with the Dictionary&lt;/em&gt;, César Aira&#39;s &lt;em&gt;An Episode in the Life of a Landscape Painter&lt;/em&gt;, Allison Bechdel&#39;s &lt;em&gt;Fun Home&lt;/em&gt;, Toni Morrison&#39;s &lt;em&gt;A Mercy&lt;/em&gt;, and Cixin Liu&#39;s &lt;em&gt;The Three Body Problem&lt;/em&gt; trilogy.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;pop-pres-ny/#_ftn7&quot;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;  In general, non-novel genres suffered here, probably because they don&#39;t attain their peak audiences or critical attention through the book format. Frederick Seidel and Fred Moten are trapped in the least canonical quadrant, but that doesn&#39;t mean much about their poems. And for my money Anne Carson seems destined for enduring canonicity, but &lt;em&gt;The Beauty of the Husband&lt;/em&gt; doesn&#39;t quite capture what she&#39;s up to.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on “Novel Worldbuilding”</title>
    <link href="https://litlab.stanford.edu/news/2018-10-29-presentation-on-novel-worldbuilding/"/>
    <updated>2018-10-30T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2018-10-29-presentation-on-novel-worldbuilding/</id>
    <content type="html">&lt;p&gt;The created worlds of novels exist on the borders between mimesis and invention, pulling from the real-world experience of the author and her readers to create a fictionalized setting for plot, theme, and narrative. But as the worlds built by authors become less reflective and more inventive, particularly in twentieth-century genres like Science Fiction and Fantasy, the strategies the author employs to educate her readers on the rules of the newly-invented world while maintaining the flow of narrative change accordingly. In this project, we explore the narrative techniques and microgeneric shifts that allow authors of genre fiction to simultaneously create and communicate invented worlds.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on “Typicality”</title>
    <link href="https://litlab.stanford.edu/news/2018-12-04-presentation-on-typicality/"/>
    <updated>2018-12-05T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2018-12-04-presentation-on-typicality/</id>
    <content type="html">&lt;p&gt;The “typical” novel often serves as a point of comparison in the process of literary critique, particularly in discussions of how the canonical departs from the majority of literary fiction. But what is the typical novel? While traditional literary criticism confuses typicality with exemplarity, this project investigates the quantitatively typical, or “average” novel. What novels lie closest to the semantic, topical or lexical mean? Can we measure an author’s most typical work, or even passage? And how do the pressures of novelty and influence drive what is considered “typical” in American literature? Algee-Hewitt and Fredner are excited to share their latest round of results with the lab and eager to have your feedback as they continue to explore literary typicality.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on “Global Digital Humanities: Text Mining Multilingual Literary Corpora”</title>
    <link href="https://litlab.stanford.edu/news/2019-01-23-presentation-on-global-digital-humanities-text-mining-multilingual-literary-corpora/"/>
    <updated>2019-01-24T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2019-01-23-presentation-on-global-digital-humanities-text-mining-multilingual-literary-corpora/</id>
    <content type="html">&lt;p&gt;On January 24th, Mark Algee-Hewitt presented a new project: “Global Digital Humanities: Text Mining Multilingual Literary Corpora.”&lt;/p&gt;
&lt;p&gt;Building on our collaborative, Mellon funded, History of “Literature”/Histoire de “littérature” project with CNRS in Paris (and drawing on our 8 additional active collaborations with other European Digital Humanities Centers), this project looks towards new ways of aligning the results of the quantitative analysis of literary phenomena across languages.&lt;/p&gt;
&lt;p&gt;Although still in its early stages, our work seeks to establish a set of methods that will allow research on multi-lingual, or even global, literary phenomena without resorting to the inaccuracies of translation. This will not only allow Digital Humanities researchers to investigate multi-national literary phenomena, but will also help to redress the inequalities created by the over-representation of English language texts in quantitative literary study.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on “Fanfiction: Generic Genesis and Evolution”</title>
    <link href="https://litlab.stanford.edu/news/2019-02-13-presentation-on-fanfiction-generic-genesis-and-evolution/"/>
    <updated>2019-02-14T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2019-02-13-presentation-on-fanfiction-generic-genesis-and-evolution/</id>
    <content type="html">&lt;p&gt;On February 14th, 2019, Steele Douris, Laura McGrath, J.D. Porter, and Quinn Dombrowski presented present their project, &amp;quot;Fanfiction: Generic Genesis and Evolution.&amp;quot;&lt;/p&gt;
&lt;p&gt;Fan fiction has become a massively productive region of the literary landscape, generating thousands of new readerly and textual communities that share fictional universes and critical feedback. We examine one subset of this scene—novel-length Harry Potter fan fiction from fanfiction.net—with respect to genre and character tags to determine how the plots and characters that preoccupy the fandom differ from those that receive the most attention in the 7 canonical books. Using some of the most common tropes, characterizations and plot devices utilized in fanfiction, we offer a method for categorizing and tracking the ways in which fanfiction authors (impressive close readers in their own right) interact with and deviate from the Harry Potter canon and each other.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on “WhatEvery1Says: Interpreting Student Responses to the Humanities”</title>
    <link href="https://litlab.stanford.edu/news/2019-03-07-presentation-on-whatevery1says-interpreting-student-responses-to-the-humanities/"/>
    <updated>2019-03-08T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2019-03-07-presentation-on-whatevery1says-interpreting-student-responses-to-the-humanities/</id>
    <content type="html">&lt;p&gt;On March 8th, 2019, Abigail Droge presented her project, &amp;quot;WhatEvery1Says: Interpreting Student Responses to the Humanities.&amp;quot;&lt;/p&gt;
&lt;p&gt;Abigail Droge, a Lit Lab alum and current postdoc at UC Santa Barbara, shared research from the digital humanities project WhatEvery1Says (WE1S), based at UCSB, CSU Northridge and the University of Miami. The project uses digital methods to understand public discourse around the humanities in popular media since the 1980s, with the ultimate goal of using that research to inform the creation of humanities advocacy resources. This talk will focus on topic models made from sources particularly related to the student experience of the humanities in higher education. What associations do students make with the humanities and how might an understanding of this landscape help us to increase the accessibility and impact of humanistic disciplines? The talk will also focus on the intersections between digital humanities and other methodologies, such as human subjects research.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on “Wild Animal Stories: Modeling Anthropomorphism in Animal Writing, 1870-1930”</title>
    <link href="https://litlab.stanford.edu/news/2019-03-13-presentation-on-wild-animal-stories-modeling-anthropomorphism-in-animal-writing-1870-1930/"/>
    <updated>2019-03-14T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2019-03-13-presentation-on-wild-animal-stories-modeling-anthropomorphism-in-animal-writing-1870-1930/</id>
    <content type="html">&lt;p&gt;On March 14th, 2019, Vicky Googasian and Ryan Heuser will present their project, &amp;quot;Wild Animal Stories: Modeling Anthropomorphism in Animal Writing, 1870-1930.&amp;quot;&lt;/p&gt;
&lt;p&gt;Is an animal a person? The question is far from idle; it is fraught with urgent ethical and legal consequences for animal welfare, and it shapes scientific norms for the study of animal behavior. It also remains a constant theme in Western philosophy from Rene Descartes through present-day Critical Animal Studies. However, lawyers, philosophers, and ethologists are not the only deciders in this question: cultural representations of animals also mediate their relation to personhood. Fiction, for instance, excels in the representation of human individuality, interiority, and action; complex, “round” characters of course populate the long history of prose fiction. How, then, does fiction engage with the personhood of animals? In fiction, when is an animal a character? What do animals do in the pages of fiction? Do they make decisions, have feelings, express interiority? Do animals function more similarly to human characters, or to things, objects, and machines? In this presentation, we will begin to approach these questions with computational methods. In a variety of corpora—from popular natural history to scientific writing about animal behavior to animal-driven fictions historically accused of anthropomorphism—we compare the semantic and syntactic footprints left behind by animals and humans. We discover that, from a computational standpoint, animals in fiction are indeed recognizable as characters, albeit characters more likely to register intentionality through physical movement over speech and to display a mental paradigm colored by instinct and associative learning. Natural history writing, on the other hand, narrates animals in ways that seem surprisingly human-like when compared to animal representations in fiction more broadly. Ultimately, our results suggest the many dimensions of anthropomorphism and the variety of scales at which it operates.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Literary Lab talks at DH 2019</title>
    <link href="https://litlab.stanford.edu/news/2019-03-14-literary-lab-talks-at-dh-2019/"/>
    <updated>2019-03-15T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2019-03-14-literary-lab-talks-at-dh-2019/</id>
    <content type="html">&lt;p&gt;Three presentations from the Stanford Literary Lab will be presented at DH 2019 in Utrecht:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;J.D. Porter, Mark Algee-Hewitt, Erik Fredner, Michaela Bronstein, Alexander Manshel, Nichole Nomura, and Abigail Droge, &amp;quot;Microgenres: A computational model of disciplinarity and the novel&amp;quot;&lt;/li&gt;
&lt;li&gt;Victoria Googasian &amp;amp; Ryan James Heuser, &amp;quot;Digital Animal Studies: Modeling Anthropomorphism in Animal Writing, 1870-1930&amp;quot;&lt;/li&gt;
&lt;li&gt;David Mimno, Meredith Martin and Mark Algee-Hewitt, &amp;quot;The Sonnet Stretcher&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on “Voice”</title>
    <link href="https://litlab.stanford.edu/news/2019-04-24-presentation-on-voice/"/>
    <updated>2019-04-25T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2019-04-24-presentation-on-voice/</id>
    <content type="html">&lt;p&gt;On April 25th, 2019, Annika Butler-Wall, Xander Manshel, Nika Mavrody, Laura McGrath, Nichole Nomura, and Alex Sherman presented their project, Voice.&lt;/p&gt;
&lt;p&gt;What is “voice” in literary criticism? What does it mean for an author to have a voice? Who has a voice, how do they find it, and once found, where does it exist? “Voice” is a central metaphorical concept in the post45 literary field. As Mark McGurl has shown, the idea of the writer’s voice has been promoted and perpetuated by the creative writing program. But “voice” (or lack thereof) also governs publishing acquisitions (McGrath), the AP English classroom (Nomura), and book review discourse (Internet). Despite the concept’s centrality, it has yet to receive any rigorous treatment; it remains undefined, imprecise—but we know it when we see it. This talk takes the imprecision of “voice” as it’s starting point, attempting to develop a rigorous and capacious definition of the concept based on its use throughout a number of reading communities in the 20th century, drawing on the internet’s largest collection of reader-response criticism: GoodReads.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on “Contemporaneity”</title>
    <link href="https://litlab.stanford.edu/news/2019-05-15-presentation-on-contemporaneity/"/>
    <updated>2019-05-16T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2019-05-15-presentation-on-contemporaneity/</id>
    <content type="html">&lt;p&gt;On May 16, 2019, Mark Algee-Hewitt, Xander Manshel, Laura McGrath, Nichole Nomura, JD Porter, Matt Warner presented their project, &amp;quot;Contemporaneity.&amp;quot;&lt;/p&gt;
&lt;p&gt;What might it look like to consider the “contemporary” in “contemporary fiction” as a marker not of history, nor of literary history, but of form? If the category of historical fiction denotes, à la Genette, any narrative “that is explicitly placed (even by only date) in a historical past, even a very recent one,” then what terms and conventions best describe fiction that is decidedly not “placed” in the near or far past, set instead in the amorphous now? This presentation outlines and investigates what we are calling “the novel of contemporaneity,” one that takes place in the vague present, unattached from a particular period or year. Looking closely at a corpus of novels in English from the eighteenth to the twenty-first centuries, we examine how these categories might combine and overlap in interesting ways. What features attach a novel to a particular time? And does the absence of these features
place a novel in the amorphous present?&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentations on “Star Texts” and “Celebrity”</title>
    <link href="https://litlab.stanford.edu/news/2019-05-21-presentations-on-star-texts-and-celebrity/"/>
    <updated>2019-05-22T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2019-05-21-presentations-on-star-texts-and-celebrity/</id>
    <content type="html">&lt;p&gt;On May 22nd, 2019, Charlotte Lindemann, Mark Algee-Hewitt, and Laura McGrath presented on two related projects, Star Texts and Celebrity.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Star Texts&lt;/strong&gt;: A “star text,” or “star image,” as Richard Dyer’s seminal Stars (1979) defines it, is a composite media text, assembled from the various appearances, visual, verbal, and aural, of a celebrity on screen or in the press. Treating celebrity performances as sites of intertextuality, a “star text” is a metanarrative, linking each film in which a given star features. How do star texts accumulate meaning? Where are they made, and how do they circulate? Charlotte Lindemann and Mark Algee-Hewitt analyze the relationship between star texts, film reviews, and advertisements in Vogue and Harper’s Bazaar.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Celebrity&lt;/strong&gt;: Still in its early stages, Celebrity is a long-term collaboration between the Literary Lab and the Smithsonian Institute of American History. Drawing on the largest Lit Lab corpus to date, a collection of US newspapers (18th Century - present), Celebrity will map a comprehensive history of fame— its uses, abuses, and currency— in American culture. Laura McGrath will present preliminary research and outline directions for this new collaboration, in the hopes of soliciting feedback from lab members.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on “Anticorruption”</title>
    <link href="https://litlab.stanford.edu/news/2019-06-05-presentation-on-anticorruption/"/>
    <updated>2019-06-06T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2019-06-05-presentation-on-anticorruption/</id>
    <content type="html">&lt;p&gt;On June 6th, 2019, there was a presentation on &amp;quot;Anticorruption&amp;quot;, a collaboration between the Literary Lab and legal scholars Laura Galindo-Romero and Mariana Rozo Paz.&lt;/p&gt;
&lt;p&gt;The OECD Working Group on Bribery (WGB) is one of the key institutions for addressing corruption in trade between wealthy nations. Yet its disciplinary powers are quite limited, consisting mainly of “naming and shaming” via harshly worded reports and press releases. In this project, a collaboration between the Literary Lab and Colombian legal scholars Laura Galindo-Romero and Mariana Rozo Paz, we explore the publications of the OECD through sentiment, distinctive words, and coded sections of “harsh” language, exploring the effectiveness and the biases that underlie the reports and press releases.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>&#39;The Hidden Dictionary&#39; at DH 2018</title>
    <link href="https://litlab.stanford.edu/news/2019-06-25-the-hidden-dictionary-at-dh-2018/"/>
    <updated>2019-06-26T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2019-06-25-the-hidden-dictionary-at-dh-2018/</id>
    <content type="html">&lt;p&gt;Mark Algee-Hewitt will be presenting &amp;quot;The Hidden Dictionary&amp;quot; at DH 2018 in Mexico City. The introduction to the abstract is as follows:&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;While written works are often encountered by readers as linear phenomena, one of the most important conceptual advances offered by the Digital Humanities is the way that computational text analysis has permitted researchers to find non-linear patterns that speak to organizational principles embedded in even a single text. The methods developed to parse thousands, or millions, of texts can, in the context of a single work, reveal connections and patterns that are unavailable to a human reader.&lt;/p&gt;
&lt;p&gt;Even in reference books, whose alphabetic order discourages the same kind of linearity found in novels, digital methods have proven effective at revealing alternative ordering principles. This has been particular important in eighteenth-century studies. For example, recent digital work on the French Encyclopédie has sought to assess the compatibility of the multiple ways in which the text was organized by its authors. In their 2002 article, Gilles Blanchard and Mark Olsen measure the knowledge domains described by Diderot in his introduction by counting the number of renvois, or “see alsos” between articles in each domain. Similarly, Heuser, Algee-Hewitt and Bender have also reconstructed the French Encyclopédie based on which articles are connected by renvois. In both cases, an alternative structure emerges: one that speaks to connections between domains of knowledge that are more meaningful than the alphabetic layout would suggest.&lt;/p&gt;
&lt;p&gt;In this project, I employ a similar set of methodologies to explore the other foundational linguistic reference book of the eighteenth century, Samuel Johnson’s 1755 Dictionary of the English Language. While it lacks a system of renvois to counter-balance the prevailing alphabetic order it shares with Diderot’s work, it nevertheless contains a hidden system of connections between seemingly disparate articles, whose organization can only be revealed through quantitative analysis: the quotations used in the definitions of each word. These quotations are what separate Johnson’s dictionary from other, earlier dictionaries. In providing a contextual basis for assessing meaning, Johnson grounds definitions in historical usage and contingent situations. Yet, by Johnson’s own definition, the quotations have an educational and referential purpose that remains implicit within their use. And, by sheer volume, their presence is the most notable aspect of the dictionary. A given page of the 1775, second edition of the text, defines 17 words using 52 quotations. The typographical imbalance between the definitions and the quotations, which overwhelm the page, is striking, even while this is a fact that should come as no surprise to users of the OED, the spiritual successor to Johnson’s Dictionary.&lt;/p&gt;
&lt;p&gt;This project, therefore, seeks to answer three questions. First, who is cited in what contexts in the Dictionary? Here, a quantitative methodology should allow for unprecedented access to the fine-grained details of the text. Second, if Johnson’s Dictionary were rearranged to group articles connected by shared quotations together, what patterns of relationship emerge? And finally, how does Johnson use his quotations to reflect back on the works that he cites?&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on “The Afterlife of Aesthetics”</title>
    <link href="https://litlab.stanford.edu/news/2019-10-23-presentation-on-the-afterlife-of-aesthetics/"/>
    <updated>2019-10-24T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2019-10-23-presentation-on-the-afterlife-of-aesthetics/</id>
    <content type="html">&lt;p&gt;On October 24th, Mark Algee-Hewitt presented &amp;quot;The Afterlife of Aesthetics.&amp;quot;&lt;/p&gt;
&lt;p&gt;The eighteenth-century gave both critics and authors a new vocabulary through which to both express and study aesthetic experience. Yet while we have many in-depth studies on the emergence of concepts like the sublime, our narratives do not take into account the multiple and overlapping ways that these ideas emerged in the seventeenth and eighteenth-centuries and, more importantly, what happened to them in the nineteenth century. In this project, which lies at the heart of my forthcoming book, I use a combination of quantitative and critical methods on over 400,000 texts to trace a new history of the sublime across the two centuries between 1660 and 1860. The sublime, I argue, can best be defined as a pattern of language, a linked network of terms that cohere across the two centuries, and whose patterns persist in the literary and critical writing of the Romantic period and beyond.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Finding needles in 34 million haystacks</title>
    <link href="https://litlab.stanford.edu/finding-needles-in-34-million-haystacks/"/>
    <updated>2019-11-09T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/finding-needles-in-34-million-haystacks/</id>
    <content type="html">&lt;p&gt;We are working on a new collaboration with the Smithsonian Institution about the histories of fame and celebrity in the United States. To ground ourselves in public discourse surrounding these topics, we began by analyzing &lt;a href=&quot;https://www.proquest.com/products-services/databases/pq-hist-news.html&quot;&gt;ProQuest&#39;s Historical Newspapers&lt;/a&gt; corpus. Working with datasets this large---tens of millions of records, billions of tokens---posed a number of technical and intellectual challenges, which are precisely the sort of subjects for which *Techne *was created. This post is the first in a series on the challenges of scale posed by this project.&lt;/p&gt;
&lt;p&gt;Our main goal in working with the newspapers is to understand more about how they represent---and, in so doing, create---concepts of fame and celebrity. In the contemporary moment, we might think of something like the style section of the &lt;em&gt;New York Times&lt;/em&gt;, magazines, or tabloids as contributing to the print production of celebrity. Of course, newspapers seem quite peripheral to contemporary celebrity: Future historians will likely be trawling through Instagram and YouTube archives to understand its modern formation. But this project is interested in the history of celebrity in the U.S. as formulated in periods when newspapers played a more central role. Perhaps too often, that history is written around major figures like P.T. Barnum, whose particular fame can be incautiously treated as structurally similar with fame generally. While Barnum is no doubt important, we know that the history of celebrity is stranger and more multifaceted than that---not least because most famous people didn&#39;t run circuses.&lt;/p&gt;
&lt;p&gt;First, the scale of the data: The Historical Newspapers corpus advertises more than 55 million pages of content. Across those &amp;quot;pages&amp;quot;---which vary greatly in size---we have at least 16.5 billion words. In order to get a handle on huge numbers like these, we often use &amp;quot;a Proust&amp;quot; as an informal unit of measure in the Literary Lab. Proust&#39;s &lt;em&gt;In Search of Lost Time&lt;/em&gt;---which takes up 6 volumes and about 10 inches of shelf  in the &lt;em&gt;Modern Library&lt;/em&gt; translation on my shelf---totals over 1 million words. ProQuest&#39;s Historical Newspapers collection contains at least 14,000 Prousts.&lt;/p&gt;
&lt;p&gt;One does not simply click on a folder containing 16.5 billion words. Even overpowered computers would struggle to list that many files. Worse still, &lt;a href=&quot;https://www.sherlock.stanford.edu/&quot;&gt;Stanford&#39;s supercomputer&lt;/a&gt;, Sherlock, caps not only the amount of disk space one can use (which is perfectly reasonable), but also the total number of files a given user can store (also reasonable, but, for our purposes, annoying). As happens often enough in text analysis, the absolute amount of data in gigabytes is less problematic than the massive number of small files to be analyzed.&lt;/p&gt;
&lt;p&gt;In this case, the unusually high number of unusually small files led to an unusual corpus structure. ProQuest&#39;s newspapers come compressed as a large number of tarballs (which are compressed files similar to .zip archives) each of which contains about 25,000 tiny XML files. Each of these XML files in turn contains a single article or other text from a single issue of a single newspaper. Besides articles, other texts might include advertisements, poems, etc. The American subset of the ProQuest corpus we have been working with contains 34 million such files spread across hundreds of tarballs.&lt;/p&gt;
&lt;p&gt;Usually, our corpora exist as directories of text files with some form of associated metadata stored in a tabular format like CSV. For instance, the corpus from &lt;a href=&quot;https://litlab.stanford.edu/LiteraryLabPamphlet8.pdf&quot;&gt;Pamphlet 8 (Algee-Hewitt and McGurl)&lt;/a&gt; is enormous by literary critical standards---containing several hundred novels from across the twentieth century---but it is computationally simple to work with compared with ProQuest. Sometimes differences in degree amount to differences in kind, and that is certainly the case when we compare that by-now-familiar scale of several hundred novels to ProQuest&#39;s thousands of Prousts.&lt;/p&gt;
&lt;p&gt;Because we could not decompress these files on disk without it either a) crashing Sherlock, b) timing out, or c) exceeding our file quota, we had to figure out how to work with them while they were still compressed. I hadn&#39;t done this before, and had to develop a new workflow for it, which I will be publishing on GitHub for the second post in this series. It turns out that working with compressed files is trivially easy, and one of the major conclusions of this work for me so far may be a shift in how I imagine storing corpora on disk. Once they get up to even a medium size, it may make sense to work with text files in compressed form (especially if your hard drive, like mine, is overcrowded with old text files and bad results you never got around to deleting). You merely need an extra column in your metadata table indicating the archive where the file is located to access it. But I&#39;m getting ahead of myself.&lt;/p&gt;
&lt;p&gt;We considered multiple options to solve this problem of scale. Again, we did not lack for storage space on Sherlock, but rather we were bumping up against the system&#39;s limit for the total number of files in our workspace. One method we considered and decided against would have extracted all of the individual files in an archive into a single large text file. All of the articles, advertisements, obituaries, etc., which are currently stored as individual XML files with ProQuest&#39;s rich metadata, would be added to a single text document, and could be parsed from there. That would have solved our problem of not being allowed to have millions of tiny files extracted on disk. These aggregate files would have been more similar in size to the text files we ordinarily work with, maxing out at a few dozen megabytes. But as an approach dumping them all into big text files seemed, to use a term of art, dumb.&lt;/p&gt;
&lt;p&gt;We soon learned that Python has a built-in module for &lt;a href=&quot;https://docs.python.org/3/library/tarfile.html&quot;&gt;working with compressed files&lt;/a&gt; that does exactly what we needed. Using this module, you can iteratively read compressed files into memory, manipulate them, and move on &lt;em&gt;without&lt;/em&gt; decompressing the archive. To me, this seemed like a bit of wizardry: it&#39;s as if you could scan a piece of paper in a filing cabinet without ever opening the drawer.&lt;/p&gt;
&lt;p&gt;Working with files at this scale had another unexpected advantage in that it required us to be a bit more efficient with our programming than usual. (After all, English Ph.D.s are rarely evaluated on their code&#39;s runtime...) Because text files are so small, suboptimal programming is usually not a serious problem. After all, the absolute amount of time required to run inefficient code is usually less than the amount of time it might take for a frankly mediocre programmer like myself to optimize it. Far better programmers than I have taken up a line from Donald Knuth as a mantra in this area: &amp;quot;...premature optimization is the root of all evil (or at least most of it) in programming.&amp;quot; That was certainly &lt;em&gt;not&lt;/em&gt; the case here. The difference between functional-but-bad and actually decent code might be a week of processing time when working with this many files.&lt;/p&gt;
&lt;p&gt;Combining lightly optimized code with Ryan Heuser&#39;s custom Python wrapper for the Message Passing Interface (MPI) on Sherlock, &lt;a href=&quot;https://github.com/quadrismegistus/slingshot&quot;&gt;Slingshot&lt;/a&gt;, we were able to extract complete metadata from all 34 million files in just a few hours. MPI, &lt;a href=&quot;https://litlab.stanford.edu/counting-words-in-hathitrust-with-python-and-mpi/&quot;&gt;as David McClure has already discussed on &lt;em&gt;Techne&lt;/em&gt;&lt;/a&gt;, distributes computational tasks across a variety of different cores on different machines. After the processing is complete, we stream all of the individual results back together. In this case, we split the metadata extraction process across 200 of Sherlock&#39;s cores, returning 200 individual datasets that can be combined into a massive metadata table that describes the whole corpus. Using that gigantic table, which &lt;a href=&quot;https://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt; handles with what can only be described as grace under pressure, we can then subset the articles by any number of factors given by the metadata: Give me all articles published south of the Mason-Dixon line between 1850 and 1860. Let me see all of the advertisements published by the &lt;em&gt;New York Times&lt;/em&gt; during the Civil Rights Era. Give me every obituary containing the phrase &amp;quot;robber baron.&amp;quot;&lt;/p&gt;
&lt;p&gt;Our first step in analysis had to involve subsetting the corpus, since it is too large to be easily (or usefully) manipulated in its entirety. The most obvious thing to do relative to our research question would have been too look for famous people in our texts. But as &lt;a href=&quot;https://ryancordell.org/research/qijtb-the-raven-mla/&quot;&gt;Ryan Cordell has pointed out&lt;/a&gt;, messy optical character recognition (OCR) causes researchers to unduly miss (and, more rarely, falsely identify) documents containing instances of their keywords. His titular example is typical: Only a literary scholar accustomed to reading bad ORC might recognize &amp;quot;Q i-jtb the Raven&amp;quot; as a corruption of the Poe line &amp;quot;quoth the raven.&amp;quot; ProQuest suffers from the same sort of OCR problems. OCR accuracy varies depending on the quality of the initial images, the text processing algorithms used, the age of the paper and the type, etc. Some articles have very high rates of accuracy, with more than 95% of all its tokens appearing in a validation dictionary, while others are much lower. Stock reports, perhaps unsurprisingly given that they contain tables, graphs, and other unusual formatting, fare worst of all.&lt;/p&gt;
&lt;p&gt;As this project is interested in documenting the history of American celebrity, we wanted to begin with people&#39;s names, specifically names that frequently recurred across the corpus. Of course, names are among the likeliest words to have serious OCR errors: many rare names do not exist in validation dictionaries, and we would need to successfully capture two well-formed names &lt;em&gt;in a row&lt;/em&gt; to get an unambiguous hit, or potentially even more if we are looking at someone conventionally called by three or more names like Harriet Beecher Stowe. Worse still, we would be over-counting people with names likely to be in the validation dictionaries (&amp;quot;John Smith&amp;quot;), and under-counting people with names less likely to be there (&amp;quot;Olaudah Equiano&amp;quot;).&lt;/p&gt;
&lt;p&gt;Although the bodies of the articles have the usual OCR imperfections Cordell addresses, we discovered that the titles are all but perfect. They appear to have been entered by hand. We don&#39;t know the names of the people who did this tremendous amount of work, but we want to express our gratitude to each and every one of them. Given the excellent reliability of the titles, we used natural language processing techniques to identify named entities that appeared there. Of course, newspaper titles have a grammar and style of their own---&amp;quot;Headless body found in topless bar&amp;quot;---and the accuracy of the named entity recognition may have suffered some as a result of the unusual dependency parsing.&lt;/p&gt;
&lt;p&gt;Fortunately, our results appear to err in the direction of over-inclusivity. It seems likely that the number of names missed in this process is lower than the number it returned; the named entity recognition process favors tagging text as a possible person even in what appear to us to be unlikely instances. For example, the two most common &amp;quot;people&amp;quot; in the titles were &amp;quot;N.Y.&amp;quot; and &amp;quot;N.J.&amp;quot; which must refer in almost all cases to New York and New Jersey.&lt;/p&gt;
&lt;p&gt;As a first approximation, we assumed that if a person appears in article titles with high relative frequency, they are more likely to be a celebrity. No doubt this overrates some celebrities and underrates others. But it is a start with the data we have. We totaled up the number of instances of unique names, and then manually filtered the results for actually existing historical persons. We did this by hand as a group. It&#39;s not always the case that you can read through a table containing tens of thousands of rows, but dividing that important if repetitious work up is one of the many advantages of the Lab&#39;s group research model. Reading through the results led to a series of provocative historical questions about the nature of celebrity. Many of the identifications we tasked ourselves with were simple: If the words &amp;quot;Malcolm X&amp;quot; appear in an article headline written after Malcolm Little&#39;s birth year, that unambiguously refers to a specific person, or to an entity named for that person. After all, the existence of &amp;quot;Malcolm X School&amp;quot; is hardly evidence &lt;em&gt;against&lt;/em&gt; Malcolm X&#39;s fame.&lt;/p&gt;
&lt;p&gt;Other cases were more ambiguous. &amp;quot;Sam Jones&amp;quot; appeared a large number of times in titles, but there are several historical persons named Sam Jones who might have appeared in newspaper titles at overlapping times. The next step in such cases is to read passages from the articles containing &amp;quot;Sam Jones&amp;quot; to identify which Sam they refer to. We also ran into cases that veered into questions of ontology. For instance, what to make of the possible celebrity of metaphorical or fictional persons? &amp;quot;Uncle Sam&amp;quot; and &amp;quot;John Bull&amp;quot;---fictional personifications of the U.S. and England, respectively---both rank highly. &amp;quot;Jim Crow&amp;quot; is a minstrelsy character, as well as a depraved legal regime. If we take his novel seriously, &amp;quot;Don Quixote&amp;quot; demands to be considered a real person.Though these names are all frequent, are they celebrities? Our manual coding scheme accounted for this ambiguity by adopting a provisional distinction between a historically specific person and a real person. &amp;quot;Muhammad Ali&amp;quot; is both specific and real. &amp;quot;Uncle Sam&amp;quot; is specific but not real. &amp;quot;Joe Shmoe&amp;quot; is neither specific nor real. We would not go to the mat for this distinction at a philosophy conference, but it&#39;s good enough to deal with these edge cases, of which there were very few. My personal favorite of these ambiguous cases was &lt;a href=&quot;http://www.americanclassicpedigrees.com/john-p-grier.html&quot;&gt;John P. Grier&lt;/a&gt;, the too-human name of a famous racehorse.&lt;/p&gt;
&lt;p&gt;Using this this data we have generated, we will be able to extract sub-corpora of articles that mention specific real people who appear frequently in titles, as well as those frequently cited in histories of American celebrity. We have P.T. Barnum&#39;s articles, but also more frequently referenced yet less well known individuals like &lt;a href=&quot;https://en.wikipedia.org/wiki/Kelly_Miller_(scientist)&quot;&gt;Kelly Miller&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Lillian_Russell&quot;&gt;Lillian Russell&lt;/a&gt;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Marian_Anderson&quot;&gt;Marian Anderson&lt;/a&gt;. The next step in our research is to learn about the forgotten histories, processes, and conceptualizations of celebrity in these documents, and how they differ from one celebrity to the next.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on “Spoken Poetry for the Public”</title>
    <link href="https://litlab.stanford.edu/news/2019-11-11-presentation-on-spoken-poetry-for-the-public/"/>
    <updated>2019-11-12T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2019-11-11-presentation-on-spoken-poetry-for-the-public/</id>
    <content type="html">&lt;p&gt;On November 12th, 2019, guests Omar Miranda (University of San Francisco) and Danny Snelson (University of California, Los Angeles), presented their project, Spoken Poetry for the Public.&lt;/p&gt;
&lt;p&gt;Can a digital humanities platform -- which focuses primarily on the sounding of poetry and the poetics of sound -- be as appealing to scholars as the general public? This talk will discuss an emerging online social community dedicated to creating and sharing original audio recordings of poetry as well as audio essays. Produced collaboratively by literary scholars, computer scientists, and graphic design artists, the platform of spoken verse is user-generated and brings together original audio recordings of poems. Aiming to transform and diversify access to the world of poetry for artists, scholars, instructors, students, and poetry enthusiasts, the project promotes new &amp;quot;readings&amp;quot; of poems through an unconventional &amp;quot;close listening&amp;quot; mode and an innovative method for &amp;quot;anthologizing&amp;quot; texts across cultures, languages, traditions, and time.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on “Distant reading of War and Peace - can we find something, that we did not know?”</title>
    <link href="https://litlab.stanford.edu/news/2019-11-21-presentation-on-distant-reading-of-war-and-peace-can-we-find-something-that-we-did-not-know/"/>
    <updated>2019-11-22T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2019-11-21-presentation-on-distant-reading-of-war-and-peace-can-we-find-something-that-we-did-not-know/</id>
    <content type="html">&lt;p&gt;On November 22, 2019, our guest presentation was be from Dr. Anastasia Bonch-Osmolovskaya, professor of Computational Linguistics at the Moscow Higher School of Economics, on: “Distant reading of War and Peace - can we find something, that we did not know?”&lt;/p&gt;
&lt;p&gt;The most fascinating question about Leo Tolstoy&#39;s masterpiece is why this tremendous novel with more than 500 characters and exhausting historical and philosophical excursus is so still so vivid, being read and also serves as a source for plays, series and opera? Is there some hidden construction inside the text, that supports its plot and composition?
In my talk I&#39;ll introduce the concept of complex layer analysis and I&#39;ll show how the combination of 5 different distant reading methods applied to one novel help to reveal rigid  structure of inner oppositions and bring some new insights about Tolstoy&#39;s art as device.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Two presentations on fanfic</title>
    <link href="https://litlab.stanford.edu/news/2019-12-04-two-presentations-on-fanfic/"/>
    <updated>2019-12-05T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2019-12-04-two-presentations-on-fanfic/</id>
    <content type="html">&lt;p&gt;On December 5th, 2019, we welcomed two presentations on fanfic: one by Mark Algee-Hewitt and Annika Butler-Wall entitled “Harry Potter and the Engaged Reader: Community Interactions and Influence in Serialized Fan Fiction&amp;quot; and one by Quinn Dombrowski, Steele Douris, Masha Gorshkova, and Antonio Lenzo, focusing on “Multilingual Fanfiction.”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract: Harry Potter and the Engaged Reader&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;(Algee-Hewitt and Butler-Wall)&lt;/p&gt;
&lt;p&gt;Fan fiction communities can help explore the complex relationships between authors and readers, offering us new opportunities to study how this reciprocal engagement affects serially published fiction. Engaged in an interactive community, invested readers become what Frank Kelleter calls “agents of narrative continuation.” The challenge for contemporary researchers is to reconstruct those communities in the absence of concrete material evidence.  Given their high publishing volume, as well as the opportunities for commenting, liking and following that the digital platform affords, quantitative approaches to the study of fan fiction can identify individual reader/author engagement and track broad trends across the archive. Our project uses the Harry Potter fan fiction archive to ask two related questions.  First, what aspects of reader engagement can predict success for a text? And, in the context of specific stories, how are authors influenced by reader suggestions for content inclusion?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract: Multilingual Fanfiction&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;(Dombrowski, Douris, Gorshkova, and Lenzo)&lt;/p&gt;
&lt;p&gt;The Harry Potter franchise is a global phenomenon, with books and films translated into hundreds of languages. While readers and viewers worldwide have the same point of departure, the fan fiction responses to that material diverge in ways that are shaped by shaped by language and culture (both national cultures and the community cultures of particular fanfic archives). In this talk, we will discuss how the affordances and conventions of three archives (Italian efpfanfic.net, Russian ficbook.net, and primarily-English fanfiction.net), and the languages and cultures of their writers, have contributed to differences ranging from rating and genre to warnings, characters, and pairings.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Lit Lab at MLA2020</title>
    <link href="https://litlab.stanford.edu/news/2020-01-07-lit-lab-at-mla2020/"/>
    <updated>2020-01-08T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2020-01-07-lit-lab-at-mla2020/</id>
    <content type="html">&lt;p&gt;Attending MLA 2020 in Seattle? Mark your dance cards for these panels and roundtable featuring current Lit Lab researchers, alums, and affiliates. We hope to see you in Seattle!&lt;/p&gt;
&lt;h2&gt;THURSDAY, 9 January&lt;/h2&gt;
&lt;h3&gt;Session #165: Digital Humanities and Nathaniel Hawthorne&lt;/h3&gt;
&lt;p&gt;7:00 – 8:15 PM | WSCC – 607&lt;/p&gt;
&lt;p&gt;Christopher Glen Diller, Berry College, Presiding&lt;/p&gt;
&lt;p&gt;“‘Nor Less Devoted to the Affairs of the Nation’: Using Word Vectors to Model Hawthorne’s Concept of the National,” &lt;strong&gt;Erik Fredner, Stanford University&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;“Actor-Network Theory and the Hawthorne Digital Archive,” Gale M. Temple, University of Alabama, Birmingham&lt;/p&gt;
&lt;p&gt;“The Digital Faun,” Evander Price, Harvard University&lt;/p&gt;
&lt;h2&gt;FRIDAY, 10 January&lt;/h2&gt;
&lt;h3&gt;Session #340: The Space Between Creative Nonfiction and Literary Criticism: Theorizing, Writing, and Publishing Critical-Creative Hybrids&lt;/h3&gt;
&lt;p&gt;3:30 – 4:45 | Sheraton – Ravenna AB&lt;/p&gt;
&lt;p&gt;Janine M. Utell, Widener University, Presiding,&lt;/p&gt;
&lt;p&gt;Lesley Wheeler, Washington and Lee University&lt;/p&gt;
&lt;p&gt;Heather McNaugher, Chatham University&lt;/p&gt;
&lt;p&gt;Annette Federico, James Madison University&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Anna Mukamal, Stanford University&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Melody Nixon, University of California, Santa Cruz&lt;/p&gt;
&lt;p&gt;Susannah B. Mintz, Skidmore College&lt;/p&gt;
&lt;h3&gt;Session #369: Public Victorians&lt;/h3&gt;
&lt;p&gt;2:20 – 4:45 | WSCC—620&lt;/p&gt;
&lt;p&gt;Renee Fox, University of California, Santa Cruz, Presiding&lt;/p&gt;
&lt;p&gt;Elizabeth Meadows, Vanderbilt University, Presiding&lt;/p&gt;
&lt;p&gt;Sheila Cordner, Boston University&lt;/p&gt;
&lt;p&gt;Bridget Draxler, St. Olaf College&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abigail Droge, University of California, Santa Barbara&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Christie Harner, Dartmouth College&lt;/p&gt;
&lt;p&gt;Teresa Mangum, University of Iowa&lt;/p&gt;
&lt;p&gt;Danielle Spratt, California State University, Northridge&lt;/p&gt;
&lt;h2&gt;SATURDAY, 11 January&lt;/h2&gt;
&lt;h3&gt;Session #582: Models of Enlightenment Knowledge&lt;/h3&gt;
&lt;p&gt;3:30 – 4:45 | WSCC – 614&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ryan Heuser, King’s College&lt;/strong&gt;, Cambridge, Presiding&lt;/p&gt;
&lt;p&gt;“Decentering Knowledge,” &lt;strong&gt;Mark Algee-Hewitt, Stanford University&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;“Mapping Knowledge and Mapping Rome in Piranesi,” Jeanne Britton, University of South Carolina, Columbia&lt;/p&gt;
&lt;p&gt;“At One View,” Collin Jennings, Miami University, and Seth Rudy, Rhodes College&lt;/p&gt;
&lt;h2&gt;SUNDAY, 12 January&lt;/h2&gt;
&lt;h3&gt;Session # 698: Computational Criticism and Critical Thoery&lt;/h3&gt;
&lt;p&gt;10:15 – 11:30 | WSCC – Chelan 2&lt;/p&gt;
&lt;p&gt;Ted Underwood, University of Illinois, Urbana Champagne, Presiding&lt;/p&gt;
&lt;p&gt;“Beyond Distance, Beyond Theory,” Lauren Klein, Emory University&lt;/p&gt;
&lt;p&gt;“Difference and Data,” Richard Jean So, McGill University&lt;/p&gt;
&lt;p&gt;“Method, Meet Theory,” &lt;strong&gt;Laura B. McGrath, Stanford University&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Jonathan Arac, University of Pittsburgh, Respondent&lt;/p&gt;
&lt;h3&gt;Session #721: Function of American Literary Criticism at the Present Time&lt;/h3&gt;
&lt;p&gt;12:00-1:15 | WSCC – 607&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mark Grief, Stanford University&lt;/strong&gt;, Presiding&lt;/p&gt;
&lt;p&gt;“American Literary Catastrophism,” Anna Brickhouse, University of Virginia&lt;/p&gt;
&lt;p&gt;“Form and Method in American Literary Criticism,” Paul D. Giles, University of Sydney&lt;/p&gt;
&lt;p&gt;“After Post-Marxism: American Readings of Capital,” Colleen Lye, University of California, Berkeley&lt;/p&gt;
&lt;p&gt;“The Function of Criticism as Poesis,” Ramon Saldivar, Stanford University&lt;/p&gt;
&lt;h3&gt;Session # 774: Contemporary Economies of Prestige&lt;/h3&gt;
&lt;p&gt;1:45 – 3:00 | WSCC – 607&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Laura B. McGrath, Stanford University&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Kinohi Nishikawa, Prineton University&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Alexander Manshel, McGill University&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Rebecca L. Walkowitz, Rutgers University, New Brunswick&lt;/p&gt;
&lt;p&gt;Priya Joshi, Temple University&lt;/p&gt;
&lt;p&gt;James F. English, University of Pennsylvania, Respondent&lt;/p&gt;
&lt;p&gt;Are you a Lit Lab affiliate, alum, or current researcher who would like to be included in our MLA roundup? Email lmcgrath@stanford.edu.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Star Texts: A Case Study in Harper’s and Vogue</title>
    <link href="https://litlab.stanford.edu/star-texts/"/>
    <updated>2020-02-07T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/star-texts/</id>
    <content type="html">&lt;p&gt;In an early scene from Quentin Tarantino&#39;s &lt;em&gt;Once Upon a Time in Hollywood&lt;/em&gt;, a casting agent warns the former star of a hit TV western against the kind of cameo roles he&#39;s been taking since his show was canceled. Short a more permanent gig, the aging actor has been making guest appearances as one-episode villains on a number of newer action slots. The casting agent&#39;s advice is something to this effect: you might think these roles are a good part-time gig, but every time the audience sees you beat at the end of the episode, they see TV&#39;s up-and-coming action star besting the reigning champ. They&#39;re using your decline to prop up some new guy&#39;s rise to success. The real drama here is not between any particular episode&#39;s hero and its villain, but between two dueling star texts.&lt;/p&gt;
&lt;p&gt;Richard Dyer&#39;s 1979 *Stars *introduces the concept of a star image or star text as the aggregate of every public appearance of, or reference to a given Hollywood studio actor. In Dyer&#39;s terms, the star image is produced by the studio as a collection of mass cultural objects across a wide range of media. These would include the actor&#39;s film and television roles, but also interviews, radio appearances, commercials, as well as published gossip, tabloids, and reviews. Part of what is most enabling about Dyer&#39;s formulation is that it allows us to separate the celebrity as a real person from the public discourse around them. &amp;quot;Star text&amp;quot; is a term that helps us think about &amp;quot;stars&amp;quot; as &amp;quot;texts&amp;quot;---and not just texts, but narratives.&lt;/p&gt;
&lt;p&gt;The Literary Lab&#39;s &lt;em&gt;Star Texts&lt;/em&gt; project departs from more traditional work on celebrity in both methodology and scale. Our project uses computational methods to trace large historical trends across a corpus of star texts, pursuing lines of questioning like: do star texts have genres? and, do they follow narrative conventions? Working through these larger conceptual questions, I wanted to test if we could already see these kinds of patterns emerging in a sample corpus. Stanford has access to the entire archive of *Harper&#39;s Bazaar *and &lt;em&gt;Vogue Magazine&lt;/em&gt;,&lt;a href=&quot;https://litlab.stanford.edu/star-texts/#_ftn1&quot;&gt;[1]&lt;/a&gt; let&#39;s start there.&lt;/p&gt;
&lt;p&gt;I hand-curated a database of Hollywood stars, combining a comprehensive list of actors nominated for Academy Awards with a number of fan-made lists of highest grossing stars of all time and cult favorites. I then ran the complete list through the corpus, pulling out every item that mentioned one of these names. Instead of counting every time a star&#39;s name appears in a given article, advertisement, or review, as soon as the name hit, I grabbed the whole object. This means that all items in each star text are equally weighted. For example, a 1954 cover story on Judy Garland counts as one &amp;quot;hit,&amp;quot; as does a brief mention in a 1993 feature about Anne Hathaway.&lt;a href=&quot;https://litlab.stanford.edu/star-texts/#_ftn2&quot;&gt;[2]&lt;/a&gt; I think this choice is in line with Dyer&#39;s concept: the actor might be dead but the Judy Garland star text continues to be shaped and reshaped. (Think of Renée Zellweger in this year&#39;s Judy Garland biopic.) An off-hand remark like, she&#39;s &amp;quot;a bit like Anne Hathaway,&amp;quot; is also a chapter in that narrative.&lt;/p&gt;
&lt;p&gt;Figure 1 offers a rough image of the corpus of star texts I compiled from the *Harper&#39;s *and *Vogue *archive. Each colored layer represents a star text. Figure 2 includes only the ten largest so we can actually see who&#39;s who. (Note the size of the Meryl Streep star text, we&#39;ll return to this later.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/all-star-texts.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/all-star-texts.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/top10-star-texts.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/top10-star-texts.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Even though there&#39;s far more material in the contemporary side of the corpus, our top ten stars are spread fairly evenly across a century of Hollywood history. We have four from the studio-system dominated era of Classical Hollywood: Katharine Hepburn, Bette Davis, Audrey Hepburn, and Elizabeth Taylor. Three from the post-studio system, the New Hollywood era: Jane Fonda, Jack Nicholson, and Meryl Streep. And three from the contemporary: Cate Blanchett, Julia Roberts, and Nicole Kidman. With living actors, periodization is tricky. For example, do we still think of Meryl Streep as &amp;quot;New Hollywood&amp;quot; when the majority of her filmography is more recent? To make things clearer (if a bit cruder) I split the corpus into two periods: &amp;quot;classical Hollywood&amp;quot; and a &amp;quot;post-classical&amp;quot; era.&lt;a href=&quot;https://litlab.stanford.edu/star-texts/#_ftn3&quot;&gt;[3]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If Figure 1 helps us compare the size and extent of the star texts in our corpus, it does little to help us read individual star texts as narratives. Figures 3 &amp;amp; 5 represent the composition of each star text as a product of reviews and advertisements.&lt;a href=&quot;https://litlab.stanford.edu/star-texts/#_ftn4&quot;&gt;[4]&lt;/a&gt; Reviews and advertisements seem a useful proxy for the two faces of Hollywood stardom: the actor and the consumer icon. By comparing them, we begin to see stories of how stars move though different spheres of celebrity.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/top10-star-texts-each.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/top10-star-texts-each.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Each graph in Figure 3 shows the number of advertisements and reviews per year that feature a given star in the classical Hollywood period. Each of the top ten biggest star texts of this era begin with a series of advertisements that significantly precede the star&#39;s first mention in a review. For example, at the beginning of her career, Bette Davis exclusively appears in advertisements.&lt;a href=&quot;https://litlab.stanford.edu/star-texts/#_ftn5&quot;&gt;[5]&lt;/a&gt; In 1934 she appears in an ad for a &amp;quot;hair conditioning oil treatment;&amp;quot; in 1936 she&#39;s selling silk stockings; in 1939, it&#39;s costume jewelry. These ads all have an aspirational quality. These are relatively mundane objects sold under the auspices of Hollywood glamour. But the sales pitch cuts both ways. Each image is also explicitly tagged with the studio brand. For example, directly under the slogan &amp;quot;This glamour can be yours!&amp;quot; in the costume jewelry ad, there&#39;s another caption. This one reads: &amp;quot;Bette Davis as she appears in a scene from the epochal drama, &lt;em&gt;Juartez&lt;/em&gt;, produced in regal splendor by Warner Bros.&amp;quot;&lt;/p&gt;
&lt;p&gt;**Figure 4 **&lt;a href=&quot;https://litlab.stanford.edu/star-texts/#_ftn6&quot;&gt;[6]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/star-texts-ad.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/star-texts-ad.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The image of Bette Davis in costume doubles as Warner Bros. promotional material. She sits gracefully at the intersection of the Hollywood studio and the consumer brand, presiding over the complex network of advertisement messaging. From this perspective, her &amp;quot;regal splendor&amp;quot; seems especially stagy. The star image is a prop, produced at this intersection to grease the wheels of brand integration.&lt;/p&gt;
&lt;p&gt;After this image has been sufficiently sedimented in almost a decade of advertisements, reviews begin to trickle into the star text. First items like &amp;quot;Movies of the Month&amp;quot; in 1941, then &amp;quot;The Oscar Jinx&amp;quot; in 1986. If there&#39;s a generalizable pattern for star texts of the classical period, it&#39;s this movement from &amp;quot;star&amp;quot; to &amp;quot;actor.&amp;quot; First, *&amp;quot;Bette Davis, consumer icon, in all her Regal Warner Bros. Splendor,&amp;quot;*and only then &lt;em&gt;&amp;quot;Bette Davis, Oscar winner.&amp;quot;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 5&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/star-texts-10-post-classical.png&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/star-texts-10-post-classical.png&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the post-classical period, we see the opposite. In Figure 5, each star text graph starts with a stretch of reviews. Ads appear significantly later. Here, &amp;quot;stars&amp;quot; have to start as indie &amp;quot;actors,&amp;quot; before they go on to make real money in blockbusters and ad campaigns. For example, the Kate Winslet star text begins with a stretch of reviews for Indie movies and prestige period pieces, like Kenneth Branagh&#39;s 1996 adaptation of *Hamlet *and Jane Campion&#39;s 1999 &lt;em&gt;Holy Smoke!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For many of these star texts, reviews are already on the decline when ads begin. If in the classical Hollywood period, &amp;quot;advertising icon&amp;quot; and &amp;quot;film star&amp;quot; go hand in hand, in the post-classical period, consumer discourse seems inimical to the star&#39;s image as an actor. Once ads constitute a significant portion of the star text, reviews dwindle.&lt;/p&gt;
&lt;p&gt;I think this suggests that star production in the studio system is replaced by something like a prestige economy. Under the studio system, star texts were tightly monopolized by the studio. Even as the studio system declined in the mid-century, many stars continued to sign long-term production deals.&lt;a href=&quot;https://litlab.stanford.edu/star-texts/#_ftn7&quot;&gt;[7]&lt;/a&gt; After the studio system, stars became free agents, but without their corporate backing, star texts seem more vulnerable to brand contamination. As Figure 5 suggests, the most successful star texts of the era carefully separate the actor from the brand ambassador, the artist from the market player.&lt;/p&gt;
&lt;p&gt;**Figure 6 **&lt;a href=&quot;https://litlab.stanford.edu/star-texts/#_ftn8&quot;&gt;[8]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://litlab.stanford.edu/assets/images/techne/star-texts-kate.jpg&quot;&gt;&lt;img src=&quot;https://litlab.stanford.edu/assets/images/techne/star-texts-kate.jpg&quot; width=&quot;800px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In 2003, &lt;em&gt;Harper&#39;s&lt;/em&gt; ran a piece titled, &amp;quot;Kate&#39;s Turn,&amp;quot; as in *Kate&#39;s turn in the limelight, *or &lt;em&gt;Kate&#39;s turn at big time Hollywood fame&lt;/em&gt;. Ironically, this data point appears in the Kate Winslet star text just as her early-career uptick in reviews starts to decline. It&#39;s a turning point in more than one sense: *Kate&#39;s turn *from a review-based star text to an ad-based one; from an indie actor to the face of an anti-aging cosmetics line.&lt;/p&gt;
&lt;p&gt;Maybe this gives us a clearer picture of why Meryl Streep is the most frequently mentioned star in all of *Harper&#39;s *and *Vogue *combined. Meryl Streep doesn&#39;t appear in a single ad. Her prestige as an actor is uncontaminated. Or maybe we&#39;re only looking at the first chapter and &lt;em&gt;Meryl&#39;s turn&lt;/em&gt; is next.&lt;/p&gt;
&lt;h4&gt;Notes&lt;/h4&gt;
&lt;p&gt;&lt;a name=&quot;#_ftn1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;  Both magazines began in the late 19thcentury, but for our purposes here, we&#39;re mostly looking at 1920 through 2017. Hollywood films didn&#39;t regularly credit actors until the mid 1910s and 2017 is as contemporary as the corpus gets.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;#_ftn2&quot;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; *Vogue *writer, Adam Green, quotes Lone Scherfig, Hathaway&#39;s director for film &lt;em&gt;One Day&lt;/em&gt;, &amp;quot;She has some of the quality of Elizabeth Taylor or Judy Garland or someone who is not iconic because of her beauty of her goddess status but because of her warmth and depth and humanity. She&#39;s not a Hitchcock blonde---she&#39;s a real person.&amp;quot; Green, Adam. &amp;quot;Happily Ever After.&amp;quot; &lt;em&gt;Vogue&lt;/em&gt;, 1 Nov. 2010, pp. 196-205.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;#_ftn3&quot;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; I separated these out mostly by hand with star texts that begin before the 1960s as &amp;quot;classical Hollywood,&amp;quot; and star texts that begin after the 1960s as &amp;quot;post-classical.&amp;quot; The star texts that begin in the 1960s, I determined on a case-by-case basis.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;#_ftn4&quot;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; I&#39;ve excluded the star texts of male celebrities from Figures 2 &amp;amp; 3. Male actors rarely appear in the ad campaigns targeting women readers in *Harper&#39;s *and &lt;em&gt;Vogue&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;#_ftn5&quot;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; Bette Davis was the first star to break from the studio by turning herself into a company, B.D. Inc. in 1942. Her first review in our corpus is a 1941 mention in &lt;em&gt;Harper&#39;s Bazaar&lt;/em&gt; for her role in &amp;quot;The Man Who Came to Dinner.&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;#_ftn6&quot;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; &amp;quot;Advertisement: Hollywood Jewelry Products, Inc. (Hollywood Jewelry Products, Inc.).&amp;quot;* Vogue*, vol. 93, no. 9, May 01, 1939, pp. 137*. ProQuest*, https://search.proquest.com/docview/897868378?accountid=14026.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;#_ftn7&quot;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; &amp;quot;[Through the 1960s and 70s,] Para­mount had Jerry Lewis, Universal had Rock Hudson and Doris Day, MGM had Elvis Presley.&amp;quot; Thompson, Krisitin, and David Bordwell. &lt;em&gt;Film History: An Introduction&lt;/em&gt;. McGraw, 2003. 512.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;#_ftn8&quot;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt; &amp;quot;Kate&#39;s Turn: Kate Winslet reveals how she balances premiers, motherhood, and her romance with director Sam Mendes.&amp;quot; Harper&#39;s Bazaar, no. 3494, 01, 2003, pp. 60-63. ProQuest, https://search.proquest.com/docview/1881113406?accountid=14026.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on &#39;Talking Like a Robot&#39;</title>
    <link href="https://litlab.stanford.edu/news/2020-02-13-presentation-on-talking-like-a-robot/"/>
    <updated>2020-02-14T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2020-02-13-presentation-on-talking-like-a-robot/</id>
    <content type="html">&lt;p&gt;On February 14th, 2020, the Lab welcomed Christopher Grobe (Amherst College), Marit MacArthur (UC Davis), and Lee Miller (UC Davis) for their presentation, &amp;quot;Talking Like a Robot.&lt;/p&gt;
&lt;p&gt;If someone asked you to talk like a robot, you probably could—but how would you know how? Personal experience? Documented fact? Popular art and culture? Your own idea of how a robot talks? Probably all four, to some degree. In this talk, we bring artistic causes into focus, asking how vocal performance culture has shaped what it means to “talk like a robot.”
We will start with quick overviews of two relevant subjects: (a) the history of voice synthesis and (b) the history of human performers giving voice to robot characters. Comparing the generic conventions of “robotic” human performance to historical and contemporary constraints of synthesized speech, from early vocoders to today’s chatbots and virtual agents, we highlight the choices performers make: the way they mimic, ignore, or even invent robotic modes of speech. Using a corpus of sample audio recordings from film and TV depictions of synthesized voices from the 1950s to the present, we will also demonstrate the explanatory power of our own leading-edge tools for analyzing pitch and timing patterns in vocal performance.&lt;/p&gt;
&lt;p&gt;Christopher Grobe is associate professor of English at Amherst College and an ACLS Burkhardt Fellow in 2019-20 in the Department of Theatre and Performance Studies at Stanford University.&lt;/p&gt;
&lt;p&gt;Marit J. MacArthur is a lecturer in the University Writing Program and is affiliate faculty in Performance Studies at UC Davis.&lt;/p&gt;
&lt;p&gt;Lee M. Miller is professor of neurobiology and technical director of the Center for Mind and Brain at UC Davis.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on &#39;Translation&#39;</title>
    <link href="https://litlab.stanford.edu/news/2020-02-13-presentation-on-translation/"/>
    <updated>2020-02-14T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2020-02-13-presentation-on-translation/</id>
    <content type="html">&lt;p&gt;On February 18, 2020, J.D. Porter, Quinn Dombrowski, Yuilia Ilchuk, Antonio Lenzo, Eun Ji Lee, Shana Hadi presented &amp;quot;Translation&amp;quot;.&lt;/p&gt;
&lt;p&gt;Discussions of translation often focus on the ineffable losses created by moving from one language to another. Yet translation may also leave (or even produce) a residue of the original language, traces of the fact that a text did not start its life in the language the reader encounters. In this project, we use a corpus of short stories translated into English from Russian, French, Spanish, Korean, Italian, Indonesian, German, and Portuguese to examine whether translation leaves a lexical or grammatical trace on its arrival in English and how these traces may vary by language. We hope to uncover the stylistic affordances of translation as such, gesture toward an atlas of the formal effects of translation into English, and potentially generate a few practical findings for translators.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on &#39;Modeling the Spread of Information within Novels&#39;</title>
    <link href="https://litlab.stanford.edu/news/2020-02-25-presentation-on-modeling-the-spread-of-information-within-novels/"/>
    <updated>2020-02-26T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2020-02-25-presentation-on-modeling-the-spread-of-information-within-novels/</id>
    <content type="html">&lt;p&gt;On February 26, 2020, we welcomed David Bamman (UC Berkeley) for his presentation, &amp;quot;Modeling the Spread of Information Within Novels.&amp;quot;&lt;/p&gt;
&lt;p&gt;Understanding the ways in which information flows through social networks is important for questions of influence--including tracking the spread of cultural trends and disinformation and measuring shifts in public opinion. Much work in this space has focused on networks where nodes, edges and information are all directly observed (such as Twitter accounts with explicit friend/follower edges and retweets as instances of propagation); in this talk, I will focus on the comparatively overlooked case of information propagation in &lt;em&gt;implicit&lt;/em&gt; networks--where we seek to discover single instances of a message passing from person A to person B to person C, only given a depiction of their activity in text.&lt;/p&gt;
&lt;p&gt;Literature in many ways presents an ideal domain for modeling information propagation described in text, since it depicts a largely closed universe in which characters interact and speak to each other.  At the same time, it poses several wholly distinct challenges--in particular, both the length of literary texts and the subtleties involved in extracting information from fictional works pose difficulties for NLP systems optimized for other domains.  In this talk, I will describe our ongoing work in measuring information propagation in these implicit networks, and detail an NLP pipeline for discovering it, focusing in detail on new datasets we have created for tagging characters and their coreference in text.  This is joint work with Matt Sims, Olivia Lewke, Anya Mansoor, Sejal Popat and Sheng Shen.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Literary Lab talks at DH 2020</title>
    <link href="https://litlab.stanford.edu/news/2020-03-05-literary-lab-talks-at-dh-2020/"/>
    <updated>2020-03-06T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2020-03-05-literary-lab-talks-at-dh-2020/</id>
    <content type="html">&lt;p&gt;Four talks from the Stanford Literary Lab have been accepted for DH 2020 in Ottawa:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;J.D. Porter, &amp;quot;A Digital Study of Ralph Ellison’s Integrative Form&amp;quot;&lt;/li&gt;
&lt;li&gt;Annika Butler-Wall &amp;amp; Mark Andrew Algee-Hewitt, &amp;quot;Harry Potter and the Engaged Reader: Community Interactions and Influence in Serialized Fan Fiction&amp;quot;&lt;/li&gt;
&lt;li&gt;Nichole Misako Nomura &amp;amp; Mark Algee-Hewitt, &amp;quot;Novel Worldbuilding: Science Fiction&amp;quot;&lt;/li&gt;
&lt;li&gt;Mark Algee-Hewitt &amp;amp; Erik Fredner, &amp;quot;Typicality in the U.S. Novel&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on the short story project</title>
    <link href="https://litlab.stanford.edu/news/2020-04-22-presentation-on-the-short-story-project/"/>
    <updated>2020-04-23T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2020-04-22-presentation-on-the-short-story-project/</id>
    <content type="html">&lt;p&gt;On April 23rd, 2020, Mark Algee-Hewitt, Anna Mukamal and J.D. Porter presented some of the first results from their project on the short story.&lt;/p&gt;
&lt;p&gt;While the insights of the Digital Humanities have revealed new facets of literary form and history that are unavailable through traditional critical methods alone, in the field of literary text mining, the novel has long dominated analyses. As such, the new histories told by computational methods are centered on longer form prose fiction, eliding the ways in which short stories not only have their own evolutionary history throughout the nineteenth and twentieth centuries, but, equally as importantly, also differ formally from the novel. How does the language of the short story diverge from that of the novel? How do short stories construct plots differently from longer narratives? Are the events of short stories the same as those of novels, or do short stories have their own logic of events? Do characters appear and serve the same functions in short stories as they do in novels? And finally, how does the form of the short story change as it develops in the periodical publications of the early twentieth century? Length, it turns out, is not only an undertheorized aspect of literary fiction, but also does critical work in differentiating the kinds of stories that can be told in short fiction, as well as how those stories can be told. These questions have all been asked, to a greater or lesser extent, of other literary forms (novels, but also poetry and drama), but a computational approach, one that harnesses techniques from topic modeling to word vectors to named entity recognition, can aid us in better understanding the particular form and history of the short story.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on the new Epidemics project</title>
    <link href="https://litlab.stanford.edu/news/2020-05-05-presentation-on-the-new-epidemics-project/"/>
    <updated>2020-05-06T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2020-05-05-presentation-on-the-new-epidemics-project/</id>
    <content type="html">&lt;p&gt;On May 6th, 2020, we discussed the work of Mark Algee-Hewitt, Yibing Du, Maciej Kurzynski, Charlotte Lindemann, Nika Mavrody, Nichole Nomura, J.D. Porter, Carmen Thong, and Matthew Warner on the new Epidemics project.&lt;/p&gt;
&lt;p&gt;Coronavirus has the topic of epidemic, and the social distancing it enforces, on everyone’s mind. This project, begun just as social distancing was first being introduced to the everyday lexicon, takes a multi-pronged approach to investigating how epidemic has interacted with literary and written forms, both present and historical. On the one hand, we’ve scoured—with mixed success—recent news sources for interesting linguistic formations and strange expressions, and we’re starting the process of hunting through historical newspaper coverage from analogs for other periods and other diseases. We’re particularly interested in examples of language of care and safety: in conditions of epidemic, who gets to care? Who gets personified, and who erased? Will public health interventions make you safe, or safer? While we ponder these questions of public rhetoric, we’ve begun a more literary investigation into social distancing and the novel of isolation. What kind of social distancing takes place in novels and how does the rhetoric of isolation reflect the formal, sociological, and cognitive experience of being in prison, being stranded on an island, or voluntarily isolating in one’s room? Does the way that we describe isolation in literary contexts change according to the gender of the characters, whether it is voluntary or not, or whether the distancing happens alone or in groups? By exploring the ways in which novels have confronted the experience of social distancing over the past three centuries, we seek to gain a better understanding of our own reactions to our current moment.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on the Celebrity collaboration with the Smithsonian Museum of American History</title>
    <link href="https://litlab.stanford.edu/news/2020-05-17-presentation-on-the-celebrity-collaboration-with-the-smithsonian-museum-of-american-history/"/>
    <updated>2020-05-18T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2020-05-17-presentation-on-the-celebrity-collaboration-with-the-smithsonian-museum-of-american-history/</id>
    <content type="html">&lt;p&gt;On May 18th, 2020, Mark Algee-Hewitt, Erik Fredner, Charlotte Lindemann, Laura McGrath and J.D. Porter presented the latest work on Celebrity, our ongoing, large-scale collaboration with the Smithsonian Museum of American History on the history of American celebrity.&lt;/p&gt;
&lt;p&gt;Fame and Celebrity have been underappreciated concepts at the heart of U.S. culture. From the time of the Founding Fathers to the rise of mass entertainment (to say nothing of social media), questions of who deserves public recognition, how it is cultivated, and how it varies among different populations, have been at the forefront of conversations about democracy, social mobility, and meritocracy. With the advent of mass market entertainment, the normative effects of celebrity became a more pressing concern as local celebrities either became subjects of national interest or were effaced from the historical record. And yet the same forces have led, in the era of digital media, to a niche economy of fame where Warhol’s fifteen minutes has been transmuted into a long tail of YouTube and Instagram stars with followers numbering in the millions. Now, more than ever, as a reality TV star holds the nation’s highest office, there is a heightened discourse over what it means to become famous and the ways that celebrities leverage their status for other kinds of political and institutional power. In this presentation, we turn to one era in one city (interwar Chicago), as chronicled in two different newspapers written by and for two different communities (The Chicago Daily Tribune and The Chicago Defender) to consider the network of celebrities represented in each.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation from the Personhood project</title>
    <link href="https://litlab.stanford.edu/news/2020-06-01-presentation-from-the-personhood-project/"/>
    <updated>2020-06-02T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2020-06-01-presentation-from-the-personhood-project/</id>
    <content type="html">&lt;p&gt;On June 2nd, 2020, Mark Algee-Hewitt, Charlotte Lindemann, Sai Ma, Laura McGrath, Lisa Mendelman, Nichole Nomura, Casey Patterson, J.D. Porter, Alex Sherman, Hannah Walser, and Matt Warner presented on Personhood.&lt;/p&gt;
&lt;p&gt;A number of recent Literary Lab projects have addressed issues of personification and depersonification, from an analysis of stories with animal protagonists to a survey of racialization in nineteenth-century fiction. This project intends to bring these investigations, along with others led by a wide range of field experts, together in the service of a common goal: mapping the space of literary techniques that construct textual personhood. We pursue this goal without imposing a false coherence onto our research questions or our results: indeed, we start from the assumption that personhood is a fuzzy category and that texts may signal personhood through diverse and sometimes contradictory means. Our methods and corpora are therefore eclectic, ranging from classification on the scale of tens of thousands of texts to collocate analysis on the scale of a few hundred. Key to the project, however, is a desire to let the results of these different scales of analysis inform each other, an approach we model in this presentation by spotlighting a number of corpora built on different principles to test different local hypotheses about personhood.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Typicality in the US Novel</title>
    <link href="https://litlab.stanford.edu/typicality-in-the-u-s-novel/"/>
    <updated>2020-07-18T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/typicality-in-the-u-s-novel/</id>
    <content type="html">&lt;p&gt;&lt;em&gt;The talk below was originally scheduled to be given as a lightning talk at &lt;a href=&quot;https://dh2020.adho.org/&quot;&gt;DH2020&lt;/a&gt;. We have adapted it here for the new virtual conference. You can follow along with our &lt;a href=&quot;https://docs.google.com/presentation/d/1g2gR44CB49VzuuhrsxDqccpkSC-aohp4OQgwmopXE-k/edit?usp=sharing&quot;&gt;slides here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For a discipline committed to rejecting reductionism, literary studies relies on typicality more than it would care to admit. For example, [SLIDE] Frederic Jameson describes an “&lt;i&gt;unexpected&lt;/i&gt;” change in a character’s life in the novel &lt;i&gt;Demos&lt;/i&gt; (1886) as something that would [SLIDE] “&lt;i&gt;normally&lt;/i&gt; generate a properly Utopian narrative” (Jameson 184, emphasis added). Of course, the subversion of his expectation is what interests Jameson. Yet so much literary criticism seeks to explain subverted expectations that critics tend to ignore an equally fascinating question: What exactly is being subverted in such moments? Our project uses computational methods in an attempt to turn away from these transgressions and toward expectations.&lt;/p&gt;
&lt;p&gt;We are not the first to attempt to study literary typicality using computational methods. Indeed, typicality has already been a question in computational literary studies for decades. In an essay on Josephine Miles’s computational reading experiments from the 1950s, Brad Pasanek quotes an objection made by a reviewer to Miles’s work: [SLIDE] “‘If Quarles is “most typical,” in any way at all, of this set of poets,’ which includes Donne, John Milton, and John Dryden, ‘I am immediately convinced that typicality is not a fruitful thing to investigate’” (Pasanek 369). Here, computational typicality and readerly typicality diverge. But we can reject Miles’s reviewer’s conclusion that typicality is not fruitful to investigate precisely because that reviewer had such strong priors about what typifies Donne, Milton, Dryden, and Quarles that they felt confident declaring Miles’s method fruitless “immediately.”&lt;/p&gt;
&lt;p&gt;Our project consists of a series of experimental provocations that analyze literary typicality using many different features. For this brief talk, I will be presenting just one of the experiments we have conducted. We do not claim that any one of our methodologies operationalizes literary typicality as such. Rather, each provides new evidence about the works and features that might typify authors and historical periods. Our argument is, in essence, a rejection of Miles’s critic: typicality is a fruitful thing to investigate in part because literary criticism has rather conspicuously refused to do it.&lt;/p&gt;
&lt;p&gt;That said, quantitative typicality is a different judgment than qualitative typicality. Heather Brink-Roby cites an analogy from nineteenth-century zoology that clarifies this point: In a letter to Charles Darwin, George Robert Waterhouse distinguishes between two ways in which zoologists identified the typical species of a group. [SLIDE] The first is Jameson’s method, whereby the critic identifies a trope as typical of Utopian fiction. A quantitative approach, by contrast, would look to a corpus of Utopian novels and ask which, if any, displays what Waterhouse calls [SLIDE] “the greatest number of [characteristics] most common to the species…in the best balanced condition.” The naivete of this latter approach is part of our intervention.&lt;/p&gt;
&lt;p&gt;The experiment we’re sharing today is one of our simplest: Using Gale’s corpus of American fiction, which contains more than 18,000 American texts published between 1774 and 1920, we identified the 2,000 most frequent nouns across the whole corpus using Apache OpenNLP. Then, we used the scaled frequencies of each of those nouns in each text as our feature set. We compare the texts using three measures of similarity in hyperdimensional space. [SLIDE] Computationally, the goal is to identify which texts are least &lt;i&gt;unlike&lt;/i&gt; the others in the corpus. Finally, we use t-stochastic neighbor embedding to visualize these relationships in high-dimensional space. [SLIDE]&lt;/p&gt;
&lt;p&gt;Here, we find clusters that correspond with known subjects of nineteenth-century American novels. For instance, [SLIDE] we have here a clear grouping of seafaring tales and desert island adventures. Another cluster [SLIDE] contains works of historical fiction, including the Arthurian medievalism that Twain parodies in &lt;i&gt;Connecticut Yankee&lt;/i&gt;.&lt;/p&gt;
&lt;p&gt;However, one novel from 1912 is closest to the mean and the median noun distribution in our corpus: &lt;i&gt;The Dragon’s Daughter&lt;/i&gt; by Clyde C. Westover. Generically, it combines tropes from the American Western with an Orientalized San Francisco Chinatown, opium smuggling, and the mob. Tellingly, &lt;i&gt;The Dragon’s Daughter&lt;/i&gt; was also adapted into a 1919 film, &lt;i&gt;The Tong Man&lt;/i&gt;. Its cinematic potential by the standards of early Hollywood is clear from the first page [SLIDE].&lt;/p&gt;
&lt;p&gt;The question this finding raises falls outside of the bounds of this brief presentation, but lies at the heart of our study: How typical is the average novel? Is our Westover like Miles’s reviewer’s Quarles? Or is &lt;i&gt;The Dragon’s Daughter&lt;/i&gt; more characteristic of American fiction than we might care to admit?&amp;lt;&lt;/p&gt;
&lt;h3&gt;Works Cited&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Jameson, Fredric. &lt;i&gt;The Political Unconscious: Narrative as a Socially Symbolic Act&lt;/i&gt;. Routledge, 2013.&lt;/li&gt;
&lt;li&gt;Pasanek, Brad. “Extreme Reading: Josephine Miles and the Scale of the Pre-Digital Digital Humanities.” &lt;i&gt;ELH&lt;/i&gt;, vol. 86, no. 2, June 2019, pp. 355–85. &lt;i&gt;Project MUSE&lt;/i&gt;, doi:&lt;a href=&quot;https://doi.org/10.1353/elh.2019.0018&quot;&gt;10.1353/elh.2019.0018&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Shklovsky, Víktor. &lt;i&gt;The Novel: An Anthology of Criticism and Theory 1900-2000&lt;/i&gt;. Edited by Dorothy J. Hale, John Wiley &amp;amp; Sons, 2005.&lt;/li&gt;
&lt;li&gt;Westover, Clyde C. &lt;i&gt;The Dragon’s Daughter&lt;/i&gt;. Neale Publishing Company, 1912.&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on the short story project: On the Affordances of Mere Length</title>
    <link href="https://litlab.stanford.edu/news/2020-11-05-presentation-on-the-short-story-project-on-the-affordances-of-mere-length/"/>
    <updated>2020-11-06T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2020-11-05-presentation-on-the-short-story-project-on-the-affordances-of-mere-length/</id>
    <content type="html">&lt;p&gt;On Nov 6th, 2020, when Mark Algee-Hewitt, Anna Mukamal and J.D. Porter presented their latest work on the short story project: On the Affordances of Mere Length.&lt;/p&gt;
&lt;p&gt;The latest stage of our short story project expands our exploration of the formal and narratological differences between short stories and novels. We not only supplement our small, control, corpus with additional texts, but also we expand our analysis to a corpus of 10,000 short stories from Women’s Magazines of the late 19th and 20th centuries and compare it to an equal sized corpus of novels. In addition to vastly expanding our analysis of character appearances, and textual similarity across the texts, this new corpus has allowed us to reach new conclusions about the history of the short story in the twentieth century. We will also present new results on the formal differences between endings of novels and endings of short stories.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation from the social distancing project</title>
    <link href="https://litlab.stanford.edu/news/2020-11-17-presentation-from-the-social-distancing-project/"/>
    <updated>2020-11-18T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2020-11-17-presentation-from-the-social-distancing-project/</id>
    <content type="html">&lt;p&gt;On November 18th, 2020, members of the social distancing project presented their latest work.&lt;/p&gt;
&lt;p&gt;More than eight months into the coronavirus pandemic, we’re all too familiar with the phrase “social distancing.” But what exactly does social distancing look like in practice? And what can literature teach us about how to live through this moment in history? This project takes the constraints the virus has placed on our own sociality as a through line, collecting scenes of isolation in novels, essays, and short stories from the eighteenth century through the present day. Our examples range from scenes of solitary confinement on deserted islands and spaceships, in nunneries and prison cells, to literary celebrations of private space. How does reading “A Room of One’s Own” alongside Robinson Crusoe inform our current experience? Where do we get by describing Snow White and the seven dwarfs as social distancing with roommates? Our project aims to identify a common language of isolation across literary genres and historical periods, while simultaneously drawing attention to a number of key differences across the corpus. For instance, are socially distanced women portrayed differently than their male counterparts? Are they more likely to be confined against their will? Does voluntary isolation look substantively different than involuntary? Does solitary confinement share a lexicon with the practice of social distancing as a family?&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on &#39;Therapy Texts&#39;</title>
    <link href="https://litlab.stanford.edu/news/2021-01-25-presentation-on-therapy-texts/"/>
    <updated>2021-01-26T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2021-01-25-presentation-on-therapy-texts/</id>
    <content type="html">&lt;p&gt;On January 26th, 2021, Mark Algee-Hewitt, Lisa Mendelman, Anna Mukamal and Kendra Terry presented their work on Therapy Texts, a new collaboration between members of the Literary Lab and Adelphi University.&lt;/p&gt;
&lt;p&gt;This collaboration combines methods from computational text analysis, literary studies, and clinical psychology to examine the therapeutic encounter as manifest in a variety of texts. In particular, we compare a set of transcribed contemporary psychotherapy sessions to other forms of discourse including 20th- and 21st-century novels and memoirs, podcast interviews, and informal recorded conversation. We are interested in what distinguishes psychotherapeutic discourse from other types of discourse and how analytical protocols from literary studies and digital humanities can be productively applied to psychotherapeutic discourse issuing from a clinical setting. How is language used differently across these types of discourse? How similar are therapeutic encounters portrayed in fictional texts to actual therapy sessions? Which parts of 20th- and 21st-century literature look the most like therapy sessions? We approach these questions using a variety of computational tools, including topic models, most distinctive words, and sentiment analysis with a particular focus on parts of speech, words of temporal space, and features sets that allow us to model the language of the therapeutic encounter in literary spaces.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation from the Data-Sitters Club</title>
    <link href="https://litlab.stanford.edu/news/2021-02-17-presentation-from-the-data-sitters-club/"/>
    <updated>2021-02-18T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2021-02-17-presentation-from-the-data-sitters-club/</id>
    <content type="html">&lt;p&gt;On February 18th, 2021, the members of the Data-Sitters Club shared their latest work from their multi-institutional collaboration.&lt;/p&gt;
&lt;p&gt;What happens when you combine a team of scholars with diverse disciplinary backgrounds and careers, an iconic 80&#39;s and 90&#39;s girls book series, and computational text analysis methods? Meet the Data-Sitters Club, a public-oriented feminist collaboration that aims to demystify computational text analysis and get real about the opportunities, challenges, and frustrations of interdisciplinary DH collaboration. Since fall 2019, this project at the Lab has brought together scholars from more than six institutions around Ann M. Martin’s series “The Baby-Sitters Club” (1986-2000, and recently revitalized by a graphic novel series and Netflix show), using this corpus as the basis for exploring what computational methods can do, and where they fall short. Each of the group&#39;s 12 &amp;quot;books&amp;quot; (including three &amp;quot;Multilingual Mysteries&amp;quot; that focus on the French translations of the series) tackles a different aspect of this kind of work, ranging from corpus creation, to copyright, to text comparison, to telling your excited collaborators that a computational &amp;quot;discovery&amp;quot; is old news in your discipline. The group will cover highlights from these books, and reflect on what’s worked, what’s failed, what kinds of questions they’ve come closer to answering, and how other Lab members can get involved and help make computational text analysis more transparent and accessible to a broader group of scholars.&lt;/p&gt;
&lt;p&gt;The DSC includes: Lee Skallerup Bessette (Georgetown University), Katherine Bowers (Univ. of British Columbia), Maria Cecire (Mellon Foundation), Quinn Dombrowski (Stanford University), Anouk Lang (University of Edinburgh) and Roopika Risam (Salem State University)&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on Personhood</title>
    <link href="https://litlab.stanford.edu/news/2021-03-01-presentation-on-personhood/"/>
    <updated>2021-03-02T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2021-03-01-presentation-on-personhood/</id>
    <content type="html">&lt;p&gt;On March 2nd, 2021 we heard an update on the Personhood project&#39;s latest work.&lt;/p&gt;
&lt;p&gt;The Personhood project is a collection of experiments that investigate the ways that the attributes of personhood are assigned to (or taken away from) literary characters. How are non-human characters (for example, animals or robots) represented so that readers encounter them as persons in the text? And how are human characters sometimes denied attributes of personhood? In this presentation, we will discuss our further extensions of the animacy model from the Wild Animal Stories project, in which we use a machine learning model trained on the grammar of humans and objects to classify entities that do not exactly belong to either category (e.g. organizations, occupations, or abstractions), and we will present our latest results from our analysis of online educational resources that offer their own complex topology of characters in a large literary corpus.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>“Whatever you do, don’t stare”: Modeling Representations of Otherness in the Star Wars Expanded Universe</title>
    <link href="https://litlab.stanford.edu/news/2021-04-20-whatever-you-do-don-t-stare-modeling-representations-of-otherness-in-the-star-wars-expanded-universe/"/>
    <updated>2021-04-21T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2021-04-20-whatever-you-do-don-t-stare-modeling-representations-of-otherness-in-the-star-wars-expanded-universe/</id>
    <content type="html">&lt;p&gt;On April 21st, 2021, Mark Algee-Hewitt, Quinn Dombrowski, Nichole Nomura and Matt Warner will present the first results on their analysis of novels in the Star Wars extended universe.&lt;/p&gt;
&lt;p&gt;The Star Wars franchise offers a powerful set of representations of otherness within its transmediated cultural artifacts. While previous work on representation of racialized, gendered and othered bodies in the Star Wars universe has rested on the visible signals of identity and otherness that visual mediation presents to viewers (in the films, televisions series, and graphic novels), relatively little work has been done on the textual representation of these markers of difference in the novels. Within a purely textual medium, how is otherness communicated and how does it map onto our own categories of race, ethnicity and gender identity? When is it necessary to identify a character as an alien? As a humanoid? How do these identities intersect with gender? And to what extent does human-ness play the role of the default representational category, echoing the ways in which whiteness is an unmarked category within racialized depictions of characters in non-science fiction literature?&lt;/p&gt;
&lt;p&gt;In this project, we leverage computational textual analysis to explore the background discourses of the human and the other in the extensive universe of the Star Wars novels. Using a corpus of nearly 400 published Star Wars novels written between 1978 and 2020, we combine embedding spaces, machine learning models and close reading to explore how different authors mobilize similar representations of the human and the non-human (the humanoid, the alien, the robotic) in ways that replicate and influence the discourse of otherness in our wider culture.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>&#39;Whatever You Do, Don&#39;t Stare&#39;: Modeling Representation of Otherness in the Star Wars Expanded Universe&#39; at &#39;Realizing Resistance Episode II: Uncharted Galaxies&#39;</title>
    <link href="https://litlab.stanford.edu/news/2021-05-02-whatever-you-do-don-t-stare-modeling-representation-of-otherness-in-the-star-wars-expanded-universe-at-realizing-resistance-episode-ii-uncharted-galaxies/"/>
    <updated>2021-05-03T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2021-05-02-whatever-you-do-don-t-stare-modeling-representation-of-otherness-in-the-star-wars-expanded-universe-at-realizing-resistance-episode-ii-uncharted-galaxies/</id>
    <content type="html">&lt;p&gt;On May 3, 2021, the Star Wars project presented &amp;quot;&#39;Whatever You Do, Don&#39;t Stare&#39;: Modeling Representation of Otherness in the Star Wars Expanded Universe&amp;quot; at the conference &lt;em&gt;Realizing Resistance Episode II: Uncharted Galaxies&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The Star Wars franchise offers a powerful set of representations of otherness within its transmediated cultural artifacts. While previous work on representation of racialized, gendered and othered bodies in the Star Wars universe (Wetmore, 2005; Pearson, 2008; Wälivaara, 2018; Howe, 2012) has rested on the visible signals of identity and otherness that visual mediation presents to viewers (in the films, televisions series, and graphic novels), relatively little work has been done on the textual representation of these markers of difference in the novels. Within a purely textual medium, how is otherness communicated and how does it map onto our own categories of race, ethnicity and gender identity? When is it necessary to identify a character as an alien? As a humanoid? How do these identities intersect with gender? And to what extent does human-ness play the role of the default representational category, echoing the ways in which whiteness is an unmarked category within racialized depictions of characters in non-science fiction literature (Brekhus, 1998)?&lt;/p&gt;
&lt;p&gt;In this project, we leverage computational textual analysis to explore the background discourses of the human and the other in the extensive universe of the Star Wars novels. Using a corpus of nearly 400 published Star Wars novels written between 1978 and 2020, we combine embedding spaces, machine learning models and close reading to explore how different authors mobilize similar representations of the human and the non-human (the humanoid, the alien, the robotic) in ways that replicate and influence the discourse of otherness in our wider culture.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Brekhus, Wayne. “A Sociology of the Unmarked: Redirecting Our Focus.” &lt;em&gt;Sociological Theory&lt;/em&gt; 16.1(1998): 34-51.&lt;/li&gt;
&lt;li&gt;Howe, Andrew. “Star Wars in Black and White: Race and Racism in a Galaxy not So Far Away, 2012” &lt;em&gt;Sex, Politics, and Religion in&lt;/em&gt; Star Wars: &lt;em&gt;An Anthology&lt;/em&gt;. Ed Douglas Brode and Leah Deyneka; Plymouth: Scarecrow Press, 2012. 11-24.&lt;/li&gt;
&lt;li&gt;Pearson, Wendy Gay. “Alien Cryptographies: The View from the Queer.” &lt;em&gt;Queer Universes: Sexualities in Science Fiction&lt;/em&gt;. Ed. Wendy Gay Pearson, Veronica Hollinger, and Joan Gordon. Liverpool: Liverpool UP, 2008. 14-28.&lt;/li&gt;
&lt;li&gt;Wälivaara, Josephine. “Blind Warriors, Supercrips, and Techno-marvels: Challenging Depictions of Disability in Star Wars.” &lt;em&gt;Journal of Popular Culture&lt;/em&gt; 51.4(2008): 1036-1056.&lt;/li&gt;
&lt;li&gt;Wetmore, Kevin J. &lt;em&gt;Empire Triumphant: Race, Religion and Rebellion in the Star Wars Films&lt;/em&gt;. New York: McFarland, 2005.&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation by Mark Algee-Hewitt on “Failed Concepts in the 18th Century”</title>
    <link href="https://litlab.stanford.edu/news/2021-05-11-presentation-by-mark-algee-hewitt-on-failed-concepts-in-the-18th-century/"/>
    <updated>2021-05-12T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2021-05-11-presentation-by-mark-algee-hewitt-on-failed-concepts-in-the-18th-century/</id>
    <content type="html">&lt;p&gt;On May 12th, 2021, Mark Algee-Hewitt presented on &amp;quot;Failed Concepts in the 18th Century&amp;quot;.&lt;/p&gt;
&lt;p&gt;The eighteenth century was a rich period for the emergence of concepts that we continue to grapple with today. Contemporary ideas such as “political justice,” or “human rights,” not only appeared in the conceptual framework of eighteenth-century political, social, or philosophical theory, but the terms themselves also became part of the cultural lexicon. These compound concepts, in which two pre-existing terms are brought together into a new configuration with a single, stable meaning, form a critical backbone of eighteenth-century thought. In this project, I have developed a new set of aligned vector models to trace the ways in which these kinds of concepts emerged within the eighteenth century. Working from the Eighteenth-Century Collections Online database, I explore how these models can trace the successful emergence and evolution of these concepts as their constituent terms collide in the discourse of the period. More importantly, however, my approach allows us to model failed concepts: points at which it appears that the same process should bring together disparate terms into a new compound concept, but in which a new concept fails to emerge, or the terms subsequently fall apart. These failed concepts not only allow us to better understand why certain combinations succeeded, but, also, for the first time, allow us to explore an unrealized history of concepts that could have happened, but didn’t.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation by Matt Warner on “Comparing Canons”</title>
    <link href="https://litlab.stanford.edu/news/2021-05-25-presentation-by-matt-warner-on-comparing-canons/"/>
    <updated>2021-05-26T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2021-05-25-presentation-by-matt-warner-on-comparing-canons/</id>
    <content type="html">&lt;p&gt;On May 26th, 2021, Matt Warner presented on &amp;quot;Comparing Canons&amp;quot;.&lt;/p&gt;
&lt;p&gt;Much work in the digital humanities is concerned—often antagonistically—with the canon, whether because it’s not the archive, or because it’s a loose index to cultural capital, or even simply because it forms the core of the small subset of a much larger corpora that interlocutors outside the field (and in it, often) are most likely to be interested in it. Currently, however, no large or carefully defined corpus of the canon is available for digital humanists to use, either for comparative work or for addressing how this or that new idea applies to the subset of literature that bygone cultural authorities considered most worthy. For the limited case of novels in English, this project aims to amend this lack by combining together publishing data, prize lists, MLA citations and other cultural authorities to create a corpus and associated data set made up of what have been treated as the most canonical novels written in English.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation by Grant Parker on “Narratives of Enslavement”</title>
    <link href="https://litlab.stanford.edu/news/2021-10-20-presentation-by-grant-parker-on-narratives-of-enslavement/"/>
    <updated>2021-10-21T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2021-10-20-presentation-by-grant-parker-on-narratives-of-enslavement/</id>
    <content type="html">&lt;p&gt;On October 21st, 2021, Professor Grant Parker of the Classics Department, along with his South African Collaborators, presented on their project Narratives of Enslavement.&lt;/p&gt;
&lt;p&gt;A new project throws up more questions than answers: I am part of a project that seeks to digitally republish transcripts of legal cases involving enslaved persons at the Cape of Good Hope (1705-94). This is a chance to scrutinize the notion of ‘slave narrative’, comparing the Cape trial records with other histories. The most urgent question is: What notion of narrative is most productive for these court trials, and what are the implications for their digitization?&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation by Mark Algee-Hewitt on “Poetic Epistemologies”</title>
    <link href="https://litlab.stanford.edu/news/2021-11-10-presentation-by-mark-algee-hewitt-on-poetic-epistemologies/"/>
    <updated>2021-11-11T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2021-11-10-presentation-by-mark-algee-hewitt-on-poetic-epistemologies/</id>
    <content type="html">&lt;p&gt;On November 11th, 2021, Mark Algee-Hewitt presented his recent work on the new project “Poetic Epistemologies.”&lt;/p&gt;
&lt;p&gt;This project explores the relationship between information and knowledge production in the poetry of the long eighteenth-century and Romantic periods. Despite the highly figurative and allusive nature of eighteenth-century poetry, its proximity to the Enlightenment allows it to rely on catalogues of empirical objects and objects of immediate sensory experience. In contrast, Romantic poetry’s own self-theorization imposes a representational gap between the objects described and their use in producing poetic knowledge (e.g. Wordsworth’s “emotions recollected in tranquility”). This project, still in its early stages, explores this changing epistemology across the poetry of the long eighteenth century. How does poetry create meaning in both periods? How are objects represented between and across poems? How does the organization of objects in poems tell us about the way that knowledge is produced and communicated? Using the 55,000 poems from these periods from the Chadwyck-Healey poetry corpus, I test a series of new methods to investigate the semantic, syntactic, and poetic relationships between objects and concepts in poetry.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on project updates</title>
    <link href="https://litlab.stanford.edu/news/2022-01-19-presentation-on-project-updates/"/>
    <updated>2022-01-20T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2022-01-19-presentation-on-project-updates/</id>
    <content type="html">&lt;p&gt;Nichole talked about her recent work on “Slaves vs Robots”:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It’s a common assumption in science fiction studies that slaves and robots are substitutable, and that books about robots therefore teach us things about slavery. This project seeks to explore the grammatical and semantic contexts in which &amp;quot;robot&amp;quot; and &amp;quot;slave&amp;quot; are substitutable in practice across various corpora, using word embedding models.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Alex discussed his work on “Four Theses on the Real and Imaginary British Empire, 1697-1829”:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What is the relationship between empire’s imaginary and material geographies? To address this question, I compared place names in a cross-section of eighteenth-century British maritime literature to the movements of British Navy, East India Company, and transatlantic slave ships. After discussing the methods for identifying place names and mapping them alongside these ship movements, including an interactive Shiny application, I will discuss four tentative patterns of imperial geography that the comparison helps us see. Finally, I will propose some potential next steps for generalizing the project’s methods, corpora, and findings.&lt;/p&gt;
&lt;/blockquote&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation by Matt Warner on “Queer literature, theory and computational methods.”</title>
    <link href="https://litlab.stanford.edu/news/2022-01-19-presentation-by-matt-warner-on-queer-literature-theory-and-computational-methods/"/>
    <updated>2022-01-20T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2022-01-19-presentation-by-matt-warner-on-queer-literature-theory-and-computational-methods/</id>
    <content type="html">&lt;p&gt;On January 20th, 2022 Matt Warner presented his recent work on “Queer literature, theory and computational methods.”&lt;/p&gt;
&lt;p&gt;The first two decades of the twenty first century saw both the emergence of computational literary studies, and an exponential increase in the publication of novels for and by queer and LGBTQIA+ readers. Historically, however, computational studies of literature have had relatively little to say about the queer, and it is easy to caricature the only field of literary criticism with an interest in binary classification as fundamentally at odds with queer studies. Instead, however, this talk explores some of the messy, imperfect results that emerge from turning traditional computational methods on queer texts, and what those results can tell us about the state of our current methods and corpora.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation by Laura McGrath on “What 25,000 Book Deals Can Tell Us About Race”</title>
    <link href="https://litlab.stanford.edu/news/2022-02-16-presentation-by-laura-mcgrath-on-what-25-000-book-deals-can-tell-us-about-race/"/>
    <updated>2022-02-17T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2022-02-16-presentation-by-laura-mcgrath-on-what-25-000-book-deals-can-tell-us-about-race/</id>
    <content type="html">&lt;p&gt;On February 17th, 2022, we welcomed back Laura McGrath, currently an Assistant Professor at Temple University, and our former Associate Director, to talking to us about her latest work on her project “What 25,000 Book Deals Can Tell Us About Race.”&lt;/p&gt;
&lt;p&gt;The international publishing conglomerates known as the Big Five have not been known historically for publishing and promoting the writing of people of color. As countless articles, essays, and twitter threads will tell you, mainstream publishing is deeply racist and very white. Yet, over the past two years, editors and executives have been surprised to find that “books about race” (to borrow an inelegant phrase from the New York Times) have topped the bestseller lists and flown off the shelves. In this talk, examine a corpus of 25,000 book deal announcements to consider the ways that race and ethnicity are represented by an industry whose controlling interests (and presumed customers) are predominantly white. Which stories, by which writers, make it through the bottleneck of acquisition— what traits do they share, what narratives do they promote, and what, by extension, are the “authorized” stories of race and racism in the United States? How do these books contribute to the discourse around race and ethnicity in the twenty-first century?&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Presentation on Gender and Domestic Technology in Mid-Century Women’s Magazine</title>
    <link href="https://litlab.stanford.edu/news/2022-03-09-presentation-on-gender-and-domestic-technology-in-mid-century-women-s-magazine/"/>
    <updated>2022-03-10T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2022-03-09-presentation-on-gender-and-domestic-technology-in-mid-century-women-s-magazine/</id>
    <content type="html">&lt;p&gt;On March 10th, 2022 members of our new project on “Gender and domestic technology in mid-century Women’s magazines” presented their work investigating the postwar representation of domestic technology in the Women’s Magazine Archive, a corpus of 20th century women’s magazines, looking at fiction, advertisements, and articles from 1945 to 1975.&lt;/p&gt;
&lt;p&gt;This project investigates the postwar representation of domestic technology in the Women’s Magazine Archive, a corpus of 20th century women’s magazines, looking at fiction, advertisements, and articles from 1945 to 1975. While the connection between fiction and the 20th Century’s changing technologies of communication and transportation has been the subject of extended scholarship, including some of the work that has emerged from the Literary Lab, this project brings the same questions into the home. We focus on household appliances because the common but specific terminology surrounding them enables us to get at larger questions about labor, gender, and ideologies thereof during a key period for their reconfiguration. Using sentence-level MDWs, word embeddings, and dependency parsing, we explore questions like: How does the domestic discourse of “modern conveniences” appear in magazine fiction aimed at women? Who works with appliances and how? Which new appliances appear in fiction, and how are they presented? What is the relationship between articles on advances in domestic technology, the advertising that sold those appliances, and fiction that represents the appliances that resulted from those advances, particularly when all appear side by side in the same publication?&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Young Readers Database of Literature (YRDL)</title>
    <link href="https://litlab.stanford.edu/news/2022-04-27-young-readers-database-of-literature-yrdl/"/>
    <updated>2022-04-28T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2022-04-27-young-readers-database-of-literature-yrdl/</id>
    <content type="html">&lt;p&gt;On April 28, 2022, Nichole Nomura, Quinn Dombrowski, and Jennifer Wolf debuted their work on the Young Readers Database of Literature (YRDL).&lt;/p&gt;
&lt;p&gt;The Young Readers Database of Literature is a collaboration between Lab members and affiliates in Education, English, and the DLCL. The database contains rich metadata for over 25,000 works of children’s, middle-grade, and YA fiction novels from the 20th and 21st century, including publisher information, Goodreads data, Horn Book reviews, readability metrics, and awards status, as well as a full-text corpus. Our initial database-building stage prioritized several subgenres—including “Hi-Lo” novels, LGBTQ+ novels, and popular series for girls—that have often been overlooked in favor of prestige literature, opening up new possibilities for quantitative research on young readers’ fiction that reaches beyond award-winners and immediately-recognizable titles.&lt;/p&gt;
&lt;p&gt;The team introduced the database and corpus and share our results from some preliminary investigations, including an exploration of “dangerous” (or exciting) key terms like “sex” and “drugs,” mapping explicit identity mentions (with a focus on Asian and Asian-American identities) across the corpus, and the beginnings of work with canon-creation projects like The Horn Book Guide and queer bibliographies.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Stanford Literary Lab at DH Unbound 2022</title>
    <link href="https://litlab.stanford.edu/news/2022-05-16-stanford-literary-lab-at-dh-unbound-2022/"/>
    <updated>2022-05-17T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2022-05-16-stanford-literary-lab-at-dh-unbound-2022/</id>
    <content type="html">&lt;p&gt;Three talks related to LitLab projects will be presented at DH Unbound 2022.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Natasha Marie Johnson, &amp;quot;A Stylometric Examination of Inter-Authorial Influence within Fanfiction Communities&amp;quot;&lt;/li&gt;
&lt;li&gt;Quinn Dombrowski, Steele Douris and Mariia Gorshkova, &amp;quot;My Tags, Your Avvertimenti, Their Предупреждения: Self-Censorship and Metadata Across Fanfic Archives&amp;quot;&lt;/li&gt;
&lt;li&gt;Nichole Misako Nomura, &amp;quot;“robots” == “slaves”: Substitutions of Slaves and Robots&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Return to Realism? Comparing 19th- and 21st-Century Novel Forms</title>
    <link href="https://litlab.stanford.edu/news/2022-05-25-return-to-realism-comparing-19th-and-21st-century-novel-forms/"/>
    <updated>2022-05-26T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2022-05-25-return-to-realism-comparing-19th-and-21st-century-novel-forms/</id>
    <content type="html">&lt;p&gt;Zuza Leniarska has been visiting the Lab on a Fulbright scholarship this year and she has been hard at work on her own project relating realism in the novel between the 19th and 21st centuries, called “Return to Realism? Comparing 19th- and 21st-Century Novel Forms.&amp;quot;&lt;/p&gt;
&lt;p&gt;Realism is a hegemonic literary tendency in late 19th century United States, with influence over fiction-writing and social imagery spreading for decades. Arguably now we see its return to contemporary American novel, with similar topics, narratological strategies and similarly high level of literary prestige. In my project I compare topics in those two realisms to see how they carry meanings related to the socio-political circumstances that accompanied the rise of the Realist novel in the 19th century, and the contemporary neoliberal shift towards individualism, marketization, and economic and political liberalism.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Stanford Literary Lab at DH 2022</title>
    <link href="https://litlab.stanford.edu/news/2022-07-24-stanford-literary-lab-at-dh-2022/"/>
    <updated>2022-07-25T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2022-07-24-stanford-literary-lab-at-dh-2022/</id>
    <content type="html">&lt;p&gt;Four talks related to LitLab projects will be presented at DH 2022 in Tokyo (virtually).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mark Algee-Hewitt, &amp;quot;A Computational Approach to Epistemology in Poetry of the Long Eighteenth Century: A Case Study in Objects and Ideas&amp;quot;&lt;/li&gt;
&lt;li&gt;Yiwen Wang &amp;amp; Maciej Kurzynski. &amp;quot;Affective Writing in Ming Dynasty &#39;Huaben&#39; Stories — A Topic Modeling Study&amp;quot;&lt;/li&gt;
&lt;li&gt;Natasha Marie Johnson, &amp;quot;A Stylometric Examination of Inter-Authorial Influence within Fanfiction Communities&amp;quot;&lt;/li&gt;
&lt;li&gt;Nichole Misako Nomura &amp;amp; Quinn Dombrowski, &amp;quot;Quantifying Representations of Asian Identity in 21st-century Anglophone Fiction for Young Readers&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Religion Remix’d: The Language of Romanticism between Canon and Archive</title>
    <link href="https://litlab.stanford.edu/news/2022-10-09-religion-remix-d-the-language-of-romanticism-between-canon-and-archive/"/>
    <updated>2022-10-10T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2022-10-09-religion-remix-d-the-language-of-romanticism-between-canon-and-archive/</id>
    <content type="html">&lt;p&gt;On Monday, October 10, Mark Algee-Hewitt presented some work from his project “Religion Remix’d: The Language of Romanticism between Canon and Archive.”&lt;/p&gt;
&lt;p&gt;The poets that we have traditionally studied as “Romantic” are outliers among their contemporaries. Even as Wordsworth issued the call for poetry on the “incidents and situations of common life” the vast majority of poetry written between 1780 and 1830 took traditional, frequently religious, subjects and imagery as its object. And yet, the vocabulary of religion is not absent from the work of the canonical poets of the period, including Wordsworth, Hemans, Smith, and, of course, Blake. Despite the semantic similarities of their poetry to the archival religious poetry of the period, their use of religious language does not bring them any closer to the religious poetry of their less famous contemporaries. In this project, I use a set of computational methods to explore how canonical poets appropriate the semantics of religion, using it to construct a wholly new discourse in a way that remains inaccessible to traditional critical methods and sheds light on the differentiation of the canon and the archive.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Character, Otherness, and the Non-Human in the Star Wars Novel Corpus</title>
    <link href="https://litlab.stanford.edu/news/2022-10-30-character-otherness-and-the-non-human-in-the-star-wars-novel-corpus/"/>
    <updated>2022-10-31T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2022-10-30-character-otherness-and-the-non-human-in-the-star-wars-novel-corpus/</id>
    <content type="html">&lt;p&gt;On Monday, October 31, Mark Algee-Hewitt, Quinn Dombrowski, Nichole Nomura and Matt Warner presented on “Character, Otherness, and the Non-Human in the Star Wars Novel Corpus.”&lt;/p&gt;
&lt;p&gt;The Star Wars franchise offers a powerful set of representations of otherness within its various cultural artifacts and media forms. While previous work on representation of racialized, gendered and othered bodies in the Star Wars universe has rested on the visible signals of identity and otherness that visual media present to viewers (in the films, televisions series, and graphic novels), relatively little work has been done on the textual representation of these markers of difference in the novels. Within a purely textual medium, how is otherness communicated and how does it map onto our own categories of race, ethnicity and gender identity? When is it necessary to identify a character as a non-human? As a humanoid? How do these non-human identities intersect with gender? And to what extent does human-ness play the role of the default representational category, echoing the ways in which whiteness is an unmarked category within racialized depictions of characters in non-science fiction literature?&lt;/p&gt;
&lt;p&gt;In this presentation, we will update our findings from 2021. Building on the Lab’s suggestions, we have deepened our NLP research in ways that allow us to focus on characters rather than species descriptors. We have also expanded our corpus to include those books missing from our previous work. As we begin the process of writing up this project, we very much welcome comments and suggestions from Lab members on both the new work that we will present and the research that we will summarize.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Loudness and Suspense in 19th Century Fiction</title>
    <link href="https://litlab.stanford.edu/news/2022-11-13-loudness-and-suspense-in-19th-century-fiction/"/>
    <updated>2022-11-14T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2022-11-13-loudness-and-suspense-in-19th-century-fiction/</id>
    <content type="html">&lt;p&gt;On Monday, November 14, Svenja Guhr, who has been visiting the lab this quarter, will present some of the work that she has undertaken during her time with us. Her talk is called “Loudness and Suspense in 19th Century Fiction.”&lt;/p&gt;
&lt;p&gt;In a sound studies approach to a literary studies use case, she analyzes whether there is a correlation between the diegetic description of environmental sounds and suspenseful text passages in a 19th century English novel corpus. Drawing on theories of suspense in literary fiction and some findings from the 2014-2018 Literary Lab Project &amp;quot;Suspense: Language, Narrative, Affect&amp;quot;, she argues that suspenseful passages contain more detailed descriptions of the story’s environmental soundscape than unsuspenseful passages (e.g., the growl of a wild animal, the creaking of a wooden floor in a silent room) which consequently stretches out the story and delays the resolution of the conflict, increasing the degree of suspense in literary text.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Two presentations from Lab members</title>
    <link href="https://litlab.stanford.edu/news/2022-12-04-two-presentations-from-lab-members/"/>
    <updated>2022-12-05T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2022-12-04-two-presentations-from-lab-members/</id>
    <content type="html">&lt;p&gt;On December 5, we concluded the quarter with two shorter presentations from Lab members.&lt;/p&gt;
&lt;p&gt;First, Annie Lamar talked about: “How can literary theorists make use of geospatial data?”&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I&#39;ll be introducing Classical Atlas, a Python package that makes it easy to connect large ancient geospatial datasets in a Python environment. Making such data, which is typically hard to access and wrangle, accessible offers new opportunities for network scientists and computational literary theorists. In particular, I&#39;ll be showing how Classical Atlas integrates with ToposText. (No Classical or Python knowledge required for this talk!)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Then we heard from Maciej Kurzynski, who shared some of the work from his dissertation project Words of Passion: Narrative Technologies of Modern China,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;My dissertation project combines DH, cognitive narratology, and critical theory to offer new perspectives on modern Chinese literature. The main thesis is that, when combined, DH and cognitive studies allow us to avoid not only the pitfalls of the formalist/structuralist paradigm but also many of the issues caused by the historicist framework, such as the tendency to see literary texts as historical documents rather than aesthetic forms. The particular chapter I would like to share and hopefully get feedback on, is entitled &amp;quot;Words Close to Heart: A Techno-Cognitive Approach to Interiority in Modern Chinese Literature.&amp;quot; I argue that what is “close to heart” is not only subjectively important to a specific fictional character but also objectively close to the word “heart” in a literary text. I tag the Chinese vocabulary of interiority according to the period of usage (five epochs: Ming-Qing, Late Qing, Republican, Maoist, Contemporary) and measure semantic shifts between time-specific vector representations of the term “in one’s heart” (xinli 心里) and its synonyms (xinzhong 心中, neixin 内心, xinxia 心下, etc.) in early-modern and modern Chinese texts. This &amp;quot;temporal referencing&amp;quot; procedure provides a great deal of evidence to challenge the notion of clear-cut boundaries between cultural epochs. Moreover, instead of grounding this repetitive literary phenomenon in some kind of Chinese tradition, as has been done, I argue that the spatial metaphor of the “heart” as a container filled with emotions is grounded in the fact that bodies have and are boundaries.&lt;/p&gt;
&lt;/blockquote&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Truth in Fiction: Scientific Facts and Truth in Fiction</title>
    <link href="https://litlab.stanford.edu/news/2023-02-05-truth-in-fiction-scientific-facts-and-truth-in-fiction/"/>
    <updated>2023-02-06T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2023-02-05-truth-in-fiction-scientific-facts-and-truth-in-fiction/</id>
    <content type="html">&lt;p&gt;On Monday, February 6th, we had the first presentation from our collaboration on Climate Fiction. Mark Algee-Hewitt was be joined by other members of the project team, which includes Julia Brush (UConn), Aaron Hanlon (Colby), Mike Hill (SUNY Albany), Yohei Igarashi (UConn), and Sunghyun Lim (SUNY Albany), to present some of the preliminary results from their work.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Truth in Fiction: Scientific Facts and Truth in Fiction&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The rise of “climate fiction,” or “cli-fi” has suggested the possibility that fiction can be more effective in convincing a skeptical public of the necessity of immediate action than well-intentioned science communication. Given the complicated relationship of fiction to truth, it has become more urgent than ever to understand the ways in which novels are able to couch fact within the fabric of their fiction. When so much science communication studies has been concerned with false or misleading narratives about climate change, how do we grapple with deliberately false accounts (fiction) that seek to spread pro-scientific messaging?&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Spatial itineraries and political identities: “The Troubles” in contemporary fiction</title>
    <link href="https://litlab.stanford.edu/news/2023-02-12-spatial-itineraries-and-political-identities-the-troubles-in-contemporary-fiction/"/>
    <updated>2023-02-13T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2023-02-12-spatial-itineraries-and-political-identities-the-troubles-in-contemporary-fiction/</id>
    <content type="html">&lt;p&gt;On Monday, February 13th, Simone Abiatti, PhD candidate in Transcultural Studies in the Humanities at the University of Bergamo, and currently a visiting scholar with the Literary Lab, presented on &#39;Spatial itineraries and political identities: “The Troubles” in contemporary fiction.&#39;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Spatial itineraries and political identities: “The Troubles” in contemporary fiction&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Drawing upon Jameson&#39;s concept of cognitive mapping, Simone is examining how the itineraries of characters in novels dealing with the Troubles (the ethno-nationalist conflict in Northern Ireland) can be interpreted as a manifestation of political identity. Utilizing tools from computational linguistics, including topic modeling, named entity recognition, and automatic recognition of syntactic dependencies, Simone is first constructing a literary corpus pertaining to the Troubles. He will then reflect on the automatic retrieval of character itineraries, and subsequently reflect on the itinerary as a symbol. He will focus on how crossing borders, fleeing from areas affected by terror attacks, or traveling to places where one&#39;s loved ones reside, can help reflect the complexity of the war.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Modeling Domestic Space</title>
    <link href="https://litlab.stanford.edu/news/2023-03-12-modeling-domestic-space/"/>
    <updated>2023-03-13T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2023-03-12-modeling-domestic-space/</id>
    <content type="html">&lt;p&gt;On Monday, March 13th, the group working on “Modeling Domestic Space” presented the first results from the project.&lt;/p&gt;
&lt;p&gt;Is it possible to model domestic space in fiction? And what might such a model show about the features of domestic space, its discursive correlates, its historical and generic distribution, and more broadly about space and domesticity? Focusing on British nineteenth-century novels, this project attempts to make such a model. Along the way, we discuss why this seems like a tractable kind of space to model; why nineteenth-century novels present an important case; and how exactly we might operationalize domestic space and conceptualize a model of it.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>A New LitLab Website</title>
    <link href="https://litlab.stanford.edu/techne/new-litlab-website/"/>
    <updated>2023-04-13T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/techne/new-litlab-website/</id>
    <content type="html">&lt;p&gt;We&#39;re pleased to introduce the new website for the Stanford Literary Lab; it&#39;s the first overhaul of the site since 2015, and the first time the site has moved off of WordPress.&lt;/p&gt;
&lt;p&gt;When the Lab first built its website (first captured by the Internet Archive in &lt;a href=&quot;https://web.archive.org/web/20110206155231/http://litlab.stanford.edu/&quot;&gt;February 2011&lt;/a&gt;), WordPress was the obvious choice for a simple, no-fuss informational website, and it  served the Lab well as low-maintenance infrastructure for a decade.&lt;/p&gt;
&lt;p&gt;As we updated the site to a recent version of WordPress, we realized how much the platform has changed. For a modern-looking theme with a reasonable amount of flexibility, prepare to pay -- and not once, but as an annual single-site subscription, typically requiring an annual subscription to one or more premium plugins. These plugins and themes override each other in surprising and one-off ways, leading to a site that is frustrating to build, challenging to document for future site maintainers, and depends on ongoing payments to keep working. We needed something else, which for us meant HTML.&lt;/p&gt;
&lt;p&gt;There are currently two common routes that lead towards static websites: principles and praxis. The former adopts minimal computing practices as an ideology, seriously considering issues of ecological impact and global infrastructural limitations. The latter is often borne out of acknowledging the unending labor that must go into maintaining the technical underpinnings of a site, in order to avoid either the consequences of being hacked, or the site falling apart as dependencies further down in the stack are upgraded beyond the site&#39;s ability to bear that change (neither was quite our situation, but the security warnings were piling up and getting harder to banish).&lt;/p&gt;
&lt;p&gt;For us, the shift to a static site (using the &lt;a href=&quot;https://www.11ty.dev/&quot;&gt;Node-based Eleventy framework&lt;/a&gt;) was more motivated by the latter path more than the former, though the &amp;quot;&lt;a href=&quot;http://www.digitalhumanities.org/dhq/vol/16/2/000646/000646.html&quot;&gt;questions of minimal computing&lt;/a&gt;&amp;quot; posited by Roopika Risam and Alex Gil also inform our thinking with this new site:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;“what do we need?”; 2) “what do we have”; 3) “what must we prioritize?”; and 4) “what are we willing to give up?”&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;h2&gt;What do we need?&lt;/h2&gt;
&lt;p&gt;We&#39;ve argued about this quite extensively, but some things are less negotiable: the Lab has an ISSN for its Pamphlets series, which resolves to a webpage according to an international metadata record. At a minimum, we need a website for that. Beyond that, how valuable is a list of current and former members—and, as a pretty open research collective, who&#39;s a member, and who &lt;em&gt;was&lt;/em&gt; a member in 2014? How useful is a list of our current projects? What about the past projects? Should we maintain a page for each project, in perpetuity, for current and former members to point to as part of their CVs? And then there&#39;s the question of anything blog-like, be it the &amp;quot;&lt;a href=&quot;https://litlab.stanford.edu/techne&quot;&gt;Techne&lt;/a&gt;&amp;quot; posts or something that looks more like &amp;quot;&lt;a href=&quot;https://litlab.stanford.edu/news&quot;&gt;News&lt;/a&gt;&amp;quot;, covering things like Lab presentations, conference talks, and grant awards. Some of these questions are newer, but some are eternal; looking back to the first version of the website, it had &lt;a href=&quot;https://web.archive.org/web/20110630171551/http://litlab.stanford.edu/?page_id=107&quot;&gt;its own News section&lt;/a&gt; with several posts in 2011... all of which &lt;a href=&quot;https://web.archive.org/web/20120113180402/https://litlab.stanford.edu/&quot;&gt;vanish by 2012&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This new version of the site pushes the Lab the farthest it&#39;s ever gone in the direction of publicly recording its work, and we&#39;ll see whether or not that sticks: will we still be creating project pages in two years? And what about the more quotidian churn of news and events? We&#39;ve gone back through our mailing list to find old project presentations from the last few years to backfill a new generation of &amp;quot;News&amp;quot; with those announcements so visitors can see what we&#39;ve been up to in the recent-ish past, but will we be able to find the time to keep this all up-to-date? It should go without saying, but the lab does not have a (full- or part-) time webmaster, or even anyone whose day-to-day responsibilities clearly include keeping the site up to date.&lt;/p&gt;
&lt;h2&gt;What do we have?&lt;/h2&gt;
&lt;p&gt;What we have is fundamentally shaped by the WordPress website framework we have been operating in: we&#39;ve got pamphlets, a static, single projects page with (varying degrees of outdated information about what once were) current projects, and Techne as a kind of blog with 16 posts over 7 years.&lt;/p&gt;
&lt;p&gt;What we have is less interesting than what we are willing to make. And to that end, over the course of rebuilding this website we have created close to 50 news posts that capture the Lab&#39;s work, quarter-by-quarter, through its presentations. We have created &lt;a href=&quot;https://litlab.stanford.edu/projects/archive/&quot;&gt;archived project&lt;/a&gt; pages to connect with each pamphlet, along with several projects from the last five years that came to a close without producing a pamphlet, but reflect a considerable amount of thought and work nonetheless. We have also grappled with the always-challenging question of &amp;quot;what are our &lt;a href=&quot;https://litlab.stanford.edu/projects&quot;&gt;current projects&lt;/a&gt;&amp;quot;, and made pages for those as well.&lt;/p&gt;
&lt;h2&gt;What must we prioritize?&lt;/h2&gt;
&lt;p&gt;Fundamentally, our most important goal was simply a refreshed site that looked and felt like the product of DH research collective active in the 2020s. Along the way, though, we&#39;ve built a data model that we&#39;re hoping will make it easier to document our work here in the Lab, especially when that work hasn&#39;t ended up in a published pamphlet (or journal article). We&#39;d like a clearer record of our past, and we&#39;re hopeful that this will benefit lab members for whom it might be useful to have a pointer to their work on Lab projects, including those on the job market. We&#39;ve also overhauled the &lt;a href=&quot;https://litlab.stanford.edu/people&quot;&gt;People&lt;/a&gt; page to give core Lab researchers more visible space to describe their work and academic interests.&lt;/p&gt;
&lt;p&gt;The desire to minimize technical site maintenance was a significant factor in our choices as well: reducing the technical infrastructure of the site means that there are fewer things that could fundamentally bring down the site, and it means that the site&#39;s basic content can be more easily contributed to by more members of the lab.&lt;/p&gt;
&lt;h2&gt;What are we willing to give up?&lt;/h2&gt;
&lt;p&gt;For people accustomed to working with WordPress&#39;s WYSIWYG interface, shifting to working directly with Markdown files can be a bit of a shock. That said, that has never been the Lab&#39;s workflow: there&#39;s always been one or two people people —- not a designated webmaster so much as an over-stretched graduate student or director -- who&#39;ve handled the vast majority of website changes, struggling with the way WordPress handles authorship (which meant that anyone who contributed to the site&#39;s ocntent needed dummy accounts for attribution).&lt;/p&gt;
&lt;p&gt;Under the new system, we&#39;ve set up a series of templates to process the site content, and &lt;a href=&quot;https://github.com/literarylab/literarylab.github.io#readme&quot;&gt;written documentation&lt;/a&gt; for how to update or add new instances of all the types of content on the site. For most Lab members, we expect their experience with the website will involve filling in text-file templates for different kinds of content using Markdown and/or YAML, and either emailing them to the webmaster or -- for the bold -- proposing them as a pull request on GitHub, where the site is published. Writing in Markdown and/or YAML may not be the most comfortable for Lab members, but those skills are useful in the context of other tools, too. Even publishing on GitHub means a set of trade-offs: it takes us outside the sphere of Stanford IT support requests, but also leaves us less vulnerable to the web platform shifts that have characterized the last several years at Stanford.&lt;/p&gt;
&lt;h2&gt;The future of the Lab site&lt;/h2&gt;
&lt;p&gt;If you&#39;re curious about what we&#39;re doing technically with the LitLab site, check out our &lt;a href=&quot;https://github.com/literarylab/literarylab.github.io&quot;&gt;GitHub repo&lt;/a&gt;. We&#39;ve come up with some interesting workarounds for things like bibliographic formatting, by using a &lt;a href=&quot;https://github.com/Savjee/eleventy-plugin-bibtex&quot;&gt;plugin that formats citations from bibtex&lt;/a&gt;, and automatically generating the bibtex from (arguably) friendlier-to-author YAML frontmatter.&lt;/p&gt;
&lt;p&gt;That said, in the end, the biggest challenges for website maintenance are social, rather than technical. Only time will tell how it all will turn out.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Operationalizing Canonicity</title>
    <link href="https://litlab.stanford.edu/news/2023-04-16-operationalizing-canonicity/"/>
    <updated>2023-04-17T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2023-04-16-operationalizing-canonicity/</id>
    <content type="html">&lt;p&gt;On Monday, April 17, two collaborators from the ENS in Paris, Jean Barré and Thierry Poibeau, presented on their project called &#39;Operationalizing Canonicity&#39;.&lt;/p&gt;
&lt;p&gt;This presentation summarized their work on the notion of canonicity in the French context. They proposed an operationalization of the French literary canon through its contemporary reception, focusing particularly on the role of the school institution in the making of the canon. Their statistical modeling based on textual elements achieves predictive results with an accuracy from 70% to 74%. They assume that this linguistic norm is the result of biased latent selection mechanisms that are producing literary &amp;quot;immortality&amp;quot;.&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>Literary Criticism and Theory</title>
    <link href="https://litlab.stanford.edu/news/2023-06-04-literary-criticism-and-theory/"/>
    <updated>2023-06-05T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/2023-06-04-literary-criticism-and-theory/</id>
    <content type="html">&lt;p&gt;We finished the year with the first presentation of our Mellon-funded investigation into Literary Theory and Criticism.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Theory and Criticism&lt;/em&gt; project explores the relationship between literary theory and literary criticism in the 20th Century. The project is in the early stages of assembling corpora of literary theoretical and critical monographs, and is excited to share its progress so far, and some preliminary results provisionally titled &amp;quot;&lt;em&gt;Capital&lt;/em&gt; and Everything Else.&amp;quot;&lt;/p&gt;
</content>
  </entry>
  
  
  <entry>
    <title>LitLab Members Present at ACH and DH Conferences</title>
    <link href="https://litlab.stanford.edu/news/-litlab-members-present-at-ach-and-dh-conferences/"/>
    <updated>2023-08-16T00:00:00Z</updated>
    <id>https://litlab.stanford.edu/news/-litlab-members-present-at-ach-and-dh-conferences/</id>
    <content type="html">&lt;p&gt;Several Lab and individual projects made an appearance at major conferences this summer. At ACH, Francesco Bacci presented on &amp;quot;Conversion Therapy Novels: a Qualitative and Quantitative Genre Study&amp;quot;, Quinn Dombrowski presented on DH conference metrics as part of the panel &amp;quot;Digital (Humanities) Trace Data: Critical Approaches to Platforms, APIs, and Metrics for Studying DH Communities&amp;quot;, Merve Tekgürler presented with Umar Patel on &amp;quot;Ottoman (Turkish) NLP: Exploring the Application of Contemporary Text Technologies in 18th Century Histories&amp;quot;, and Carmen Thong presented on &amp;quot;Scaling to the World: DH as Theorized Method&amp;quot;. At DH 2023 in Graz, Quinn Dombrowski and Nichole Nomura presented on the panel &amp;quot;Readers, Tropes, and Translations: Directions for Digital Research into Youth Literature&amp;quot;, Nichole on &amp;quot;Young Readers, Textual Difficulty, and Genre&amp;quot;, and Quinn on &amp;quot;Wherefore Art Thou Shakespeare: How Identifiable are Shakespeare Adaptations for Young Readers?&amp;quot;&lt;/p&gt;
</content>
  </entry>
  
</feed>
